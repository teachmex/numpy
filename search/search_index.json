{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Numpy This document provides the tutorial on implementation of Numpy for Array Introduction, Arange , Arange demo with oscillation, Arange demo with wave, Arange demo with Fourier Series, Meshgrid Introduction, Meshgrid demo with, Spherical Harmonics, Meshgrid demo with Optimization, Meshgrid demo with Simulation, Algebra, Algebra with Quantum Mechanics, Algebra with Composite Hamiltonian, Statistics, Statistics with Probability, Statistics with sampling.","title":"Home"},{"location":"#introduction-to-numpy","text":"This document provides the tutorial on implementation of Numpy for Array Introduction, Arange , Arange demo with oscillation, Arange demo with wave, Arange demo with Fourier Series, Meshgrid Introduction, Meshgrid demo with, Spherical Harmonics, Meshgrid demo with Optimization, Meshgrid demo with Simulation, Algebra, Algebra with Quantum Mechanics, Algebra with Composite Hamiltonian, Statistics, Statistics with Probability, Statistics with sampling.","title":"Introduction to Numpy"},{"location":"Algebra-QuantumMechanics/","text":"Algebra: Quantum Mechanics Reference: http://qutip.org/docs/latest/index.html import numpy as np from qutip import * import seaborn as sns import matplotlib.pyplot as plt import numpy.linalg as LA from scipy.linalg import expm % matplotlib inline sns . set() #!pip install qutip Some Facts from Quantum Physics: Web function \\(\\psi(x)\\) is projection of abstract quantum state \\(|\\psi \\rangle \\) (in a certain representation) to a position space (representation) basis \\(|x \\rangle\\). Where, \\(|x\\rangle\\) is a continuous basis with orthogonality relation $\\langle x^{'}|x\\rangle = \\delta(x,x^{'})$. $$ \\psi(x) = \\langle x|\\psi \\rangle$$ Web function \\(\\psi(p)\\) is projection of abstract quantum state \\(|\\psi \\rangle \\) in a certain representation to a momentum space (representation) basis \\(|p \\rangle\\). Where, \\(|p\\rangle\\) is a continuous basis with orthogonality relation \\(\\langle p^{'}|p\\rangle = \\delta(p,p^{'})\\). $$\\phi(p) = \\langle p|\\phi \\rangle$$ Spherical Harmonics \\(Y_{l,m}(\\theta,\\phi)\\) is projection of abstract quantum state \\(| l,m \\rangle\\) (in angular momnetum representation) to a position space (representation) basis \\(|\\theta, \\phi \\rangle\\). $$Y_{l,m}(\\theta,\\phi) = \\langle \\theta,\\phi|l,m \\rangle$$ An unitary operator U can be constructed from exponentiation of Hermitian Operator H. $$U = exp(-i\\alpha H)$$ One application of this approach is defining Rotation matrix R in Hilbert space by implementation of angular momentum Operator (e.g., \\(L_x, L_y, L_z\\)) as generator of rotation in specific irreducible subspace (e.g.,l=0,l=1,l=2...) of the Hilbert space. A general rotation in Hilbert space is infinite dimentional rotation matrix. In eigen basis of anfgular momentum (\\(L^{2}, L_z\\)), this matrix appears as block diagonal matrix with block representing rotation in specific irriducible sub-space. In hydrogen like system with spinless particle, operator hamiltonian $H$, square of Angular momentum $L^{2}$ and z-omponent of $L$ operator(\\(L_z\\)) commute with eachother which means these operators are simultaneously diagonalized. The benefit of of this relation is that once we are able to find eigen basis of \\(L_z\\) operator, we get the eigen basis of Hamiltonian as well. The eighen states of hamiltonian represents the energy level of the system. 1. Hydrogen Atom: Angular Momentum and Spherical Harmonics The matrix element of general angular momnetum operators $J,J_z,J_+,J_-$ are as follows: One can write a python function to provide a matrix element for an arbitrary operators $J,J_z,J_+,J_-$ , but we dont have to work hard now python package qutip provides us these operators as quantum object Quobj (of kind operators). We will try to play around with some of them. Spin Angular Momentum \\(s = 1/2, ms = -1/2, 1/2\\) Matrix size = 2x2 sigmax() Quantum object: dims = [[2], [2]], shape = (2, 2), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 1.0\\1.0 & 0.0\\\\end{array}\\right)\\end{equation*} sx = np . array(sigmax()) sy = np . array(sigmay()) sz = np . array(sigmaz()) sx,sy,sz (array([[0.+0.j, 1.+0.j], [1.+0.j, 0.+0.j]]), array([[0.+0.j, 0.-1.j], [0.+1.j, 0.+0.j]]), array([[ 1.+0.j, 0.+0.j], [ 0.+0.j, -1.+0.j]])) np . dot(sx,sx), np . dot(sy,sy), np . dot(sz,sz) (array([[1.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]]), array([[1.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]]), array([[1.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]])) Experiment 1 : A generic rotation \\( U_n(\\theta) = e^{-i \\theta n.\\sigma} = e^{-i (\\theta_x \\sigma_x + \\theta_y \\sigma_y + \\theta_z \\sigma_z)} \\) Note: \\( U_n(\\theta) = e^{-i \\theta n.\\sigma} \\neq e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)} \\) since pauli matrices are non-commuting. Generic quantum state in ($s =1/2$) subspace: \\(|\\psi \\rangle = \\alpha |\\psi_{1/2}\\rangle + \\beta |\\psi_{-1/2}\\rangle\\) Non-commuting Matrix exponentials Let us tast above fact by evaluating generic rotation operator $U_n$ around a arbitrary axix $n$ (i.e., $e^{-i (\\theta_x \\sigma_x + \\theta_y \\sigma_y + \\theta_z \\sigma_z)})$ as U_direct and the product of individual rotation operator (i.e., $ e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)}$) as U_prod in the code cells below. We will clearly see that thes two terms are not equal verifying relation $e^{-i \\theta n.\\sigma} \\neq e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)}$. Calculate \\(e^{-i (\\theta_x \\sigma_x + \\theta_y \\sigma_y + \\theta_z \\sigma_z)})\\) as U_direct U_direct = expm( - 1 j * ((np . pi / 12 ) * sx + (np . pi / 12 ) * sy + (np . pi / 12 ) * sz)) U_direct array([[ 0.89894119-0.25291945j, -0.25291945-0.25291945j], [ 0.25291945-0.25291945j, 0.89894119+0.25291945j]]) Calculate \\(e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)}\\) as product of three individual rotation as U_prod Ux = expm( - 1 j * np . pi / 12 * sx) Uy = expm( - 1 j * np . pi / 10 * sy) Uz = expm( - 1 j * np . pi / 8 * sz) Uz,Ux,Uy (array([[0.92387953-0.38268343j, 0. +0.j ], [0. +0.j , 0.92387953+0.38268343j]]), array([[0.96592583+0.j , 0. -0.25881905j], [0. -0.25881905j, 0.96592583+0.j ]]), array([[ 0.95105652+0.j, -0.30901699+0.j], [ 0.30901699+0.j, 0.95105652+0.j]])) U_prod = np . dot(Ux,np . dot(Uy,Uz)) U_prod array([[ 0.81811516-0.42544356j, -0.18156837-0.34164059j], [ 0.18156837-0.34164059j, 0.81811516+0.42544356j]]) Are U_direct and U_prod same Operators? Why? We can see these two operators are not same by implement them in same initial state psi0 vector and observe the final states are not same. psi0 = 1 / np . sqrt( 2 ) * np . array([ 1 , 1 ]) psi0 array([0.70710678, 0.70710678]) np . dot(U_direct,psi0), np . dot(U_prod,psi0) (array([0.45680635-3.57682117e-01j, 0.81448847+8.32667268e-17j]), array([0.45010655-0.5424104j , 0.706883 +0.05925765j])) In fact, both of them are Unitary operators with determinant 1 LA . det(U_direct), LA . det(U_prod) ((0.9999999999999999+1.1102230246251564e-16j), (1.0000000000000002+0j)) Angular Momentum \\(l =1, m = -1,0,1\\) Matrix size = 3x3 jmat( 1 ) (Quantum object: dims = [[3], [3]], shape = (3, 3), type = oper, isherm = True Qobj data = [[0. 0.70710678 0. ] [0.70710678 0. 0.70710678] [0. 0.70710678 0. ]], Quantum object: dims = [[3], [3]], shape = (3, 3), type = oper, isherm = True Qobj data = [[0.+0.j 0.-0.70710678j 0.+0.j ] [0.+0.70710678j 0.+0.j 0.-0.70710678j] [0.+0.j 0.+0.70710678j 0.+0.j ]], Quantum object: dims = [[3], [3]], shape = (3, 3), type = oper, isherm = True Qobj data = [[ 1. 0. 0.] [ 0. 0. 0.] [ 0. 0. -1.]]) LX = np . array(jmat( 1 , 'x' )) LY = np . array(jmat( 1 , 'y' )) LZ = np . array(jmat( 1 , 'z' )) Do \\(L_x, L_y\\) commute? np . dot(LX,LY) == np . dot(LY,LX) array([[False, True, True], [ True, False, True], [ True, True, False]]) What is matrix element of $L^{2}$ ? L_square = (np . dot(LX,LX) + np . dot(LY,LY) + np . dot(LZ,LZ)) L_square array([[2.+0.j, 0.+0.j, 0.+0.j], [0.+0.j, 2.+0.j, 0.+0.j], [0.+0.j, 0.+0.j, 2.+0.j]]) Experiment 2 Rotaion \\( R(\\theta) = e^{-i \\theta n.L} = e^{-i (\\theta_x L_x + \\theta_y L_y + \\theta_z L_z)} \\) Generic quantum state in ($l =0$) subspace: \\(|\\psi \\rangle = \\alpha |\\psi_{10}\\rangle + \\beta |\\psi_{11} \\rangle + \\gamma |\\psi_{1-1} \\rangle\\) Let us find rotation matrix for subspace (l=1) with different values of \\(\\theta_x, \\theta_y, \\theta_z\\) Rx = expm( - ( 1.0 j) * 0.1 * LX) Ry = expm( - ( 1.0 j) * 0.2 * LY) Rz = expm( - ( 1.0 j) * 0.3 * LZ) R_prod = np . dot(Rx,np . dot(Ry,Rz)) R_direct = expm( - ( 1.0 j) * ( 0.1 * LX + 0.2 * LY + 0.3 * LZ)) R_prod, R_direct (array([[ 0.94049792-3.01310652e-01j, -0.14048043-6.91857281e-02j, 0.00420456+1.16811741e-02j], [ 0.11267399-1.08747365e-01j, 0.97517033+6.69983947e-18j, -0.11267399-1.08747365e-01j], [ 0.00420456-1.16811741e-02j, 0.14048043-6.91857281e-02j, 0.94049792+3.01310652e-01j]]), array([[ 0.94316771-2.93048837e-01j, -0.14862798-4.81054052e-02j, 0.00741291+9.88387642e-03j], [ 0.12766111-9.00391414e-02j, 0.97529031-1.50304582e-18j, -0.12766111-9.00391414e-02j], [ 0.00741291-9.88387642e-03j, 0.14862798-4.81054052e-02j, 0.94316771+2.93048837e-01j]])) Mini Assignment: - Roate a random vector \\(|\\psi \\rangle \\),i.e. (\\(|\\psi \\rangle = \\alpha |\\psi_{10}\\rangle + \\beta |\\psi_{11} \\rangle + \\gamma |\\psi_{1-1} \\rangle)\\) by implementing R_prod and R_direct calculated above and compere the final state vectors. Angular Momentum plus Spin: \\(l = 3/2, m = -3/2,-1/2,1/2,3/2\\) Matrix size = 4x4 jmat( 3 / 2 , 'x' ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.866 & 0.0 & 0.0\\0.866 & 0.0 & 1.0 & 0.0\\0.0 & 1.0 & 0.0 & 0.866\\0.0 & 0.0 & 0.866 & 0.0\\\\end{array}\\right)\\end{equation*} jmat( 3 / 2 , 'y' ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & -0.866j & 0.0 & 0.0\\0.866j & 0.0 & -1.0j & 0.0\\0.0 & 1.0j & 0.0 & -0.866j\\0.0 & 0.0 & 0.866j & 0.0\\\\end{array}\\right)\\end{equation*} Summary: Rotation in Hilbert Space Structure of a general rotation matrix (R) in Hilbert space j = 1/2,1,3/2,2,5/2,3,... A general rotation matrix ($R$) in hilbert space of basis \\(|l,m\\rangle\\) appears as block diagonal matrix, wher every block represents the rotation with specific subspaces called irriducible subspace. In the same basis $|l,m\\rangle$ Hamiltonian Matrix $H$, $J^2$ and $J_z$ are simultaneously diagonalized. 2. Quantum Harmonic Oscillator momentum( 5 ) Quantum object: dims = [[5], [5]], shape = (5, 5), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & -0.707j & 0.0 & 0.0 & 0.0\\0.707j & 0.0 & -1.0j & 0.0 & 0.0\\0.0 & 1.0j & 0.0 & -1.225j & 0.0\\0.0 & 0.0 & 1.225j & 0.0 & -1.414j\\0.0 & 0.0 & 0.0 & 1.414j & 0.0\\\\end{array}\\right)\\end{equation*} position( 5 ) Quantum object: dims = [[5], [5]], shape = (5, 5), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.707 & 0.0 & 0.0 & 0.0\\0.707 & 0.0 & 1.0 & 0.0 & 0.0\\0.0 & 1.0 & 0.0 & 1.225 & 0.0\\0.0 & 0.0 & 1.225 & 0.0 & 1.414\\0.0 & 0.0 & 0.0 & 1.414 & 0.0\\\\end{array}\\right)\\end{equation*} create( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = False\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.0 & 0.0 & 0.0\\1.0 & 0.0 & 0.0 & 0.0\\0.0 & 1.414 & 0.0 & 0.0\\0.0 & 0.0 & 1.732 & 0.0\\\\end{array}\\right)\\end{equation*} destroy( 5 ) Quantum object: dims = [[5], [5]], shape = (5, 5), type = oper, isherm = False\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 1.0 & 0.0 & 0.0 & 0.0\\0.0 & 0.0 & 1.414 & 0.0 & 0.0\\0.0 & 0.0 & 0.0 & 1.732 & 0.0\\0.0 & 0.0 & 0.0 & 0.0 & 2.0\\0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\\end{array}\\right)\\end{equation*} num( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.0 & 0.0 & 0.0\\0.0 & 1.0 & 0.0 & 0.0\\0.0 & 0.0 & 2.0 & 0.0\\0.0 & 0.0 & 0.0 & 3.0\\\\end{array}\\right)\\end{equation*} 3. Random Matrices Random hermitian matrix rand_herm( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}-0.501 & (-0.172+0.146j) & 0.0 & (-0.060+0.697j)\\(-0.172-0.146j) & 0.169 & (-0.305+0.588j) & (0.044+0.641j)\\0.0 & (-0.305-0.588j) & 0.0 & (-0.458-0.087j)\\(-0.060-0.697j) & (0.044-0.641j) & (-0.458+0.087j) & 0.756\\\\end{array}\\right)\\end{equation*} Random Unitary Matrix rand_unitary( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = False\\begin{equation }\\left(\\begin{array}{ {11}c}(0.332-0.610j) & (-0.248+0.007j) & (-0.332-0.267j) & (0.468-0.235j)\\(-0.062-0.085j) & (0.838-0.353j) & (-0.312-0.253j) & (3.589\\times10^{-05}-0.034j)\\(-0.032-0.481j) & (0.145-0.293j) & (0.790+0.106j) & (0.020-0.156j)\\(-0.427+0.302j) & (0.069+0.006j) & (0.109+0.097j) & (0.834-0.066j)\\\\end{array}\\right)\\end{equation*} Mini Assignment: Generate a random Hermitian matrix $H$ of size 10 by 10. Diagonalize the Hermitian Operator $H$ and find eigne values and eigen vectors. Create a Unitary operator $U$ by exponentiating the Hermitian matrix i.e. $U = e^{-i \\alpha H}$. Apply operator $U$ over eigen vectors of operator $H$. Does this operation preserve the norm of eigen vectors? Check that $U$ is unitary or not. What is the determinant of $U$? Reference Rotation Spherical Harmonics Angular Momentum","title":"Algebra with Quantum Mechanics"},{"location":"Algebra-QuantumMechanics/#algebra-quantum-mechanics","text":"Reference: http://qutip.org/docs/latest/index.html import numpy as np from qutip import * import seaborn as sns import matplotlib.pyplot as plt import numpy.linalg as LA from scipy.linalg import expm % matplotlib inline sns . set() #!pip install qutip","title":"Algebra: Quantum Mechanics"},{"location":"Algebra-QuantumMechanics/#some-facts-from-quantum-physics","text":"Web function \\(\\psi(x)\\) is projection of abstract quantum state \\(|\\psi \\rangle \\) (in a certain representation) to a position space (representation) basis \\(|x \\rangle\\). Where, \\(|x\\rangle\\) is a continuous basis with orthogonality relation $\\langle x^{'}|x\\rangle = \\delta(x,x^{'})$. $$ \\psi(x) = \\langle x|\\psi \\rangle$$ Web function \\(\\psi(p)\\) is projection of abstract quantum state \\(|\\psi \\rangle \\) in a certain representation to a momentum space (representation) basis \\(|p \\rangle\\). Where, \\(|p\\rangle\\) is a continuous basis with orthogonality relation \\(\\langle p^{'}|p\\rangle = \\delta(p,p^{'})\\). $$\\phi(p) = \\langle p|\\phi \\rangle$$ Spherical Harmonics \\(Y_{l,m}(\\theta,\\phi)\\) is projection of abstract quantum state \\(| l,m \\rangle\\) (in angular momnetum representation) to a position space (representation) basis \\(|\\theta, \\phi \\rangle\\). $$Y_{l,m}(\\theta,\\phi) = \\langle \\theta,\\phi|l,m \\rangle$$ An unitary operator U can be constructed from exponentiation of Hermitian Operator H. $$U = exp(-i\\alpha H)$$ One application of this approach is defining Rotation matrix R in Hilbert space by implementation of angular momentum Operator (e.g., \\(L_x, L_y, L_z\\)) as generator of rotation in specific irreducible subspace (e.g.,l=0,l=1,l=2...) of the Hilbert space. A general rotation in Hilbert space is infinite dimentional rotation matrix. In eigen basis of anfgular momentum (\\(L^{2}, L_z\\)), this matrix appears as block diagonal matrix with block representing rotation in specific irriducible sub-space. In hydrogen like system with spinless particle, operator hamiltonian $H$, square of Angular momentum $L^{2}$ and z-omponent of $L$ operator(\\(L_z\\)) commute with eachother which means these operators are simultaneously diagonalized. The benefit of of this relation is that once we are able to find eigen basis of \\(L_z\\) operator, we get the eigen basis of Hamiltonian as well. The eighen states of hamiltonian represents the energy level of the system.","title":"Some Facts from Quantum Physics:"},{"location":"Algebra-QuantumMechanics/#1-hydrogen-atom-angular-momentum-and-spherical-harmonics","text":"The matrix element of general angular momnetum operators $J,J_z,J_+,J_-$ are as follows: One can write a python function to provide a matrix element for an arbitrary operators $J,J_z,J_+,J_-$ , but we dont have to work hard now python package qutip provides us these operators as quantum object Quobj (of kind operators). We will try to play around with some of them.","title":"1. Hydrogen Atom: Angular Momentum and Spherical Harmonics"},{"location":"Algebra-QuantumMechanics/#spin-angular-momentum-s-12-ms-12-12","text":"Matrix size = 2x2 sigmax() Quantum object: dims = [[2], [2]], shape = (2, 2), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 1.0\\1.0 & 0.0\\\\end{array}\\right)\\end{equation*} sx = np . array(sigmax()) sy = np . array(sigmay()) sz = np . array(sigmaz()) sx,sy,sz (array([[0.+0.j, 1.+0.j], [1.+0.j, 0.+0.j]]), array([[0.+0.j, 0.-1.j], [0.+1.j, 0.+0.j]]), array([[ 1.+0.j, 0.+0.j], [ 0.+0.j, -1.+0.j]])) np . dot(sx,sx), np . dot(sy,sy), np . dot(sz,sz) (array([[1.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]]), array([[1.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]]), array([[1.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]]))","title":"Spin Angular Momentum \\(s = 1/2, ms = -1/2, 1/2\\)"},{"location":"Algebra-QuantumMechanics/#experiment-1","text":"A generic rotation \\( U_n(\\theta) = e^{-i \\theta n.\\sigma} = e^{-i (\\theta_x \\sigma_x + \\theta_y \\sigma_y + \\theta_z \\sigma_z)} \\) Note: \\( U_n(\\theta) = e^{-i \\theta n.\\sigma} \\neq e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)} \\) since pauli matrices are non-commuting. Generic quantum state in ($s =1/2$) subspace: \\(|\\psi \\rangle = \\alpha |\\psi_{1/2}\\rangle + \\beta |\\psi_{-1/2}\\rangle\\) Non-commuting Matrix exponentials Let us tast above fact by evaluating generic rotation operator $U_n$ around a arbitrary axix $n$ (i.e., $e^{-i (\\theta_x \\sigma_x + \\theta_y \\sigma_y + \\theta_z \\sigma_z)})$ as U_direct and the product of individual rotation operator (i.e., $ e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)}$) as U_prod in the code cells below. We will clearly see that thes two terms are not equal verifying relation $e^{-i \\theta n.\\sigma} \\neq e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)}$. Calculate \\(e^{-i (\\theta_x \\sigma_x + \\theta_y \\sigma_y + \\theta_z \\sigma_z)})\\) as U_direct U_direct = expm( - 1 j * ((np . pi / 12 ) * sx + (np . pi / 12 ) * sy + (np . pi / 12 ) * sz)) U_direct array([[ 0.89894119-0.25291945j, -0.25291945-0.25291945j], [ 0.25291945-0.25291945j, 0.89894119+0.25291945j]]) Calculate \\(e^{-i (\\theta_x \\sigma_x)} e^{-i (\\theta_y \\sigma_y)} e^{-i(\\theta_z \\sigma_z)}\\) as product of three individual rotation as U_prod Ux = expm( - 1 j * np . pi / 12 * sx) Uy = expm( - 1 j * np . pi / 10 * sy) Uz = expm( - 1 j * np . pi / 8 * sz) Uz,Ux,Uy (array([[0.92387953-0.38268343j, 0. +0.j ], [0. +0.j , 0.92387953+0.38268343j]]), array([[0.96592583+0.j , 0. -0.25881905j], [0. -0.25881905j, 0.96592583+0.j ]]), array([[ 0.95105652+0.j, -0.30901699+0.j], [ 0.30901699+0.j, 0.95105652+0.j]])) U_prod = np . dot(Ux,np . dot(Uy,Uz)) U_prod array([[ 0.81811516-0.42544356j, -0.18156837-0.34164059j], [ 0.18156837-0.34164059j, 0.81811516+0.42544356j]]) Are U_direct and U_prod same Operators? Why? We can see these two operators are not same by implement them in same initial state psi0 vector and observe the final states are not same. psi0 = 1 / np . sqrt( 2 ) * np . array([ 1 , 1 ]) psi0 array([0.70710678, 0.70710678]) np . dot(U_direct,psi0), np . dot(U_prod,psi0) (array([0.45680635-3.57682117e-01j, 0.81448847+8.32667268e-17j]), array([0.45010655-0.5424104j , 0.706883 +0.05925765j])) In fact, both of them are Unitary operators with determinant 1 LA . det(U_direct), LA . det(U_prod) ((0.9999999999999999+1.1102230246251564e-16j), (1.0000000000000002+0j))","title":"Experiment 1 :"},{"location":"Algebra-QuantumMechanics/#angular-momentum-l-1-m-101","text":"Matrix size = 3x3 jmat( 1 ) (Quantum object: dims = [[3], [3]], shape = (3, 3), type = oper, isherm = True Qobj data = [[0. 0.70710678 0. ] [0.70710678 0. 0.70710678] [0. 0.70710678 0. ]], Quantum object: dims = [[3], [3]], shape = (3, 3), type = oper, isherm = True Qobj data = [[0.+0.j 0.-0.70710678j 0.+0.j ] [0.+0.70710678j 0.+0.j 0.-0.70710678j] [0.+0.j 0.+0.70710678j 0.+0.j ]], Quantum object: dims = [[3], [3]], shape = (3, 3), type = oper, isherm = True Qobj data = [[ 1. 0. 0.] [ 0. 0. 0.] [ 0. 0. -1.]]) LX = np . array(jmat( 1 , 'x' )) LY = np . array(jmat( 1 , 'y' )) LZ = np . array(jmat( 1 , 'z' )) Do \\(L_x, L_y\\) commute? np . dot(LX,LY) == np . dot(LY,LX) array([[False, True, True], [ True, False, True], [ True, True, False]]) What is matrix element of $L^{2}$ ? L_square = (np . dot(LX,LX) + np . dot(LY,LY) + np . dot(LZ,LZ)) L_square array([[2.+0.j, 0.+0.j, 0.+0.j], [0.+0.j, 2.+0.j, 0.+0.j], [0.+0.j, 0.+0.j, 2.+0.j]])","title":"Angular Momentum \\(l =1, m = -1,0,1\\)"},{"location":"Algebra-QuantumMechanics/#experiment-2","text":"Rotaion \\( R(\\theta) = e^{-i \\theta n.L} = e^{-i (\\theta_x L_x + \\theta_y L_y + \\theta_z L_z)} \\) Generic quantum state in ($l =0$) subspace: \\(|\\psi \\rangle = \\alpha |\\psi_{10}\\rangle + \\beta |\\psi_{11} \\rangle + \\gamma |\\psi_{1-1} \\rangle\\) Let us find rotation matrix for subspace (l=1) with different values of \\(\\theta_x, \\theta_y, \\theta_z\\) Rx = expm( - ( 1.0 j) * 0.1 * LX) Ry = expm( - ( 1.0 j) * 0.2 * LY) Rz = expm( - ( 1.0 j) * 0.3 * LZ) R_prod = np . dot(Rx,np . dot(Ry,Rz)) R_direct = expm( - ( 1.0 j) * ( 0.1 * LX + 0.2 * LY + 0.3 * LZ)) R_prod, R_direct (array([[ 0.94049792-3.01310652e-01j, -0.14048043-6.91857281e-02j, 0.00420456+1.16811741e-02j], [ 0.11267399-1.08747365e-01j, 0.97517033+6.69983947e-18j, -0.11267399-1.08747365e-01j], [ 0.00420456-1.16811741e-02j, 0.14048043-6.91857281e-02j, 0.94049792+3.01310652e-01j]]), array([[ 0.94316771-2.93048837e-01j, -0.14862798-4.81054052e-02j, 0.00741291+9.88387642e-03j], [ 0.12766111-9.00391414e-02j, 0.97529031-1.50304582e-18j, -0.12766111-9.00391414e-02j], [ 0.00741291-9.88387642e-03j, 0.14862798-4.81054052e-02j, 0.94316771+2.93048837e-01j]])) Mini Assignment: - Roate a random vector \\(|\\psi \\rangle \\),i.e. (\\(|\\psi \\rangle = \\alpha |\\psi_{10}\\rangle + \\beta |\\psi_{11} \\rangle + \\gamma |\\psi_{1-1} \\rangle)\\) by implementing R_prod and R_direct calculated above and compere the final state vectors.","title":"Experiment 2"},{"location":"Algebra-QuantumMechanics/#angular-momentum-plus-spin-l-32-m-32-121232","text":"Matrix size = 4x4 jmat( 3 / 2 , 'x' ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.866 & 0.0 & 0.0\\0.866 & 0.0 & 1.0 & 0.0\\0.0 & 1.0 & 0.0 & 0.866\\0.0 & 0.0 & 0.866 & 0.0\\\\end{array}\\right)\\end{equation*} jmat( 3 / 2 , 'y' ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & -0.866j & 0.0 & 0.0\\0.866j & 0.0 & -1.0j & 0.0\\0.0 & 1.0j & 0.0 & -0.866j\\0.0 & 0.0 & 0.866j & 0.0\\\\end{array}\\right)\\end{equation*}","title":"Angular Momentum plus Spin: \\(l = 3/2, m = -3/2,-1/2,1/2,3/2\\)"},{"location":"Algebra-QuantumMechanics/#summary-rotation-in-hilbert-space","text":"Structure of a general rotation matrix (R) in Hilbert space j = 1/2,1,3/2,2,5/2,3,... A general rotation matrix ($R$) in hilbert space of basis \\(|l,m\\rangle\\) appears as block diagonal matrix, wher every block represents the rotation with specific subspaces called irriducible subspace. In the same basis $|l,m\\rangle$ Hamiltonian Matrix $H$, $J^2$ and $J_z$ are simultaneously diagonalized.","title":"Summary: Rotation in Hilbert Space"},{"location":"Algebra-QuantumMechanics/#2-quantum-harmonic-oscillator","text":"momentum( 5 ) Quantum object: dims = [[5], [5]], shape = (5, 5), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & -0.707j & 0.0 & 0.0 & 0.0\\0.707j & 0.0 & -1.0j & 0.0 & 0.0\\0.0 & 1.0j & 0.0 & -1.225j & 0.0\\0.0 & 0.0 & 1.225j & 0.0 & -1.414j\\0.0 & 0.0 & 0.0 & 1.414j & 0.0\\\\end{array}\\right)\\end{equation*} position( 5 ) Quantum object: dims = [[5], [5]], shape = (5, 5), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.707 & 0.0 & 0.0 & 0.0\\0.707 & 0.0 & 1.0 & 0.0 & 0.0\\0.0 & 1.0 & 0.0 & 1.225 & 0.0\\0.0 & 0.0 & 1.225 & 0.0 & 1.414\\0.0 & 0.0 & 0.0 & 1.414 & 0.0\\\\end{array}\\right)\\end{equation*} create( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = False\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.0 & 0.0 & 0.0\\1.0 & 0.0 & 0.0 & 0.0\\0.0 & 1.414 & 0.0 & 0.0\\0.0 & 0.0 & 1.732 & 0.0\\\\end{array}\\right)\\end{equation*} destroy( 5 ) Quantum object: dims = [[5], [5]], shape = (5, 5), type = oper, isherm = False\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 1.0 & 0.0 & 0.0 & 0.0\\0.0 & 0.0 & 1.414 & 0.0 & 0.0\\0.0 & 0.0 & 0.0 & 1.732 & 0.0\\0.0 & 0.0 & 0.0 & 0.0 & 2.0\\0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\\end{array}\\right)\\end{equation*} num( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 0.0 & 0.0 & 0.0\\0.0 & 1.0 & 0.0 & 0.0\\0.0 & 0.0 & 2.0 & 0.0\\0.0 & 0.0 & 0.0 & 3.0\\\\end{array}\\right)\\end{equation*}","title":"2. Quantum Harmonic Oscillator"},{"location":"Algebra-QuantumMechanics/#3-random-matrices","text":"Random hermitian matrix rand_herm( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}-0.501 & (-0.172+0.146j) & 0.0 & (-0.060+0.697j)\\(-0.172-0.146j) & 0.169 & (-0.305+0.588j) & (0.044+0.641j)\\0.0 & (-0.305-0.588j) & 0.0 & (-0.458-0.087j)\\(-0.060-0.697j) & (0.044-0.641j) & (-0.458+0.087j) & 0.756\\\\end{array}\\right)\\end{equation*} Random Unitary Matrix rand_unitary( 4 ) Quantum object: dims = [[4], [4]], shape = (4, 4), type = oper, isherm = False\\begin{equation }\\left(\\begin{array}{ {11}c}(0.332-0.610j) & (-0.248+0.007j) & (-0.332-0.267j) & (0.468-0.235j)\\(-0.062-0.085j) & (0.838-0.353j) & (-0.312-0.253j) & (3.589\\times10^{-05}-0.034j)\\(-0.032-0.481j) & (0.145-0.293j) & (0.790+0.106j) & (0.020-0.156j)\\(-0.427+0.302j) & (0.069+0.006j) & (0.109+0.097j) & (0.834-0.066j)\\\\end{array}\\right)\\end{equation*}","title":"3. Random Matrices"},{"location":"Algebra-QuantumMechanics/#mini-assignment","text":"Generate a random Hermitian matrix $H$ of size 10 by 10. Diagonalize the Hermitian Operator $H$ and find eigne values and eigen vectors. Create a Unitary operator $U$ by exponentiating the Hermitian matrix i.e. $U = e^{-i \\alpha H}$. Apply operator $U$ over eigen vectors of operator $H$. Does this operation preserve the norm of eigen vectors? Check that $U$ is unitary or not. What is the determinant of $U$?","title":"Mini Assignment:"},{"location":"Algebra-QuantumMechanics/#reference","text":"Rotation Spherical Harmonics Angular Momentum","title":"Reference"},{"location":"Statistics/","text":"Satistics - Introduction https://numpy.org/doc/stable/reference/routines.statistics.html import numpy as np import pandas as pd 1. Simple Statistical Exploration A = np . random . rand( 10 , 10 ) A array([[7.16622070e-01, 8.67937006e-01, 8.56064808e-01, 8.57568350e-01, 3.67796230e-01, 2.14346363e-01, 6.31599734e-01, 3.57119296e-01, 4.35844862e-01, 2.35483572e-01], [7.22209004e-01, 9.63310994e-01, 8.16973750e-01, 8.06853501e-01, 6.89970795e-01, 5.83418094e-01, 2.65411361e-02, 7.51624520e-01, 9.59391711e-02, 4.15581862e-01], [8.57467652e-01, 9.63049501e-01, 7.90326873e-01, 7.95018533e-01, 7.18152201e-01, 4.38823904e-01, 6.45775017e-01, 7.60717860e-01, 9.24224445e-01, 3.13183949e-01], [3.83836565e-01, 8.56498268e-01, 5.93058237e-01, 2.46908006e-01, 3.10960918e-01, 6.21511447e-01, 2.91997569e-01, 8.13963097e-01, 2.01951380e-01, 5.79517413e-01], [8.41978469e-02, 3.81861596e-01, 1.24320969e-01, 1.92482070e-01, 4.70194470e-01, 9.69335675e-01, 2.07682914e-01, 8.99462218e-01, 3.56681685e-01, 2.10451446e-01], [2.50727875e-01, 4.45871405e-01, 4.75621851e-01, 1.76392569e-01, 4.44494388e-01, 1.97214743e-01, 2.23416686e-01, 9.79333246e-01, 2.13115739e-01, 3.26530393e-01], [6.18496895e-01, 8.46193255e-01, 9.20304738e-01, 4.49655661e-01, 6.66495695e-01, 7.23878345e-01, 4.71483753e-01, 3.35079701e-01, 2.80457713e-01, 4.17035078e-01], [3.74931699e-01, 1.64469071e-02, 8.24518210e-01, 4.40508140e-01, 3.04546586e-01, 9.61964988e-01, 9.13281778e-01, 1.08067818e-01, 3.52596222e-01, 9.71936660e-01], [6.24254898e-01, 6.83181203e-01, 7.55043126e-01, 5.46638933e-01, 1.40146074e-01, 8.45252325e-02, 2.54143985e-01, 4.85118512e-04, 8.08711713e-01, 9.33249065e-01], [4.10573251e-01, 6.30441143e-01, 6.04971277e-01, 4.07647377e-01, 8.25351233e-01, 5.19909474e-01, 9.76184695e-02, 8.11742544e-01, 6.04160360e-01, 4.48347194e-01]]) A . shape (10, 10) A . max() 0.9793332463866128 A . min() 0.00048511851159238617 A . mean() 0.523381592476641 np . median(A) 0.47355280212201306 A . std() 0.2772531462004282 np . cov(A) array([[ 0.06873698, 0.03278827, 0.03668793, -0.00331421, -0.05083651, -0.00850994, 0.02390898, -0.02626347, 0.03131505, -0.01397033], [ 0.03278827, 0.09816338, 0.01792033, 0.04059124, 0.01004886, 0.02993195, 0.03987967, -0.05153719, -0.00568755, 0.03430207], [ 0.03668793, 0.01792033, 0.04244191, -0.00450095, -0.0155604 , 0.00718968, 0.00519803, -0.05720041, 0.00948249, 0.01091863], [-0.00331421, 0.04059124, -0.00450095, 0.05553323, 0.03406878, 0.03667575, 0.01972246, -0.01604353, -0.00940025, 0.02010751], [-0.05083651, 0.01004886, -0.0155604 , 0.03406878, 0.09681987, 0.03356765, -0.00601017, -0.01500941, -0.0759788 , 0.0324855 ], [-0.00850994, 0.02993195, 0.00718968, 0.03667575, 0.03356765, 0.0581328 , -0.00229493, -0.04089154, -0.03034172, 0.03377649], [ 0.02390898, 0.03987967, 0.00519803, 0.01972246, -0.00601017, -0.00229493, 0.04676247, 0.00605692, 0.00312176, 0.00634826], [-0.02626347, -0.05153719, -0.05720041, -0.01604353, -0.01500941, -0.04089154, 0.00605692, 0.13017446, 0.01244017, -0.04495907], [ 0.03131505, -0.00568755, 0.00948249, -0.00940025, -0.0759788 , -0.03034172, 0.00312176, 0.01244017, 0.11204687, -0.01455545], [-0.01397033, 0.03430207, 0.01091863, 0.02010751, 0.0324855 , 0.03377649, 0.00634826, -0.04495907, -0.01455545, 0.04560073]]) Mean and std of row np . mean(A,axis = 0 ) array([0.50433178, 0.66547913, 0.67612038, 0.49196731, 0.49381086, 0.53149283, 0.3763541 , 0.58175954, 0.42736833, 0.48513166]) np . std(A,axis = 0 ) array([0.22924647, 0.28962689, 0.22578381, 0.24259586, 0.21017855, 0.2908006 , 0.26627669, 0.33170298, 0.25736132, 0.25495454]) Mean and std of col np . mean(A,axis = 1 ) array([0.55403823, 0.58724228, 0.72067399, 0.49002029, 0.38966709, 0.37327189, 0.57290808, 0.5268799 , 0.48303793, 0.53607623]) np . std(A,axis = 1 ) array([0.2487233 , 0.29723231, 0.19544236, 0.22356186, 0.29519126, 0.2287346 , 0.20514927, 0.34228207, 0.31755658, 0.20258494]) 2. Feature Scaling: Mini-max scalar A = ( 100 * np . random . rand( 10 , 10 )) . astype( int ) A array([[59, 39, 55, 69, 55, 10, 89, 79, 71, 77], [36, 72, 69, 36, 38, 43, 21, 53, 65, 18], [29, 78, 85, 77, 11, 85, 73, 83, 47, 82], [17, 70, 94, 48, 10, 68, 92, 96, 12, 56], [ 5, 54, 30, 57, 86, 49, 10, 22, 17, 73], [46, 36, 63, 60, 64, 95, 76, 64, 13, 93], [79, 98, 56, 67, 38, 21, 16, 67, 15, 13], [45, 11, 90, 26, 64, 25, 10, 33, 97, 36], [86, 88, 68, 24, 40, 20, 17, 59, 83, 74], [67, 76, 64, 47, 73, 82, 77, 67, 15, 49]]) A_mm = (A - A . min()) / (A . max() - A . min()) A_mm array([[0.58064516, 0.3655914 , 0.53763441, 0.68817204, 0.53763441, 0.05376344, 0.90322581, 0.79569892, 0.70967742, 0.77419355], [0.33333333, 0.72043011, 0.68817204, 0.33333333, 0.35483871, 0.40860215, 0.17204301, 0.51612903, 0.64516129, 0.13978495], [0.25806452, 0.78494624, 0.86021505, 0.77419355, 0.06451613, 0.86021505, 0.7311828 , 0.83870968, 0.4516129 , 0.82795699], [0.12903226, 0.69892473, 0.95698925, 0.46236559, 0.05376344, 0.67741935, 0.93548387, 0.97849462, 0.07526882, 0.5483871 ], [0. , 0.52688172, 0.2688172 , 0.55913978, 0.87096774, 0.47311828, 0.05376344, 0.1827957 , 0.12903226, 0.7311828 ], [0.44086022, 0.33333333, 0.62365591, 0.59139785, 0.6344086 , 0.96774194, 0.76344086, 0.6344086 , 0.08602151, 0.94623656], [0.79569892, 1. , 0.5483871 , 0.66666667, 0.35483871, 0.17204301, 0.11827957, 0.66666667, 0.10752688, 0.08602151], [0.43010753, 0.06451613, 0.91397849, 0.22580645, 0.6344086 , 0.21505376, 0.05376344, 0.30107527, 0.98924731, 0.33333333], [0.87096774, 0.89247312, 0.67741935, 0.20430108, 0.37634409, 0.16129032, 0.12903226, 0.58064516, 0.83870968, 0.74193548], [0.66666667, 0.76344086, 0.6344086 , 0.4516129 , 0.7311828 , 0.82795699, 0.77419355, 0.66666667, 0.10752688, 0.47311828]]) from sklearn import preprocessing A_scaled = preprocessing . MinMaxScaler() . fit(A) . transform(A) A_scaled C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler. warnings.warn(msg, DataConversionWarning) array([[0.66666667, 0.32183908, 0.390625 , 0.8490566 , 0.59210526, 0. , 0.96341463, 0.77027027, 0.69411765, 0.8 ], [0.38271605, 0.70114943, 0.609375 , 0.22641509, 0.36842105, 0.38823529, 0.13414634, 0.41891892, 0.62352941, 0.0625 ], [0.2962963 , 0.77011494, 0.859375 , 1. , 0.01315789, 0.88235294, 0.76829268, 0.82432432, 0.41176471, 0.8625 ], [0.14814815, 0.67816092, 1. , 0.45283019, 0. , 0.68235294, 1. , 1. , 0. , 0.5375 ], [0. , 0.49425287, 0. , 0.62264151, 1. , 0.45882353, 0. , 0. , 0.05882353, 0.75 ], [0.50617284, 0.28735632, 0.515625 , 0.67924528, 0.71052632, 1. , 0.80487805, 0.56756757, 0.01176471, 1. ], [0.91358025, 1. , 0.40625 , 0.81132075, 0.36842105, 0.12941176, 0.07317073, 0.60810811, 0.03529412, 0. ], [0.49382716, 0. , 0.9375 , 0.03773585, 0.71052632, 0.17647059, 0. , 0.14864865, 1. , 0.2875 ], [1. , 0.88505747, 0.59375 , 0. , 0.39473684, 0.11764706, 0.08536585, 0.5 , 0.83529412, 0.7625 ], [0.7654321 , 0.74712644, 0.53125 , 0.43396226, 0.82894737, 0.84705882, 0.81707317, 0.60810811, 0.03529412, 0.45 ]]) Standard Scalar A = ( 100 * np . random . rand( 10 , 10 )) . astype( int ) A array([[17, 26, 40, 45, 98, 42, 77, 67, 1, 98], [ 8, 52, 68, 2, 6, 89, 76, 44, 2, 4], [30, 57, 9, 48, 57, 26, 71, 42, 49, 0], [13, 18, 65, 81, 36, 2, 25, 7, 80, 18], [17, 88, 95, 57, 67, 38, 12, 78, 21, 64], [75, 41, 8, 72, 47, 77, 13, 98, 56, 71], [96, 14, 50, 87, 24, 9, 48, 25, 11, 25], [68, 16, 94, 30, 23, 41, 22, 96, 93, 33], [10, 72, 84, 29, 59, 40, 86, 31, 82, 97], [72, 33, 16, 78, 12, 6, 63, 25, 28, 24]]) A_ss = A - A . mean() / A . std() A_ss array([[15.48180308, 24.48180308, 38.48180308, 43.48180308, 96.48180308, 40.48180308, 75.48180308, 65.48180308, -0.51819692, 96.48180308], [ 6.48180308, 50.48180308, 66.48180308, 0.48180308, 4.48180308, 87.48180308, 74.48180308, 42.48180308, 0.48180308, 2.48180308], [28.48180308, 55.48180308, 7.48180308, 46.48180308, 55.48180308, 24.48180308, 69.48180308, 40.48180308, 47.48180308, -1.51819692], [11.48180308, 16.48180308, 63.48180308, 79.48180308, 34.48180308, 0.48180308, 23.48180308, 5.48180308, 78.48180308, 16.48180308], [15.48180308, 86.48180308, 93.48180308, 55.48180308, 65.48180308, 36.48180308, 10.48180308, 76.48180308, 19.48180308, 62.48180308], [73.48180308, 39.48180308, 6.48180308, 70.48180308, 45.48180308, 75.48180308, 11.48180308, 96.48180308, 54.48180308, 69.48180308], [94.48180308, 12.48180308, 48.48180308, 85.48180308, 22.48180308, 7.48180308, 46.48180308, 23.48180308, 9.48180308, 23.48180308], [66.48180308, 14.48180308, 92.48180308, 28.48180308, 21.48180308, 39.48180308, 20.48180308, 94.48180308, 91.48180308, 31.48180308], [ 8.48180308, 70.48180308, 82.48180308, 27.48180308, 57.48180308, 38.48180308, 84.48180308, 29.48180308, 80.48180308, 95.48180308], [70.48180308, 31.48180308, 14.48180308, 76.48180308, 10.48180308, 4.48180308, 61.48180308, 23.48180308, 26.48180308, 22.48180308]]) from sklearn import preprocessing A_scaled = preprocessing . StandardScaler() . fit(A) . transform(A) A_scaled C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler. warnings.warn(msg, DataConversionWarning) C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler. warnings.warn(msg, DataConversionWarning) array([[-0.74717972, -0.65678879, -0.40228321, -0.30436663, 2.05218778, 0.18323502, 1.00930339, 0.52228691, -1.25857909, 1.57983042], [-1.03212114, 0.43088692, 0.47088965, -1.96104578, -1.37433265, 1.90564425, 0.97286644, -0.24284678, -1.22810502, -1.14002415], [-0.33559767, 0.64005532, -1.3690103 , -0.18878437, 0.5251515 , -0.40311705, 0.79068171, -0.30938014, 0.20417627, -1.25576265], [-0.87382035, -0.99145824, 0.37733541, 1.08262056, -0.25698903, -1.28264517, -0.88541777, -1.47371401, 1.14887244, -0.73493943], [-0.74717972, 1.93689944, 1.31287775, 0.15796243, 0.89759938, 0.036647 , -1.35909806, 0.88822041, -0.64909769, 0.59605324], [ 1.08910942, -0.02928358, -1.40019504, 0.73587376, 0.15270363, 1.46588019, -1.32266112, 1.55355405, 0.41749476, 0.7985956 ], [ 1.75397273, -1.15879297, -0.09043576, 1.31378509, -0.70392648, -1.02611613, -0.04736803, -0.87491373, -0.95383839, -0.53239707], [ 0.86748832, -1.07512561, 1.28169301, -0.88227796, -0.74117127, 0.14658802, -0.99472861, 1.48702068, 1.54503535, -0.30092008], [-0.96880082, 1.26756054, 0.96984556, -0.92080539, 0.59964108, 0.10994101, 1.33723589, -0.67531364, 1.20982058, 1.5508958 ], [ 0.99412895, -0.36395303, -1.15071708, 0.96703829, -1.15086393, -1.13605715, 0.49918615, -0.87491373, -0.4357792 , -0.56133169]]) Baye's Theorem: An important philosophy in Machine Learning and Statistics. Important Topics: 1. Bayesian Classifier : Example of Machine Learning in Bayesian theory 2. Boltzman Machine : Example of Machine Learning for Physicist. 4. Quantum Boltzman Machine : Fusion of Machine Learning and Quantum Physics 5. Markove Random Field : Is there a Field Theory in Machine Learning? A simple demo illustration Baye's theorm A = ( 100 * np . random . rand( 10 , 4 )) . astype( int ) A array([[38, 15, 66, 92], [16, 60, 96, 18], [18, 80, 14, 60], [88, 41, 31, 33], [88, 95, 24, 98], [39, 64, 97, 76], [17, 20, 1, 87], [90, 34, 2, 70], [81, 10, 93, 86], [60, 56, 3, 63]]) df = pd . DataFrame(A, columns = [ 'bud' , 'green' , 'ripen' , 'rotten' ],\\ index = [ 'apple' , 'guava' , 'mango' , 'orange' , 'banana' , 'pear' , 'papaya' , 'strwberrey' , 'grape' , 'melon' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bud green ripen rotten apple 38 15 66 92 guava 16 60 96 18 mango 18 80 14 60 orange 88 41 31 33 banana 88 95 24 98 pear 39 64 97 76 papaya 17 20 1 87 strwberrey 90 34 2 70 grape 81 10 93 86 melon 60 56 3 63 df . sum() bud 535 green 475 ripen 427 rotten 683 dtype: int64 sum (df . sum()) 2120 df . loc[ 'apple' , 'bud' ] 38 df . loc[ 'apple' ,:] bud 38 green 15 ripen 66 rotten 92 Name: apple, dtype: int32 df . loc[:, 'ripen' ] apple 66 guava 96 mango 14 orange 31 banana 24 pear 97 papaya 1 strwberrey 2 grape 93 melon 3 Name: ripen, dtype: int32 P(ripen|apple) = ? df . loc[ 'apple' , 'ripen' ] / df . loc[ 'apple' ,:] . sum() 0.3127962085308057 P(apple|ripen) = ? df . loc[ 'apple' , 'ripen' ] / df . loc[:, 'ripen' ] . sum() 0.15456674473067916 Implementing Baye's theorem $P(apple|ripen) = \\frac{p(ripen|apple)*p(apple)}{p(ripen)}$ p_apple = df . loc[ 'apple' ,:] . sum() / sum (df . sum()) p_ripen_given_apple = df . loc[ 'apple' , 'ripen' ] / df . loc[ 'apple' ,:] . sum() p_ripen = df . loc[:, 'ripen' ] . sum() / sum (df . sum()) p_apple_given_ripen = (p_ripen_given_apple * p_apple) / p_ripen p_apple_given_ripen 0.15456674473067916 Mini Assignment: Generate a randum array of size 10 by 10 and convert it to pandas dataframe with column name and row name of your interest Perform feature scaling (minimax scalar and standard scalar) by direct calculation of minimunm, maximum, mean, standard deviation from dataframe/array Implement Scikit-learn preprocessing pipeline to perform feature scaling","title":"Statistics"},{"location":"Statistics/#satistics-introduction","text":"https://numpy.org/doc/stable/reference/routines.statistics.html import numpy as np import pandas as pd","title":"Satistics - Introduction"},{"location":"Statistics/#1-simple-statistical-exploration","text":"A = np . random . rand( 10 , 10 ) A array([[7.16622070e-01, 8.67937006e-01, 8.56064808e-01, 8.57568350e-01, 3.67796230e-01, 2.14346363e-01, 6.31599734e-01, 3.57119296e-01, 4.35844862e-01, 2.35483572e-01], [7.22209004e-01, 9.63310994e-01, 8.16973750e-01, 8.06853501e-01, 6.89970795e-01, 5.83418094e-01, 2.65411361e-02, 7.51624520e-01, 9.59391711e-02, 4.15581862e-01], [8.57467652e-01, 9.63049501e-01, 7.90326873e-01, 7.95018533e-01, 7.18152201e-01, 4.38823904e-01, 6.45775017e-01, 7.60717860e-01, 9.24224445e-01, 3.13183949e-01], [3.83836565e-01, 8.56498268e-01, 5.93058237e-01, 2.46908006e-01, 3.10960918e-01, 6.21511447e-01, 2.91997569e-01, 8.13963097e-01, 2.01951380e-01, 5.79517413e-01], [8.41978469e-02, 3.81861596e-01, 1.24320969e-01, 1.92482070e-01, 4.70194470e-01, 9.69335675e-01, 2.07682914e-01, 8.99462218e-01, 3.56681685e-01, 2.10451446e-01], [2.50727875e-01, 4.45871405e-01, 4.75621851e-01, 1.76392569e-01, 4.44494388e-01, 1.97214743e-01, 2.23416686e-01, 9.79333246e-01, 2.13115739e-01, 3.26530393e-01], [6.18496895e-01, 8.46193255e-01, 9.20304738e-01, 4.49655661e-01, 6.66495695e-01, 7.23878345e-01, 4.71483753e-01, 3.35079701e-01, 2.80457713e-01, 4.17035078e-01], [3.74931699e-01, 1.64469071e-02, 8.24518210e-01, 4.40508140e-01, 3.04546586e-01, 9.61964988e-01, 9.13281778e-01, 1.08067818e-01, 3.52596222e-01, 9.71936660e-01], [6.24254898e-01, 6.83181203e-01, 7.55043126e-01, 5.46638933e-01, 1.40146074e-01, 8.45252325e-02, 2.54143985e-01, 4.85118512e-04, 8.08711713e-01, 9.33249065e-01], [4.10573251e-01, 6.30441143e-01, 6.04971277e-01, 4.07647377e-01, 8.25351233e-01, 5.19909474e-01, 9.76184695e-02, 8.11742544e-01, 6.04160360e-01, 4.48347194e-01]]) A . shape (10, 10) A . max() 0.9793332463866128 A . min() 0.00048511851159238617 A . mean() 0.523381592476641 np . median(A) 0.47355280212201306 A . std() 0.2772531462004282 np . cov(A) array([[ 0.06873698, 0.03278827, 0.03668793, -0.00331421, -0.05083651, -0.00850994, 0.02390898, -0.02626347, 0.03131505, -0.01397033], [ 0.03278827, 0.09816338, 0.01792033, 0.04059124, 0.01004886, 0.02993195, 0.03987967, -0.05153719, -0.00568755, 0.03430207], [ 0.03668793, 0.01792033, 0.04244191, -0.00450095, -0.0155604 , 0.00718968, 0.00519803, -0.05720041, 0.00948249, 0.01091863], [-0.00331421, 0.04059124, -0.00450095, 0.05553323, 0.03406878, 0.03667575, 0.01972246, -0.01604353, -0.00940025, 0.02010751], [-0.05083651, 0.01004886, -0.0155604 , 0.03406878, 0.09681987, 0.03356765, -0.00601017, -0.01500941, -0.0759788 , 0.0324855 ], [-0.00850994, 0.02993195, 0.00718968, 0.03667575, 0.03356765, 0.0581328 , -0.00229493, -0.04089154, -0.03034172, 0.03377649], [ 0.02390898, 0.03987967, 0.00519803, 0.01972246, -0.00601017, -0.00229493, 0.04676247, 0.00605692, 0.00312176, 0.00634826], [-0.02626347, -0.05153719, -0.05720041, -0.01604353, -0.01500941, -0.04089154, 0.00605692, 0.13017446, 0.01244017, -0.04495907], [ 0.03131505, -0.00568755, 0.00948249, -0.00940025, -0.0759788 , -0.03034172, 0.00312176, 0.01244017, 0.11204687, -0.01455545], [-0.01397033, 0.03430207, 0.01091863, 0.02010751, 0.0324855 , 0.03377649, 0.00634826, -0.04495907, -0.01455545, 0.04560073]])","title":"1. Simple Statistical Exploration"},{"location":"Statistics/#mean-and-std-of-row","text":"np . mean(A,axis = 0 ) array([0.50433178, 0.66547913, 0.67612038, 0.49196731, 0.49381086, 0.53149283, 0.3763541 , 0.58175954, 0.42736833, 0.48513166]) np . std(A,axis = 0 ) array([0.22924647, 0.28962689, 0.22578381, 0.24259586, 0.21017855, 0.2908006 , 0.26627669, 0.33170298, 0.25736132, 0.25495454])","title":"Mean and std of row"},{"location":"Statistics/#mean-and-std-of-col","text":"np . mean(A,axis = 1 ) array([0.55403823, 0.58724228, 0.72067399, 0.49002029, 0.38966709, 0.37327189, 0.57290808, 0.5268799 , 0.48303793, 0.53607623]) np . std(A,axis = 1 ) array([0.2487233 , 0.29723231, 0.19544236, 0.22356186, 0.29519126, 0.2287346 , 0.20514927, 0.34228207, 0.31755658, 0.20258494])","title":"Mean and std of col"},{"location":"Statistics/#2-feature-scaling","text":"Mini-max scalar A = ( 100 * np . random . rand( 10 , 10 )) . astype( int ) A array([[59, 39, 55, 69, 55, 10, 89, 79, 71, 77], [36, 72, 69, 36, 38, 43, 21, 53, 65, 18], [29, 78, 85, 77, 11, 85, 73, 83, 47, 82], [17, 70, 94, 48, 10, 68, 92, 96, 12, 56], [ 5, 54, 30, 57, 86, 49, 10, 22, 17, 73], [46, 36, 63, 60, 64, 95, 76, 64, 13, 93], [79, 98, 56, 67, 38, 21, 16, 67, 15, 13], [45, 11, 90, 26, 64, 25, 10, 33, 97, 36], [86, 88, 68, 24, 40, 20, 17, 59, 83, 74], [67, 76, 64, 47, 73, 82, 77, 67, 15, 49]]) A_mm = (A - A . min()) / (A . max() - A . min()) A_mm array([[0.58064516, 0.3655914 , 0.53763441, 0.68817204, 0.53763441, 0.05376344, 0.90322581, 0.79569892, 0.70967742, 0.77419355], [0.33333333, 0.72043011, 0.68817204, 0.33333333, 0.35483871, 0.40860215, 0.17204301, 0.51612903, 0.64516129, 0.13978495], [0.25806452, 0.78494624, 0.86021505, 0.77419355, 0.06451613, 0.86021505, 0.7311828 , 0.83870968, 0.4516129 , 0.82795699], [0.12903226, 0.69892473, 0.95698925, 0.46236559, 0.05376344, 0.67741935, 0.93548387, 0.97849462, 0.07526882, 0.5483871 ], [0. , 0.52688172, 0.2688172 , 0.55913978, 0.87096774, 0.47311828, 0.05376344, 0.1827957 , 0.12903226, 0.7311828 ], [0.44086022, 0.33333333, 0.62365591, 0.59139785, 0.6344086 , 0.96774194, 0.76344086, 0.6344086 , 0.08602151, 0.94623656], [0.79569892, 1. , 0.5483871 , 0.66666667, 0.35483871, 0.17204301, 0.11827957, 0.66666667, 0.10752688, 0.08602151], [0.43010753, 0.06451613, 0.91397849, 0.22580645, 0.6344086 , 0.21505376, 0.05376344, 0.30107527, 0.98924731, 0.33333333], [0.87096774, 0.89247312, 0.67741935, 0.20430108, 0.37634409, 0.16129032, 0.12903226, 0.58064516, 0.83870968, 0.74193548], [0.66666667, 0.76344086, 0.6344086 , 0.4516129 , 0.7311828 , 0.82795699, 0.77419355, 0.66666667, 0.10752688, 0.47311828]]) from sklearn import preprocessing A_scaled = preprocessing . MinMaxScaler() . fit(A) . transform(A) A_scaled C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler. warnings.warn(msg, DataConversionWarning) array([[0.66666667, 0.32183908, 0.390625 , 0.8490566 , 0.59210526, 0. , 0.96341463, 0.77027027, 0.69411765, 0.8 ], [0.38271605, 0.70114943, 0.609375 , 0.22641509, 0.36842105, 0.38823529, 0.13414634, 0.41891892, 0.62352941, 0.0625 ], [0.2962963 , 0.77011494, 0.859375 , 1. , 0.01315789, 0.88235294, 0.76829268, 0.82432432, 0.41176471, 0.8625 ], [0.14814815, 0.67816092, 1. , 0.45283019, 0. , 0.68235294, 1. , 1. , 0. , 0.5375 ], [0. , 0.49425287, 0. , 0.62264151, 1. , 0.45882353, 0. , 0. , 0.05882353, 0.75 ], [0.50617284, 0.28735632, 0.515625 , 0.67924528, 0.71052632, 1. , 0.80487805, 0.56756757, 0.01176471, 1. ], [0.91358025, 1. , 0.40625 , 0.81132075, 0.36842105, 0.12941176, 0.07317073, 0.60810811, 0.03529412, 0. ], [0.49382716, 0. , 0.9375 , 0.03773585, 0.71052632, 0.17647059, 0. , 0.14864865, 1. , 0.2875 ], [1. , 0.88505747, 0.59375 , 0. , 0.39473684, 0.11764706, 0.08536585, 0.5 , 0.83529412, 0.7625 ], [0.7654321 , 0.74712644, 0.53125 , 0.43396226, 0.82894737, 0.84705882, 0.81707317, 0.60810811, 0.03529412, 0.45 ]]) Standard Scalar A = ( 100 * np . random . rand( 10 , 10 )) . astype( int ) A array([[17, 26, 40, 45, 98, 42, 77, 67, 1, 98], [ 8, 52, 68, 2, 6, 89, 76, 44, 2, 4], [30, 57, 9, 48, 57, 26, 71, 42, 49, 0], [13, 18, 65, 81, 36, 2, 25, 7, 80, 18], [17, 88, 95, 57, 67, 38, 12, 78, 21, 64], [75, 41, 8, 72, 47, 77, 13, 98, 56, 71], [96, 14, 50, 87, 24, 9, 48, 25, 11, 25], [68, 16, 94, 30, 23, 41, 22, 96, 93, 33], [10, 72, 84, 29, 59, 40, 86, 31, 82, 97], [72, 33, 16, 78, 12, 6, 63, 25, 28, 24]]) A_ss = A - A . mean() / A . std() A_ss array([[15.48180308, 24.48180308, 38.48180308, 43.48180308, 96.48180308, 40.48180308, 75.48180308, 65.48180308, -0.51819692, 96.48180308], [ 6.48180308, 50.48180308, 66.48180308, 0.48180308, 4.48180308, 87.48180308, 74.48180308, 42.48180308, 0.48180308, 2.48180308], [28.48180308, 55.48180308, 7.48180308, 46.48180308, 55.48180308, 24.48180308, 69.48180308, 40.48180308, 47.48180308, -1.51819692], [11.48180308, 16.48180308, 63.48180308, 79.48180308, 34.48180308, 0.48180308, 23.48180308, 5.48180308, 78.48180308, 16.48180308], [15.48180308, 86.48180308, 93.48180308, 55.48180308, 65.48180308, 36.48180308, 10.48180308, 76.48180308, 19.48180308, 62.48180308], [73.48180308, 39.48180308, 6.48180308, 70.48180308, 45.48180308, 75.48180308, 11.48180308, 96.48180308, 54.48180308, 69.48180308], [94.48180308, 12.48180308, 48.48180308, 85.48180308, 22.48180308, 7.48180308, 46.48180308, 23.48180308, 9.48180308, 23.48180308], [66.48180308, 14.48180308, 92.48180308, 28.48180308, 21.48180308, 39.48180308, 20.48180308, 94.48180308, 91.48180308, 31.48180308], [ 8.48180308, 70.48180308, 82.48180308, 27.48180308, 57.48180308, 38.48180308, 84.48180308, 29.48180308, 80.48180308, 95.48180308], [70.48180308, 31.48180308, 14.48180308, 76.48180308, 10.48180308, 4.48180308, 61.48180308, 23.48180308, 26.48180308, 22.48180308]]) from sklearn import preprocessing A_scaled = preprocessing . StandardScaler() . fit(A) . transform(A) A_scaled C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler. warnings.warn(msg, DataConversionWarning) C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler. warnings.warn(msg, DataConversionWarning) array([[-0.74717972, -0.65678879, -0.40228321, -0.30436663, 2.05218778, 0.18323502, 1.00930339, 0.52228691, -1.25857909, 1.57983042], [-1.03212114, 0.43088692, 0.47088965, -1.96104578, -1.37433265, 1.90564425, 0.97286644, -0.24284678, -1.22810502, -1.14002415], [-0.33559767, 0.64005532, -1.3690103 , -0.18878437, 0.5251515 , -0.40311705, 0.79068171, -0.30938014, 0.20417627, -1.25576265], [-0.87382035, -0.99145824, 0.37733541, 1.08262056, -0.25698903, -1.28264517, -0.88541777, -1.47371401, 1.14887244, -0.73493943], [-0.74717972, 1.93689944, 1.31287775, 0.15796243, 0.89759938, 0.036647 , -1.35909806, 0.88822041, -0.64909769, 0.59605324], [ 1.08910942, -0.02928358, -1.40019504, 0.73587376, 0.15270363, 1.46588019, -1.32266112, 1.55355405, 0.41749476, 0.7985956 ], [ 1.75397273, -1.15879297, -0.09043576, 1.31378509, -0.70392648, -1.02611613, -0.04736803, -0.87491373, -0.95383839, -0.53239707], [ 0.86748832, -1.07512561, 1.28169301, -0.88227796, -0.74117127, 0.14658802, -0.99472861, 1.48702068, 1.54503535, -0.30092008], [-0.96880082, 1.26756054, 0.96984556, -0.92080539, 0.59964108, 0.10994101, 1.33723589, -0.67531364, 1.20982058, 1.5508958 ], [ 0.99412895, -0.36395303, -1.15071708, 0.96703829, -1.15086393, -1.13605715, 0.49918615, -0.87491373, -0.4357792 , -0.56133169]])","title":"2. Feature Scaling:"},{"location":"Statistics/#bayes-theorem","text":"An important philosophy in Machine Learning and Statistics. Important Topics: 1. Bayesian Classifier : Example of Machine Learning in Bayesian theory 2. Boltzman Machine : Example of Machine Learning for Physicist. 4. Quantum Boltzman Machine : Fusion of Machine Learning and Quantum Physics 5. Markove Random Field : Is there a Field Theory in Machine Learning?","title":"Baye's Theorem:"},{"location":"Statistics/#a-simple-demo-illustration-bayes-theorm","text":"A = ( 100 * np . random . rand( 10 , 4 )) . astype( int ) A array([[38, 15, 66, 92], [16, 60, 96, 18], [18, 80, 14, 60], [88, 41, 31, 33], [88, 95, 24, 98], [39, 64, 97, 76], [17, 20, 1, 87], [90, 34, 2, 70], [81, 10, 93, 86], [60, 56, 3, 63]]) df = pd . DataFrame(A, columns = [ 'bud' , 'green' , 'ripen' , 'rotten' ],\\ index = [ 'apple' , 'guava' , 'mango' , 'orange' , 'banana' , 'pear' , 'papaya' , 'strwberrey' , 'grape' , 'melon' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bud green ripen rotten apple 38 15 66 92 guava 16 60 96 18 mango 18 80 14 60 orange 88 41 31 33 banana 88 95 24 98 pear 39 64 97 76 papaya 17 20 1 87 strwberrey 90 34 2 70 grape 81 10 93 86 melon 60 56 3 63 df . sum() bud 535 green 475 ripen 427 rotten 683 dtype: int64 sum (df . sum()) 2120 df . loc[ 'apple' , 'bud' ] 38 df . loc[ 'apple' ,:] bud 38 green 15 ripen 66 rotten 92 Name: apple, dtype: int32 df . loc[:, 'ripen' ] apple 66 guava 96 mango 14 orange 31 banana 24 pear 97 papaya 1 strwberrey 2 grape 93 melon 3 Name: ripen, dtype: int32 P(ripen|apple) = ? df . loc[ 'apple' , 'ripen' ] / df . loc[ 'apple' ,:] . sum() 0.3127962085308057 P(apple|ripen) = ? df . loc[ 'apple' , 'ripen' ] / df . loc[:, 'ripen' ] . sum() 0.15456674473067916","title":"A simple demo illustration Baye's theorm"},{"location":"Statistics/#implementing-bayes-theorem","text":"$P(apple|ripen) = \\frac{p(ripen|apple)*p(apple)}{p(ripen)}$ p_apple = df . loc[ 'apple' ,:] . sum() / sum (df . sum()) p_ripen_given_apple = df . loc[ 'apple' , 'ripen' ] / df . loc[ 'apple' ,:] . sum() p_ripen = df . loc[:, 'ripen' ] . sum() / sum (df . sum()) p_apple_given_ripen = (p_ripen_given_apple * p_apple) / p_ripen p_apple_given_ripen 0.15456674473067916","title":"Implementing Baye's theorem"},{"location":"Statistics/#mini-assignment","text":"Generate a randum array of size 10 by 10 and convert it to pandas dataframe with column name and row name of your interest Perform feature scaling (minimax scalar and standard scalar) by direct calculation of minimunm, maximum, mean, standard deviation from dataframe/array Implement Scikit-learn preprocessing pipeline to perform feature scaling","title":"Mini Assignment:"},{"location":"Algebra/code/","text":"Algebra: import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set() 1. Dot Product $$ u = M.v, C = A.B $$ v = np . random . rand( 5 ) v array([ 0.61880652, 0.16277711, 0.77725885, 0.39357105, 0.72518988]) M = np . random . rand( 5 , 5 ) M array([[ 0.44722927, 0.75871176, 0.74971577, 0.83610699, 0.18085266], [ 0.00121306, 0.84271476, 0.65241612, 0.27094445, 0.63364293], [ 0.07906587, 0.85301928, 0.62214839, 0.59205861, 0.70427723], [ 0.88162665, 0.76250728, 0.15724579, 0.63985535, 0.04617605], [ 0.57250482, 0.6551876 , 0.562632 , 0.29052545, 0.29159418]]) u = np . dot(M,v) u array([ 1.44319253, 1.21116885, 1.41510068, 1.07721068, 1.22403352]) M = np . random . rand( 5 , 5 ) N = np . random . rand( 5 , 5 ) np . dot(M,N) array([[2.04187416, 1.27819139, 1.95077838, 1.61949482, 1.17248351], [1.7066333 , 0.83570007, 1.52109089, 1.39001526, 0.71187379], [1.44468595, 0.79383607, 1.35776556, 1.11653487, 0.79871042], [1.51494078, 1.10723018, 1.52666191, 1.23628874, 1.00665592], [0.51859666, 0.65862101, 0.61337647, 0.40747731, 0.66203929]]) 2. Kronecker Product I = np . eye( 3 ) O = np . ones(( 3 , 3 )) I,O (array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]), array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])) KIO = np . kron(I,O) KIO array([[1., 1., 1., 0., 0., 0., 0., 0., 0.], [1., 1., 1., 0., 0., 0., 0., 0., 0.], [1., 1., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 1., 1.], [0., 0., 0., 0., 0., 0., 1., 1., 1.], [0., 0., 0., 0., 0., 0., 1., 1., 1.]]) KOI = np . kron(O,I) KOI array([[1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 1., 0., 0., 1., 0., 0., 1., 0.], [0., 0., 1., 0., 0., 1., 0., 0., 1.], [1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 1., 0., 0., 1., 0., 0., 1., 0.], [0., 0., 1., 0., 0., 1., 0., 0., 1.], [1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 1., 0., 0., 1., 0., 0., 1., 0.], [0., 0., 1., 0., 0., 1., 0., 0., 1.]]) 3. Transpose of a matrix plt . figure(figsize = [ 15 , 6 ]) plt . subplot( 1 , 2 , 1 ) sns . heatmap(M, annot = True ) plt . subplot( 1 , 2 , 2 ) sns . heatmap(M . T, annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x11ce902b0> 4. Solve Matrix Equation : $$\\large{Ax = b}$$ $$2x_1 + 3x_2 +5x_3 + 4x_4 +2x_5 = 19$$ $$5x_1 + 4x_2 +2x_3 + 6x_4 +1x_5 = 23$$ $$9x_1 + 2x_2 +4x_3 + 5x_4 +2x_5 = 45$$ $$1x_1 + 9x_2 +6x_3 + 9x_4 +3x_5 = 56$$ $49x_1 + 7x_2 +8x_3 + 4x_4 +x_5 = 12$$ import numpy.linalg as LA A = np . array([[ 2 , 3 , 5 , 4 , 2 ],[ 5 , 4 , 2 , 6 , 1 ],[ 9 , 2 , 4 , 5 , 2 ],[ 1 , 9 , 6 , 9 , 3 ],[ 9 , 7 , 8 , 4 , 1 ]]) b = np . array([ 19 , 23 , 45 , 56 , 12 ]) x = LA . solve(A,b) x array([ 4.57281553, 9.84789644, -12.78964401, -8.95145631, 40.03236246]) np . dot(A,x) array([19., 23., 45., 56., 12.]) 5. Inverse LA . det(A) -927.0000000000007 AI = LA . inv(A) AI array([[-0.21359223, -0.09708738, 0.17152104, 0.04854369, 0.03559871], [-0.57605178, -0.35275081, 0.18985976, 0.34304207, 0.09600863], [ 0.60517799, 0.27508091, -0.3193096 , -0.30420712, 0.06580367], [ 0.52427184, 0.60194175, -0.33009709, -0.30097087, -0.08737864], [-0.98381877, -1.26537217, 1.0021575 , 0.79935275, -0.16936354]]) np . dot(AI,A) array([[ 1.00000000e+00, 6.24500451e-17, 1.66533454e-16, 1.66533454e-16, 6.93889390e-17], [-9.71445147e-17, 1.00000000e+00, 0.00000000e+00, 3.33066907e-16, 1.80411242e-16], [ 4.57966998e-16, -2.91433544e-16, 1.00000000e+00, -2.22044605e-16, -1.52655666e-16], [-6.52256027e-16, -1.38777878e-17, -5.55111512e-16, 1.00000000e+00, -2.08166817e-16], [ 5.82867088e-16, -5.82867088e-16, 2.22044605e-16, 9.99200722e-16, 1.00000000e+00]]) 6. Singular Value Decomposition: Find Principle Axis $$\\large{A =PDQ}$$ $$X^{T}AX = 4x_1^2 + 6x_2^2 + 8x_3^2+ 8x_1x_2 + 6x_2x_3 - 16x_3x_1$$ $$ X^{T}AX = \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}\\begin{pmatrix} 4 & 4 & -8 \\\\ 4 & 3 & 3 \\\\ -8 & 3 & 4 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$$ $$X^{T}AX = 12.06 x_1^2 + 6.56 x_2^2 + 5.5 x_3^2 $$ A = np . array([[ 4 , 4 , - 8 ],[ 4 , 3 , 3 ],[ - 8 , 3 , 4 ]]) P,D,Q = LA . svd(A) D array([12.06630095, 6.56787841, 5.50157746]) P array([[-0.72405638, 0.63996989, 0.25725649], [-0.09331565, -0.46043612, 0.88277447], [ 0.68339926, 0.61517243, 0.39310092]]) Q array([[-0.72405638, -0.09331565, 0.68339926], [-0.63996989, 0.46043612, -0.61517243], [ 0.25725649, 0.88277447, 0.39310092]]) LA . inv(P) array([[-0.72405638, -0.09331565, 0.68339926], [ 0.63996989, -0.46043612, 0.61517243], [ 0.25725649, 0.88277447, 0.39310092]]) AN = np . matmul(np . matmul(P, np . diag(D)), Q) AN array([[ 4., 4., -8.], [ 4., 3., 3.], [-8., 3., 4.]]) A array([[ 4, 4, -8], [ 4, 3, 3], [-8, 3, 4]]) LA . det(P),LA . det(Q) (1.0000000000000004, -1.0000000000000002)","title":"Algebra"},{"location":"Algebra/code/#algebra","text":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set()","title":"Algebra:"},{"location":"Algebra/code/#1-dot-product","text":"$$ u = M.v, C = A.B $$ v = np . random . rand( 5 ) v array([ 0.61880652, 0.16277711, 0.77725885, 0.39357105, 0.72518988]) M = np . random . rand( 5 , 5 ) M array([[ 0.44722927, 0.75871176, 0.74971577, 0.83610699, 0.18085266], [ 0.00121306, 0.84271476, 0.65241612, 0.27094445, 0.63364293], [ 0.07906587, 0.85301928, 0.62214839, 0.59205861, 0.70427723], [ 0.88162665, 0.76250728, 0.15724579, 0.63985535, 0.04617605], [ 0.57250482, 0.6551876 , 0.562632 , 0.29052545, 0.29159418]]) u = np . dot(M,v) u array([ 1.44319253, 1.21116885, 1.41510068, 1.07721068, 1.22403352]) M = np . random . rand( 5 , 5 ) N = np . random . rand( 5 , 5 ) np . dot(M,N) array([[2.04187416, 1.27819139, 1.95077838, 1.61949482, 1.17248351], [1.7066333 , 0.83570007, 1.52109089, 1.39001526, 0.71187379], [1.44468595, 0.79383607, 1.35776556, 1.11653487, 0.79871042], [1.51494078, 1.10723018, 1.52666191, 1.23628874, 1.00665592], [0.51859666, 0.65862101, 0.61337647, 0.40747731, 0.66203929]])","title":"1. Dot Product"},{"location":"Algebra/code/#2-kronecker-product","text":"I = np . eye( 3 ) O = np . ones(( 3 , 3 )) I,O (array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]), array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])) KIO = np . kron(I,O) KIO array([[1., 1., 1., 0., 0., 0., 0., 0., 0.], [1., 1., 1., 0., 0., 0., 0., 0., 0.], [1., 1., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 1., 1.], [0., 0., 0., 0., 0., 0., 1., 1., 1.], [0., 0., 0., 0., 0., 0., 1., 1., 1.]]) KOI = np . kron(O,I) KOI array([[1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 1., 0., 0., 1., 0., 0., 1., 0.], [0., 0., 1., 0., 0., 1., 0., 0., 1.], [1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 1., 0., 0., 1., 0., 0., 1., 0.], [0., 0., 1., 0., 0., 1., 0., 0., 1.], [1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 1., 0., 0., 1., 0., 0., 1., 0.], [0., 0., 1., 0., 0., 1., 0., 0., 1.]])","title":"2. Kronecker Product"},{"location":"Algebra/code/#3-transpose-of-a-matrix","text":"plt . figure(figsize = [ 15 , 6 ]) plt . subplot( 1 , 2 , 1 ) sns . heatmap(M, annot = True ) plt . subplot( 1 , 2 , 2 ) sns . heatmap(M . T, annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x11ce902b0>","title":"3. Transpose of a matrix"},{"location":"Algebra/code/#4-solve-matrix-equation-largeax-b","text":"$$2x_1 + 3x_2 +5x_3 + 4x_4 +2x_5 = 19$$ $$5x_1 + 4x_2 +2x_3 + 6x_4 +1x_5 = 23$$ $$9x_1 + 2x_2 +4x_3 + 5x_4 +2x_5 = 45$$ $$1x_1 + 9x_2 +6x_3 + 9x_4 +3x_5 = 56$$ $49x_1 + 7x_2 +8x_3 + 4x_4 +x_5 = 12$$ import numpy.linalg as LA A = np . array([[ 2 , 3 , 5 , 4 , 2 ],[ 5 , 4 , 2 , 6 , 1 ],[ 9 , 2 , 4 , 5 , 2 ],[ 1 , 9 , 6 , 9 , 3 ],[ 9 , 7 , 8 , 4 , 1 ]]) b = np . array([ 19 , 23 , 45 , 56 , 12 ]) x = LA . solve(A,b) x array([ 4.57281553, 9.84789644, -12.78964401, -8.95145631, 40.03236246]) np . dot(A,x) array([19., 23., 45., 56., 12.])","title":"4. Solve Matrix Equation : $$\\large{Ax = b}$$"},{"location":"Algebra/code/#5-inverse","text":"LA . det(A) -927.0000000000007 AI = LA . inv(A) AI array([[-0.21359223, -0.09708738, 0.17152104, 0.04854369, 0.03559871], [-0.57605178, -0.35275081, 0.18985976, 0.34304207, 0.09600863], [ 0.60517799, 0.27508091, -0.3193096 , -0.30420712, 0.06580367], [ 0.52427184, 0.60194175, -0.33009709, -0.30097087, -0.08737864], [-0.98381877, -1.26537217, 1.0021575 , 0.79935275, -0.16936354]]) np . dot(AI,A) array([[ 1.00000000e+00, 6.24500451e-17, 1.66533454e-16, 1.66533454e-16, 6.93889390e-17], [-9.71445147e-17, 1.00000000e+00, 0.00000000e+00, 3.33066907e-16, 1.80411242e-16], [ 4.57966998e-16, -2.91433544e-16, 1.00000000e+00, -2.22044605e-16, -1.52655666e-16], [-6.52256027e-16, -1.38777878e-17, -5.55111512e-16, 1.00000000e+00, -2.08166817e-16], [ 5.82867088e-16, -5.82867088e-16, 2.22044605e-16, 9.99200722e-16, 1.00000000e+00]])","title":"5. Inverse"},{"location":"Algebra/code/#6-singular-value-decomposition-find-principle-axis","text":"$$\\large{A =PDQ}$$ $$X^{T}AX = 4x_1^2 + 6x_2^2 + 8x_3^2+ 8x_1x_2 + 6x_2x_3 - 16x_3x_1$$ $$ X^{T}AX = \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}\\begin{pmatrix} 4 & 4 & -8 \\\\ 4 & 3 & 3 \\\\ -8 & 3 & 4 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$$ $$X^{T}AX = 12.06 x_1^2 + 6.56 x_2^2 + 5.5 x_3^2 $$ A = np . array([[ 4 , 4 , - 8 ],[ 4 , 3 , 3 ],[ - 8 , 3 , 4 ]]) P,D,Q = LA . svd(A) D array([12.06630095, 6.56787841, 5.50157746]) P array([[-0.72405638, 0.63996989, 0.25725649], [-0.09331565, -0.46043612, 0.88277447], [ 0.68339926, 0.61517243, 0.39310092]]) Q array([[-0.72405638, -0.09331565, 0.68339926], [-0.63996989, 0.46043612, -0.61517243], [ 0.25725649, 0.88277447, 0.39310092]]) LA . inv(P) array([[-0.72405638, -0.09331565, 0.68339926], [ 0.63996989, -0.46043612, 0.61517243], [ 0.25725649, 0.88277447, 0.39310092]]) AN = np . matmul(np . matmul(P, np . diag(D)), Q) AN array([[ 4., 4., -8.], [ 4., 3., 3.], [-8., 3., 4.]]) A array([[ 4, 4, -8], [ 4, 3, 3], [-8, 3, 4]]) LA . det(P),LA . det(Q) (1.0000000000000004, -1.0000000000000002)","title":"6. Singular Value Decomposition: Find Principle Axis"},{"location":"Algebra-CompositeHamiltonian/code/","text":"Algebra: Composite-Hamiltonian and Time Evolution https://numpy.org/doc/stable/reference/routines.linalg.html http://qutip.org/docs/latest/guide/dynamics/dynamics-master.html import numpy as np from qutip import * import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set() Given a Hamiltonian, we can calculate the unitary (non-dissipative) time-evolution of an arbitrary state vector |\u03c80\u27e9 (psi0) using the QuTiP function qutip.sesolve . It evolves the state vector and evaluates the expectation values for a set of operators expt_ops at the points in time in the list times, using an ordinary differential equation solver. Scrodinger Equation: $$i\\hbar\\frac{d}{dt}\\left|\\psi\\right> = H \\left|\\psi\\right>$$ Time evolution: A . Single qubit System Consider a single qubit hamiltonian constructed by implementing pauli x operator: $$H = 2 \\pi \\sigma_x$$ # single qubit hamiltonian H = 2.0 * np . pi * sigmax() H Quantum object: dims = [[2], [2]], shape = (2, 2), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 6.283\\6.283 & 0.0\\\\end{array}\\right)\\end{equation*} Create the initial state of the system #initial state of the single qubit system #psi0 = basis(2,0) psi0 = basis( 2 , 1 ) #psi0 = (1/np.sqrt(2))*basis(2,0)+ (1/np.sqrt(2))*basis(2,1) psi0 Quantum object: dims = [[2], [1]], shape = (2, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}0.0\\1.0\\\\end{array}\\right)\\end{equation*} Create time array to conduct measurement time points during the evolution times = np . linspace( 0.0 , 10.0 , 20 ) times array([ 0. , 0.52631579, 1.05263158, 1.57894737, 2.10526316, 2.63157895, 3.15789474, 3.68421053, 4.21052632, 4.73684211, 5.26315789, 5.78947368, 6.31578947, 6.84210526, 7.36842105, 7.89473684, 8.42105263, 8.94736842, 9.47368421, 10. ]) Simulate the time evolution by implementing sesolve function by supplying hamiltonian H , initial state psi0 , time array times and list of operators [sigmaz(), sigmay()] representing what you want to measure as expectation value of. result = sesolve(H, psi0, times, [sigmaz(), sigmay()]) Explore the result result Result object with sesolve data. -------------------------------- expect = True num_expect = 2, num_collapse = 0 Where are the expectation value of the implemented operator for measurement during time evolution? result . expect [array([-1. , -0.94581561, -0.78913418, -0.54693623, -0.24546751, 0.08260204, 0.40171988, 0.67730341, 0.87948895, 0.9863673 , 0.98635446, 0.87945165, 0.67724451, 0.4016443 , 0.08252413, -0.24554292, -0.54700067, -0.78917951, -0.94583863, -1. ]), array([ 0.00000000e+00, 3.24704211e-01, 6.14220851e-01, 8.37174272e-01, 9.69404818e-01, 9.96582612e-01, 9.15762600e-01, 7.35703806e-01, 4.75919309e-01, 1.64558631e-01, -1.64635573e-01, -4.75988221e-01, -7.35758027e-01, -9.15795752e-01, -9.96589067e-01, -9.69385719e-01, -8.37132169e-01, -6.14162600e-01, -3.24637165e-01, 7.13395936e-05])] 7 Plot the time evolution of the expection values of the implemented operators import matplotlib.pyplot as plt plt . figure(figsize = [ 20 , 4 ]) plt . plot(result . times, result . expect[ 0 ]) plt . plot(result . times, result . expect[ 1 ]) plt . xlabel( 'Time' ) plt . ylabel( 'Expectation values' ) plt . legend(( \"sigmaz\" , \"sigmay\" )) plt . show() Lets repeat the same steps for multi-qubit system. Before that, we need to construct the composite hamiltonian representing the quantum mechanical system. B. Constructing composite Hamiltonians 1. Two coupled qubits Consider a Hamiltonial of a 2 qubits system representated by composite hamiltonian shwn below. The first term represents the contribution of second qubit Z spin operator (sigmaz()) while first qubit remains inactive(identity); the second term represents the contribution of first qubit spin operator while second qubit remains inactive(identity); the third represents the contribution of first and second qubit X spin operator. $H = 0.2(\\sigma_z \\otimes I) + 0.4(I \\otimes \\sigma_z) + \\frac{1}{4} (\\sigma_x \\otimes \\sigma_x)$ H2 = 0.2 * np . pi * tensor(sigmaz(), identity( 2 )) \\ + 0.4 * np . pi * tensor(identity( 2 ),sigmaz()) \\ + 0.25 * np . pi * tensor(sigmax(), sigmax()) H2 Quantum object: dims = [[2, 2], [2, 2]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}1.885 & 0.0 & 0.0 & 0.785\\0.0 & -0.628 & 0.785 & 0.0\\0.0 & 0.785 & 0.628 & 0.0\\0.785 & 0.0 & 0.0 & -1.885\\\\end{array}\\right)\\end{equation*} 2. Three Coupled Qubit Consider a Hamiltonial of a 3 qubits system representated by composite hamiltonian shwn below. The first term represents the contribution of third qubit Z spin operator (sigmaz()) while rest of the qubits remains inactive(identity); the second term represents the contribution of second qubit spin operator while remaining qubits inactive and so on. $H = (\\sigma_z \\otimes I \\otimes I) + (I \\otimes \\sigma_z \\otimes I) + (I \\otimes I \\otimes \\sigma_z ) + 1/2 (\\sigma_x \\otimes \\sigma_x \\otimes I) + 1/4 ( I \\otimes \\sigma_x \\otimes \\sigma_x)$ H3 = (tensor(sigmaz(), identity( 2 ), identity( 2 )) + tensor(identity( 2 ), sigmaz(), identity( 2 )) + tensor(identity( 2 ), identity( 2 ), sigmaz()) + 0.5 * tensor(sigmax(), sigmax(), identity( 2 )) + 0.25 * tensor(identity( 2 ), sigmax(), sigmax())) H3 Quantum object: dims = [[2, 2, 2], [2, 2, 2]], shape = (8, 8), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}3.0 & 0.0 & 0.0 & 0.250 & 0.0 & 0.0 & 0.500 & 0.0\\0.0 & 1.0 & 0.250 & 0.0 & 0.0 & 0.0 & 0.0 & 0.500\\0.0 & 0.250 & 1.0 & 0.0 & 0.500 & 0.0 & 0.0 & 0.0\\0.250 & 0.0 & 0.0 & -1.0 & 0.0 & 0.500 & 0.0 & 0.0\\0.0 & 0.0 & 0.500 & 0.0 & 1.0 & 0.0 & 0.0 & 0.250\\0.0 & 0.0 & 0.0 & 0.500 & 0.0 & -1.0 & 0.250 & 0.0\\0.500 & 0.0 & 0.0 & 0.0 & 0.0 & 0.250 & -1.0 & 0.0\\0.0 & 0.500 & 0.0 & 0.0 & 0.250 & 0.0 & 0.0 & -3.0\\\\end{array}\\right)\\end{equation*} C. Time Evolution with composite Hamiltonian How to generate a initial state vector? This is 2 qubit quantum mechanical system, there are 4 basis states $|00 \\rangle, |01 \\rangle, |10 \\rangle, |11 \\rangle$ in 4 dimentional complex vector space. All operators in 2 qubit system should be 4 by 4 matrices while state vector need to be 4 element column ket(bra) vector. psi0 = basis( 4 , 0 ) psi0 Quantum object: dims = [[4], [1]], shape = (4, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}1.0\\0.0\\0.0\\0.0\\\\end{array}\\right)\\end{equation*} Q: Could we start with random ket vector? - Answer: yes! # Example 1 psi1 = basis( 4 , 1 ) psi1 Quantum object: dims = [[4], [1]], shape = (4, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}0.0\\1.0\\0.0\\0.0\\\\end{array}\\right)\\end{equation*} # Example 2 psi2 = ( 1 / 2 ) * basis( 4 , 0 ) + ( 1 / 2 ) * basis( 4 , 1 ) + ( 1 / 2 ) * basis( 4 , 2 ) + ( 1 / 2 ) * basis( 4 , 3 ) psi2 Quantum object: dims = [[4], [1]], shape = (4, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}0.500\\0.500\\0.500\\0.500\\\\end{array}\\right)\\end{equation*} Lets perform the time evolution with two new operators for measurement ( m1-operator, m2-operator defined below in code block) of size 4 by 4 (why? remember, measurement operator in 2 qubit system also need to be 4 by 4 matrices). # time array times = np . linspace( 0.0 , 10.0 , 100 ) # measurement operator m1_operator = tensor(identity( 2 ), sigmaz()) m2_operator = tensor(sigmax(), sigmay()) # time evolution of state psi2 with hamiltonian H2 to calculate expectation # value of m-operator and m-2 operator at 100 different time points result = sesolve(H2, psi2, times, [m1_operator,m2_operator]) # array holding multiple subarray of results result Result object with sesolve data. -------------------------------- expect = True num_expect = 2, num_collapse = 0 # expectation value of m1 and m2 operator at 20 different time point result . expect [array([0. , 0.01990965, 0.07693353, 0.16338415, 0.26780409, 0.37676296, 0.47694252, 0.55716702, 0.6100466 , 0.63296734, 0.62827241, 0.60261422, 0.56559699, 0.52794726, 0.49952837, 0.48753968, 0.49520857, 0.52119896, 0.55983745, 0.60211928, 0.63732295, 0.65495828, 0.64671514, 0.60807788, 0.5393257 , 0.44574105, 0.33698063, 0.22570382, 0.12567867, 0.04967351, 0.00747984, 0.00439134, 0.04038908, 0.11016689, 0.20399138, 0.30925384, 0.41245688, 0.50130765, 0.56657378, 0.60339885, 0.61186652, 0.5967288 , 0.56635478, 0.53108657, 0.50128824, 0.48542398, 0.48849536, 0.51110591, 0.54931383, 0.59529942, 0.6387351 , 0.66862566, 0.6753056 , 0.65225216, 0.59740204, 0.51374158, 0.40906153, 0.29490684, 0.1848873 , 0.09262206, 0.0296538 , 0.00367407, 0.01735098, 0.06795116, 0.14781638, 0.24561427, 0.34815462, 0.44247004, 0.51781734, 0.56727093, 0.58864968, 0.5846312 , 0.56204482, 0.53047246, 0.50040267, 0.4812572 , 0.47963015, 0.49804245, 0.53442637, 0.58243045, 0.63249529, 0.67351942, 0.69483308, 0.68814418, 0.64912366, 0.57835608, 0.48148692, 0.36853161, 0.25245172, 0.1472267 , 0.06573458, 0.01778779, 0.00864563, 0.0382465 , 0.10128381, 0.18811041, 0.28631891, 0.38273458, 0.46548989, 0.5258384 ]), array([ 0. , 0.24807532, 0.46250991, 0.61497901, 0.68691127, 0.67231624, 0.57853938, 0.42482815, 0.23895805, 0.05249073, -0.10453705, -0.20860165, -0.24652328, -0.21750119, -0.13301915, -0.01464653, 0.10988487, 0.21165762, 0.26575066, 0.25574337, 0.17677289, 0.03661915, -0.14537537, -0.34137929, -0.51963064, -0.64971981, -0.70771236, -0.6802516 , -0.56694875, -0.38065373, -0.14555266, 0.10659923, 0.34148914, 0.5276749 , 0.64153252, 0.67090663, 0.61686403, 0.493268 , 0.324255 , 0.14004515, -0.02820489, -0.15363968, -0.21815444, -0.21531163, -0.15124055, -0.04337637, 0.08274019, 0.19805446, 0.27519744, 0.29340795, 0.24233764, 0.12408733, -0.0468794 , -0.24594165, -0.44228493, -0.60396499, -0.7032264 , -0.72118534, -0.65108967, -0.49960562, -0.28590896, -0.03872439, 0.20820059, 0.42144006, 0.57316865, 0.64550295, 0.63319037, 0.54420723, 0.39817726, 0.22288599, 0.04948365, -0.09281286, -0.18162714, -0.20511603, -0.1638308 , -0.0704398 , 0.05263326, 0.17724956, 0.27458447, 0.32028959, 0.29890787, 0.20678744, 0.05299223, -0.14195583, -0.34930263, -0.53672363, -0.67364595, -0.7363421 , -0.71194204, -0.60068927, -0.41605836, -0.18270779, 0.06739733, 0.30005268, 0.48423444, 0.5969749 , 0.62688952, 0.5757772 , 0.45803793, 0.29801709])] import matplotlib.pyplot as plt plt . figure(figsize = [ 20 , 4 ]) plt . plot(result . times, result . expect[ 0 ]) plt . plot(result . times, result . expect[ 1 ]) plt . xlabel( 'Time' ) plt . ylabel( 'Expectation values' ) plt . legend(( \"m1-operator\" , \"m2-operator\" )) plt . show() References Pauli basis for any 2 by 2 matrices","title":"Algebra with Composite Hamiltonian"},{"location":"Algebra-CompositeHamiltonian/code/#algebra-composite-hamiltonian-and-time-evolution","text":"https://numpy.org/doc/stable/reference/routines.linalg.html http://qutip.org/docs/latest/guide/dynamics/dynamics-master.html import numpy as np from qutip import * import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set() Given a Hamiltonian, we can calculate the unitary (non-dissipative) time-evolution of an arbitrary state vector |\u03c80\u27e9 (psi0) using the QuTiP function qutip.sesolve . It evolves the state vector and evaluates the expectation values for a set of operators expt_ops at the points in time in the list times, using an ordinary differential equation solver. Scrodinger Equation: $$i\\hbar\\frac{d}{dt}\\left|\\psi\\right> = H \\left|\\psi\\right>$$ Time evolution:","title":"Algebra: Composite-Hamiltonian and Time Evolution"},{"location":"Algebra-CompositeHamiltonian/code/#a-single-qubit-system","text":"Consider a single qubit hamiltonian constructed by implementing pauli x operator: $$H = 2 \\pi \\sigma_x$$ # single qubit hamiltonian H = 2.0 * np . pi * sigmax() H Quantum object: dims = [[2], [2]], shape = (2, 2), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}0.0 & 6.283\\6.283 & 0.0\\\\end{array}\\right)\\end{equation*} Create the initial state of the system #initial state of the single qubit system #psi0 = basis(2,0) psi0 = basis( 2 , 1 ) #psi0 = (1/np.sqrt(2))*basis(2,0)+ (1/np.sqrt(2))*basis(2,1) psi0 Quantum object: dims = [[2], [1]], shape = (2, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}0.0\\1.0\\\\end{array}\\right)\\end{equation*} Create time array to conduct measurement time points during the evolution times = np . linspace( 0.0 , 10.0 , 20 ) times array([ 0. , 0.52631579, 1.05263158, 1.57894737, 2.10526316, 2.63157895, 3.15789474, 3.68421053, 4.21052632, 4.73684211, 5.26315789, 5.78947368, 6.31578947, 6.84210526, 7.36842105, 7.89473684, 8.42105263, 8.94736842, 9.47368421, 10. ]) Simulate the time evolution by implementing sesolve function by supplying hamiltonian H , initial state psi0 , time array times and list of operators [sigmaz(), sigmay()] representing what you want to measure as expectation value of. result = sesolve(H, psi0, times, [sigmaz(), sigmay()]) Explore the result result Result object with sesolve data. -------------------------------- expect = True num_expect = 2, num_collapse = 0 Where are the expectation value of the implemented operator for measurement during time evolution? result . expect [array([-1. , -0.94581561, -0.78913418, -0.54693623, -0.24546751, 0.08260204, 0.40171988, 0.67730341, 0.87948895, 0.9863673 , 0.98635446, 0.87945165, 0.67724451, 0.4016443 , 0.08252413, -0.24554292, -0.54700067, -0.78917951, -0.94583863, -1. ]), array([ 0.00000000e+00, 3.24704211e-01, 6.14220851e-01, 8.37174272e-01, 9.69404818e-01, 9.96582612e-01, 9.15762600e-01, 7.35703806e-01, 4.75919309e-01, 1.64558631e-01, -1.64635573e-01, -4.75988221e-01, -7.35758027e-01, -9.15795752e-01, -9.96589067e-01, -9.69385719e-01, -8.37132169e-01, -6.14162600e-01, -3.24637165e-01, 7.13395936e-05])] 7 Plot the time evolution of the expection values of the implemented operators import matplotlib.pyplot as plt plt . figure(figsize = [ 20 , 4 ]) plt . plot(result . times, result . expect[ 0 ]) plt . plot(result . times, result . expect[ 1 ]) plt . xlabel( 'Time' ) plt . ylabel( 'Expectation values' ) plt . legend(( \"sigmaz\" , \"sigmay\" )) plt . show() Lets repeat the same steps for multi-qubit system. Before that, we need to construct the composite hamiltonian representing the quantum mechanical system.","title":"A . Single qubit System"},{"location":"Algebra-CompositeHamiltonian/code/#b-constructing-composite-hamiltonians","text":"","title":"B. Constructing composite Hamiltonians"},{"location":"Algebra-CompositeHamiltonian/code/#1-two-coupled-qubits","text":"Consider a Hamiltonial of a 2 qubits system representated by composite hamiltonian shwn below. The first term represents the contribution of second qubit Z spin operator (sigmaz()) while first qubit remains inactive(identity); the second term represents the contribution of first qubit spin operator while second qubit remains inactive(identity); the third represents the contribution of first and second qubit X spin operator. $H = 0.2(\\sigma_z \\otimes I) + 0.4(I \\otimes \\sigma_z) + \\frac{1}{4} (\\sigma_x \\otimes \\sigma_x)$ H2 = 0.2 * np . pi * tensor(sigmaz(), identity( 2 )) \\ + 0.4 * np . pi * tensor(identity( 2 ),sigmaz()) \\ + 0.25 * np . pi * tensor(sigmax(), sigmax()) H2 Quantum object: dims = [[2, 2], [2, 2]], shape = (4, 4), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}1.885 & 0.0 & 0.0 & 0.785\\0.0 & -0.628 & 0.785 & 0.0\\0.0 & 0.785 & 0.628 & 0.0\\0.785 & 0.0 & 0.0 & -1.885\\\\end{array}\\right)\\end{equation*}","title":"1. Two coupled qubits"},{"location":"Algebra-CompositeHamiltonian/code/#2-three-coupled-qubit","text":"Consider a Hamiltonial of a 3 qubits system representated by composite hamiltonian shwn below. The first term represents the contribution of third qubit Z spin operator (sigmaz()) while rest of the qubits remains inactive(identity); the second term represents the contribution of second qubit spin operator while remaining qubits inactive and so on. $H = (\\sigma_z \\otimes I \\otimes I) + (I \\otimes \\sigma_z \\otimes I) + (I \\otimes I \\otimes \\sigma_z ) + 1/2 (\\sigma_x \\otimes \\sigma_x \\otimes I) + 1/4 ( I \\otimes \\sigma_x \\otimes \\sigma_x)$ H3 = (tensor(sigmaz(), identity( 2 ), identity( 2 )) + tensor(identity( 2 ), sigmaz(), identity( 2 )) + tensor(identity( 2 ), identity( 2 ), sigmaz()) + 0.5 * tensor(sigmax(), sigmax(), identity( 2 )) + 0.25 * tensor(identity( 2 ), sigmax(), sigmax())) H3 Quantum object: dims = [[2, 2, 2], [2, 2, 2]], shape = (8, 8), type = oper, isherm = True\\begin{equation }\\left(\\begin{array}{ {11}c}3.0 & 0.0 & 0.0 & 0.250 & 0.0 & 0.0 & 0.500 & 0.0\\0.0 & 1.0 & 0.250 & 0.0 & 0.0 & 0.0 & 0.0 & 0.500\\0.0 & 0.250 & 1.0 & 0.0 & 0.500 & 0.0 & 0.0 & 0.0\\0.250 & 0.0 & 0.0 & -1.0 & 0.0 & 0.500 & 0.0 & 0.0\\0.0 & 0.0 & 0.500 & 0.0 & 1.0 & 0.0 & 0.0 & 0.250\\0.0 & 0.0 & 0.0 & 0.500 & 0.0 & -1.0 & 0.250 & 0.0\\0.500 & 0.0 & 0.0 & 0.0 & 0.0 & 0.250 & -1.0 & 0.0\\0.0 & 0.500 & 0.0 & 0.0 & 0.250 & 0.0 & 0.0 & -3.0\\\\end{array}\\right)\\end{equation*}","title":"2. Three Coupled Qubit"},{"location":"Algebra-CompositeHamiltonian/code/#c-time-evolution-with-composite-hamiltonian","text":"","title":"C. Time Evolution with composite Hamiltonian"},{"location":"Algebra-CompositeHamiltonian/code/#how-to-generate-a-initial-state-vector","text":"This is 2 qubit quantum mechanical system, there are 4 basis states $|00 \\rangle, |01 \\rangle, |10 \\rangle, |11 \\rangle$ in 4 dimentional complex vector space. All operators in 2 qubit system should be 4 by 4 matrices while state vector need to be 4 element column ket(bra) vector. psi0 = basis( 4 , 0 ) psi0 Quantum object: dims = [[4], [1]], shape = (4, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}1.0\\0.0\\0.0\\0.0\\\\end{array}\\right)\\end{equation*} Q: Could we start with random ket vector? - Answer: yes! # Example 1 psi1 = basis( 4 , 1 ) psi1 Quantum object: dims = [[4], [1]], shape = (4, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}0.0\\1.0\\0.0\\0.0\\\\end{array}\\right)\\end{equation*} # Example 2 psi2 = ( 1 / 2 ) * basis( 4 , 0 ) + ( 1 / 2 ) * basis( 4 , 1 ) + ( 1 / 2 ) * basis( 4 , 2 ) + ( 1 / 2 ) * basis( 4 , 3 ) psi2 Quantum object: dims = [[4], [1]], shape = (4, 1), type = ket\\begin{equation }\\left(\\begin{array}{ {11}c}0.500\\0.500\\0.500\\0.500\\\\end{array}\\right)\\end{equation*} Lets perform the time evolution with two new operators for measurement ( m1-operator, m2-operator defined below in code block) of size 4 by 4 (why? remember, measurement operator in 2 qubit system also need to be 4 by 4 matrices). # time array times = np . linspace( 0.0 , 10.0 , 100 ) # measurement operator m1_operator = tensor(identity( 2 ), sigmaz()) m2_operator = tensor(sigmax(), sigmay()) # time evolution of state psi2 with hamiltonian H2 to calculate expectation # value of m-operator and m-2 operator at 100 different time points result = sesolve(H2, psi2, times, [m1_operator,m2_operator]) # array holding multiple subarray of results result Result object with sesolve data. -------------------------------- expect = True num_expect = 2, num_collapse = 0 # expectation value of m1 and m2 operator at 20 different time point result . expect [array([0. , 0.01990965, 0.07693353, 0.16338415, 0.26780409, 0.37676296, 0.47694252, 0.55716702, 0.6100466 , 0.63296734, 0.62827241, 0.60261422, 0.56559699, 0.52794726, 0.49952837, 0.48753968, 0.49520857, 0.52119896, 0.55983745, 0.60211928, 0.63732295, 0.65495828, 0.64671514, 0.60807788, 0.5393257 , 0.44574105, 0.33698063, 0.22570382, 0.12567867, 0.04967351, 0.00747984, 0.00439134, 0.04038908, 0.11016689, 0.20399138, 0.30925384, 0.41245688, 0.50130765, 0.56657378, 0.60339885, 0.61186652, 0.5967288 , 0.56635478, 0.53108657, 0.50128824, 0.48542398, 0.48849536, 0.51110591, 0.54931383, 0.59529942, 0.6387351 , 0.66862566, 0.6753056 , 0.65225216, 0.59740204, 0.51374158, 0.40906153, 0.29490684, 0.1848873 , 0.09262206, 0.0296538 , 0.00367407, 0.01735098, 0.06795116, 0.14781638, 0.24561427, 0.34815462, 0.44247004, 0.51781734, 0.56727093, 0.58864968, 0.5846312 , 0.56204482, 0.53047246, 0.50040267, 0.4812572 , 0.47963015, 0.49804245, 0.53442637, 0.58243045, 0.63249529, 0.67351942, 0.69483308, 0.68814418, 0.64912366, 0.57835608, 0.48148692, 0.36853161, 0.25245172, 0.1472267 , 0.06573458, 0.01778779, 0.00864563, 0.0382465 , 0.10128381, 0.18811041, 0.28631891, 0.38273458, 0.46548989, 0.5258384 ]), array([ 0. , 0.24807532, 0.46250991, 0.61497901, 0.68691127, 0.67231624, 0.57853938, 0.42482815, 0.23895805, 0.05249073, -0.10453705, -0.20860165, -0.24652328, -0.21750119, -0.13301915, -0.01464653, 0.10988487, 0.21165762, 0.26575066, 0.25574337, 0.17677289, 0.03661915, -0.14537537, -0.34137929, -0.51963064, -0.64971981, -0.70771236, -0.6802516 , -0.56694875, -0.38065373, -0.14555266, 0.10659923, 0.34148914, 0.5276749 , 0.64153252, 0.67090663, 0.61686403, 0.493268 , 0.324255 , 0.14004515, -0.02820489, -0.15363968, -0.21815444, -0.21531163, -0.15124055, -0.04337637, 0.08274019, 0.19805446, 0.27519744, 0.29340795, 0.24233764, 0.12408733, -0.0468794 , -0.24594165, -0.44228493, -0.60396499, -0.7032264 , -0.72118534, -0.65108967, -0.49960562, -0.28590896, -0.03872439, 0.20820059, 0.42144006, 0.57316865, 0.64550295, 0.63319037, 0.54420723, 0.39817726, 0.22288599, 0.04948365, -0.09281286, -0.18162714, -0.20511603, -0.1638308 , -0.0704398 , 0.05263326, 0.17724956, 0.27458447, 0.32028959, 0.29890787, 0.20678744, 0.05299223, -0.14195583, -0.34930263, -0.53672363, -0.67364595, -0.7363421 , -0.71194204, -0.60068927, -0.41605836, -0.18270779, 0.06739733, 0.30005268, 0.48423444, 0.5969749 , 0.62688952, 0.5757772 , 0.45803793, 0.29801709])] import matplotlib.pyplot as plt plt . figure(figsize = [ 20 , 4 ]) plt . plot(result . times, result . expect[ 0 ]) plt . plot(result . times, result . expect[ 1 ]) plt . xlabel( 'Time' ) plt . ylabel( 'Expectation values' ) plt . legend(( \"m1-operator\" , \"m2-operator\" )) plt . show()","title":"How to generate a initial state vector?"},{"location":"Algebra-CompositeHamiltonian/code/#references","text":"Pauli basis for any 2 by 2 matrices","title":"References"},{"location":"Arange/code/","text":"Arange : arange import numpy as np X = np . arange( 10 ) X array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) X - 1.0 array([-1., 0., 1., 2., 3., 4., 5., 6., 7., 8.]) 10 + X array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) np . arange( 3 , 12 ) array([ 3, 4, 5, 6, 7, 8, 9, 10, 11]) np . arange( 1 , 10 , 2 ) array([1, 3, 5, 7, 9]) np . arange( 10 , 11 , 0.01 ) array([10. , 10.01, 10.02, 10.03, 10.04, 10.05, 10.06, 10.07, 10.08, 10.09, 10.1 , 10.11, 10.12, 10.13, 10.14, 10.15, 10.16, 10.17, 10.18, 10.19, 10.2 , 10.21, 10.22, 10.23, 10.24, 10.25, 10.26, 10.27, 10.28, 10.29, 10.3 , 10.31, 10.32, 10.33, 10.34, 10.35, 10.36, 10.37, 10.38, 10.39, 10.4 , 10.41, 10.42, 10.43, 10.44, 10.45, 10.46, 10.47, 10.48, 10.49, 10.5 , 10.51, 10.52, 10.53, 10.54, 10.55, 10.56, 10.57, 10.58, 10.59, 10.6 , 10.61, 10.62, 10.63, 10.64, 10.65, 10.66, 10.67, 10.68, 10.69, 10.7 , 10.71, 10.72, 10.73, 10.74, 10.75, 10.76, 10.77, 10.78, 10.79, 10.8 , 10.81, 10.82, 10.83, 10.84, 10.85, 10.86, 10.87, 10.88, 10.89, 10.9 , 10.91, 10.92, 10.93, 10.94, 10.95, 10.96, 10.97, 10.98, 10.99]) Application : Plot multi-functions import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline sns . set() Gaussian Distribution len (np . arange( - 4 , 4 , 0.01 )) 800 xs = np . arange( - 4 , 4 , 0.01 ) plt . figure(figsize = [ 10 , 2 ]) sigma = 1.0 mu = 0.0 N = 1 / np . sqrt( 2 * np . pi * sigma ** 2 ) fx = N * np . exp( - (xs - mu) ** 2 / ( 2 * sigma ** 2 )) plt . plot(xs,fx) plt . xlabel( 'x' ,fontsize = 20 ) plt . ylabel( 'probability' ) plt . show() xs = np . arange( - 10 , 10 , 0.01 ) plt . figure(figsize = [ 10 , 6 ]) sigmas = [ 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 , 4.0 ] mus = [ 0.0 , 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 ] for s,m in zip (sigmas,mus): N = 1 / np . sqrt( 2 * np . pi * s ** 2 ) fx = N * np . exp( - (xs - m) ** 2 / ( 2 * s ** 2 )) plt . plot(xs,fx, label = \"mu=\" + str (m) + \",sigma=\" + str (s)) plt . xlabel( 'x' ) plt . ylabel( 'probability' ) plt . show() xs = np . arange( - 10 , 10 , 0.01 ) plt . figure(figsize = [ 14 , 16 ]) sigmas = [ 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 , 4.0 ] mus = [ 0.0 , 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 ] k = 1 for s,m in zip (sigmas,mus): plt . subplot( 4 , 2 ,k) N = 1 / np . sqrt( 2 * np . pi * s ** 2 ) fx = N * np . exp( - (xs - m) ** 2 / ( 2 * s ** 2 )) plt . plot(xs,fx, label = \"mu=\" + str (m) + \",sigma=\" + str (s)) plt . xlabel( 'x' ,fontsize = 20 ) plt . ylabel( 'prabability' ) plt . legend() k = k + 1 plt . show() List Comprehension with arange xs = np . arange( 0 , 3.14 , 0.1 ) np . sin(xs) array([0. , 0.09983342, 0.19866933, 0.29552021, 0.38941834, 0.47942554, 0.56464247, 0.64421769, 0.71735609, 0.78332691, 0.84147098, 0.89120736, 0.93203909, 0.96355819, 0.98544973, 0.99749499, 0.9995736 , 0.99166481, 0.97384763, 0.94630009, 0.90929743, 0.86320937, 0.8084964 , 0.74570521, 0.67546318, 0.59847214, 0.51550137, 0.42737988, 0.33498815, 0.23924933, 0.14112001, 0.04158066]) np . cos(xs) array([ 1. , 0.99500417, 0.98006658, 0.95533649, 0.92106099, 0.87758256, 0.82533561, 0.76484219, 0.69670671, 0.62160997, 0.54030231, 0.45359612, 0.36235775, 0.26749883, 0.16996714, 0.0707372 , -0.02919952, -0.12884449, -0.22720209, -0.32328957, -0.41614684, -0.5048461 , -0.58850112, -0.66627602, -0.73739372, -0.80114362, -0.85688875, -0.90407214, -0.94222234, -0.97095817, -0.9899925 , -0.99913515]) Sum of sin(x)+cos(x) np . sin(xs) + np . cos(xs) array([ 1. , 1.09483758, 1.17873591, 1.2508567 , 1.31047934, 1.3570081 , 1.38997809, 1.40905987, 1.4140628 , 1.40493688, 1.38177329, 1.34480348, 1.29439684, 1.23105701, 1.15541687, 1.06823219, 0.97037408, 0.86282032, 0.74664554, 0.62301052, 0.49315059, 0.35836326, 0.21999529, 0.07942919, -0.06193053, -0.20267147, -0.34138738, -0.47669226, -0.60723419, -0.73170884, -0.84887249, -0.95755449]) List including array for cos(x), cos(2x), cos(3x), cos(4x) [np . cos(n * xs) for n in range ( 1 , 5 )] [array([ 1. , 0.99500417, 0.98006658, 0.95533649, 0.92106099, 0.87758256, 0.82533561, 0.76484219, 0.69670671, 0.62160997, 0.54030231, 0.45359612, 0.36235775, 0.26749883, 0.16996714, 0.0707372 , -0.02919952, -0.12884449, -0.22720209, -0.32328957, -0.41614684, -0.5048461 , -0.58850112, -0.66627602, -0.73739372, -0.80114362, -0.85688875, -0.90407214, -0.94222234, -0.97095817, -0.9899925 , -0.99913515]), array([ 1. , 0.98006658, 0.92106099, 0.82533561, 0.69670671, 0.54030231, 0.36235775, 0.16996714, -0.02919952, -0.22720209, -0.41614684, -0.58850112, -0.73739372, -0.85688875, -0.94222234, -0.9899925 , -0.99829478, -0.96679819, -0.89675842, -0.79096771, -0.65364362, -0.49026082, -0.30733287, -0.11215253, 0.08749898, 0.28366219, 0.46851667, 0.63469288, 0.77556588, 0.88551952, 0.96017029, 0.9965421 ]), array([ 1. , 0.95533649, 0.82533561, 0.62160997, 0.36235775, 0.0707372 , -0.22720209, -0.5048461 , -0.73739372, -0.90407214, -0.9899925 , -0.98747977, -0.89675842, -0.7259323 , -0.49026082, -0.2107958 , 0.08749898, 0.37797774, 0.63469288, 0.83471278, 0.96017029, 0.99985864, 0.95023259, 0.8157251 , 0.60835131, 0.34663532, 0.05395542, -0.24354415, -0.51928865, -0.74864665, -0.91113026, -0.99222533]), array([ 1. , 0.92106099, 0.69670671, 0.36235775, -0.02919952, -0.41614684, -0.73739372, -0.94222234, -0.99829478, -0.89675842, -0.65364362, -0.30733287, 0.08749898, 0.46851667, 0.77556588, 0.96017029, 0.99318492, 0.86939749, 0.60835131, 0.25125984, -0.14550003, -0.51928865, -0.81109301, -0.97484362, -0.98468786, -0.83907153, -0.56098426, -0.19432991, 0.20300486, 0.56828963, 0.84385396, 0.9861923 ])] Sum of cos(x)+cos(2x)+cos(3x)+cos(4x) sum ([np . cos(n * xs) for n in range ( 1 , 5 )]) array([ 4. , 3.85146823, 3.4231699 , 2.76463983, 1.95092594, 1.07247523, 0.22309756, -0.51225912, -1.0681813 , -1.40642268, -1.51948065, -1.42971764, -1.18429539, -0.84680556, -0.48695014, -0.16988081, 0.0531896 , 0.15173255, 0.11908368, -0.02828465, -0.2551202 , -0.51453694, -0.75669441, -0.93754707, -1.02623127, -1.00991764, -0.89540092, -0.70725333, -0.48294025, -0.26579566, -0.09709851, -0.00862608])","title":"Arange"},{"location":"Arange/code/#arange-arange","text":"import numpy as np X = np . arange( 10 ) X array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) X - 1.0 array([-1., 0., 1., 2., 3., 4., 5., 6., 7., 8.]) 10 + X array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) np . arange( 3 , 12 ) array([ 3, 4, 5, 6, 7, 8, 9, 10, 11]) np . arange( 1 , 10 , 2 ) array([1, 3, 5, 7, 9]) np . arange( 10 , 11 , 0.01 ) array([10. , 10.01, 10.02, 10.03, 10.04, 10.05, 10.06, 10.07, 10.08, 10.09, 10.1 , 10.11, 10.12, 10.13, 10.14, 10.15, 10.16, 10.17, 10.18, 10.19, 10.2 , 10.21, 10.22, 10.23, 10.24, 10.25, 10.26, 10.27, 10.28, 10.29, 10.3 , 10.31, 10.32, 10.33, 10.34, 10.35, 10.36, 10.37, 10.38, 10.39, 10.4 , 10.41, 10.42, 10.43, 10.44, 10.45, 10.46, 10.47, 10.48, 10.49, 10.5 , 10.51, 10.52, 10.53, 10.54, 10.55, 10.56, 10.57, 10.58, 10.59, 10.6 , 10.61, 10.62, 10.63, 10.64, 10.65, 10.66, 10.67, 10.68, 10.69, 10.7 , 10.71, 10.72, 10.73, 10.74, 10.75, 10.76, 10.77, 10.78, 10.79, 10.8 , 10.81, 10.82, 10.83, 10.84, 10.85, 10.86, 10.87, 10.88, 10.89, 10.9 , 10.91, 10.92, 10.93, 10.94, 10.95, 10.96, 10.97, 10.98, 10.99])","title":"Arange : arange"},{"location":"Arange/code/#application-plot-multi-functions","text":"import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline sns . set()","title":"Application : Plot multi-functions"},{"location":"Arange/code/#gaussian-distribution","text":"len (np . arange( - 4 , 4 , 0.01 )) 800 xs = np . arange( - 4 , 4 , 0.01 ) plt . figure(figsize = [ 10 , 2 ]) sigma = 1.0 mu = 0.0 N = 1 / np . sqrt( 2 * np . pi * sigma ** 2 ) fx = N * np . exp( - (xs - mu) ** 2 / ( 2 * sigma ** 2 )) plt . plot(xs,fx) plt . xlabel( 'x' ,fontsize = 20 ) plt . ylabel( 'probability' ) plt . show() xs = np . arange( - 10 , 10 , 0.01 ) plt . figure(figsize = [ 10 , 6 ]) sigmas = [ 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 , 4.0 ] mus = [ 0.0 , 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 ] for s,m in zip (sigmas,mus): N = 1 / np . sqrt( 2 * np . pi * s ** 2 ) fx = N * np . exp( - (xs - m) ** 2 / ( 2 * s ** 2 )) plt . plot(xs,fx, label = \"mu=\" + str (m) + \",sigma=\" + str (s)) plt . xlabel( 'x' ) plt . ylabel( 'probability' ) plt . show() xs = np . arange( - 10 , 10 , 0.01 ) plt . figure(figsize = [ 14 , 16 ]) sigmas = [ 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 , 4.0 ] mus = [ 0.0 , 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 ] k = 1 for s,m in zip (sigmas,mus): plt . subplot( 4 , 2 ,k) N = 1 / np . sqrt( 2 * np . pi * s ** 2 ) fx = N * np . exp( - (xs - m) ** 2 / ( 2 * s ** 2 )) plt . plot(xs,fx, label = \"mu=\" + str (m) + \",sigma=\" + str (s)) plt . xlabel( 'x' ,fontsize = 20 ) plt . ylabel( 'prabability' ) plt . legend() k = k + 1 plt . show()","title":"Gaussian Distribution"},{"location":"Arange/code/#list-comprehension-with-arange","text":"xs = np . arange( 0 , 3.14 , 0.1 ) np . sin(xs) array([0. , 0.09983342, 0.19866933, 0.29552021, 0.38941834, 0.47942554, 0.56464247, 0.64421769, 0.71735609, 0.78332691, 0.84147098, 0.89120736, 0.93203909, 0.96355819, 0.98544973, 0.99749499, 0.9995736 , 0.99166481, 0.97384763, 0.94630009, 0.90929743, 0.86320937, 0.8084964 , 0.74570521, 0.67546318, 0.59847214, 0.51550137, 0.42737988, 0.33498815, 0.23924933, 0.14112001, 0.04158066]) np . cos(xs) array([ 1. , 0.99500417, 0.98006658, 0.95533649, 0.92106099, 0.87758256, 0.82533561, 0.76484219, 0.69670671, 0.62160997, 0.54030231, 0.45359612, 0.36235775, 0.26749883, 0.16996714, 0.0707372 , -0.02919952, -0.12884449, -0.22720209, -0.32328957, -0.41614684, -0.5048461 , -0.58850112, -0.66627602, -0.73739372, -0.80114362, -0.85688875, -0.90407214, -0.94222234, -0.97095817, -0.9899925 , -0.99913515]) Sum of sin(x)+cos(x) np . sin(xs) + np . cos(xs) array([ 1. , 1.09483758, 1.17873591, 1.2508567 , 1.31047934, 1.3570081 , 1.38997809, 1.40905987, 1.4140628 , 1.40493688, 1.38177329, 1.34480348, 1.29439684, 1.23105701, 1.15541687, 1.06823219, 0.97037408, 0.86282032, 0.74664554, 0.62301052, 0.49315059, 0.35836326, 0.21999529, 0.07942919, -0.06193053, -0.20267147, -0.34138738, -0.47669226, -0.60723419, -0.73170884, -0.84887249, -0.95755449]) List including array for cos(x), cos(2x), cos(3x), cos(4x) [np . cos(n * xs) for n in range ( 1 , 5 )] [array([ 1. , 0.99500417, 0.98006658, 0.95533649, 0.92106099, 0.87758256, 0.82533561, 0.76484219, 0.69670671, 0.62160997, 0.54030231, 0.45359612, 0.36235775, 0.26749883, 0.16996714, 0.0707372 , -0.02919952, -0.12884449, -0.22720209, -0.32328957, -0.41614684, -0.5048461 , -0.58850112, -0.66627602, -0.73739372, -0.80114362, -0.85688875, -0.90407214, -0.94222234, -0.97095817, -0.9899925 , -0.99913515]), array([ 1. , 0.98006658, 0.92106099, 0.82533561, 0.69670671, 0.54030231, 0.36235775, 0.16996714, -0.02919952, -0.22720209, -0.41614684, -0.58850112, -0.73739372, -0.85688875, -0.94222234, -0.9899925 , -0.99829478, -0.96679819, -0.89675842, -0.79096771, -0.65364362, -0.49026082, -0.30733287, -0.11215253, 0.08749898, 0.28366219, 0.46851667, 0.63469288, 0.77556588, 0.88551952, 0.96017029, 0.9965421 ]), array([ 1. , 0.95533649, 0.82533561, 0.62160997, 0.36235775, 0.0707372 , -0.22720209, -0.5048461 , -0.73739372, -0.90407214, -0.9899925 , -0.98747977, -0.89675842, -0.7259323 , -0.49026082, -0.2107958 , 0.08749898, 0.37797774, 0.63469288, 0.83471278, 0.96017029, 0.99985864, 0.95023259, 0.8157251 , 0.60835131, 0.34663532, 0.05395542, -0.24354415, -0.51928865, -0.74864665, -0.91113026, -0.99222533]), array([ 1. , 0.92106099, 0.69670671, 0.36235775, -0.02919952, -0.41614684, -0.73739372, -0.94222234, -0.99829478, -0.89675842, -0.65364362, -0.30733287, 0.08749898, 0.46851667, 0.77556588, 0.96017029, 0.99318492, 0.86939749, 0.60835131, 0.25125984, -0.14550003, -0.51928865, -0.81109301, -0.97484362, -0.98468786, -0.83907153, -0.56098426, -0.19432991, 0.20300486, 0.56828963, 0.84385396, 0.9861923 ])] Sum of cos(x)+cos(2x)+cos(3x)+cos(4x) sum ([np . cos(n * xs) for n in range ( 1 , 5 )]) array([ 4. , 3.85146823, 3.4231699 , 2.76463983, 1.95092594, 1.07247523, 0.22309756, -0.51225912, -1.0681813 , -1.40642268, -1.51948065, -1.42971764, -1.18429539, -0.84680556, -0.48695014, -0.16988081, 0.0531896 , 0.15173255, 0.11908368, -0.02828465, -0.2551202 , -0.51453694, -0.75669441, -0.93754707, -1.02623127, -1.00991764, -0.89540092, -0.70725333, -0.48294025, -0.26579566, -0.09709851, -0.00862608])","title":"List Comprehension with arange"},{"location":"Arange-FourierSeries/code/","text":"Fourier Series import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() Basis functions (\\(\\sin(\\theta), \\sin(2\\theta) ... )\\) fs = 100 thetas = np . arange( 0 , 2 * np . pi, 1 / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 6 ): amp = 1 * 1 / float (i) '''Y= A sin(n.theta)''' ys = amp * np . sin(i * thetas) lb = \"sin(\" + str (i) + r'$\\theta$)' plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\sin(n\\theta)$' ) plt . legend() plt . show() Basis function Periodicity //(sin(\\theta)//) 2$\\pi$ $sin(2\\theta)$ $\\pi$ $sin(3\\theta)$ 2$\\pi$/3 $sin(4\\theta)$ $\\pi$/4 Basis functions (\\(\\cos(\\theta), \\cos(2\\theta) ...)\\) fs = 100 thetas = np . arange( 0 , 2 * np . pi, 1 / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 6 ): amp = 1 * 1 / float (i) '''Y= A sin(n.theta)''' ys = amp * np . cos(i * thetas) lb = \"cos(\" + str (i) + r'$\\theta$)' plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\cos(n\\theta)$' ) plt . legend() plt . show() Basis function Periodicity $cos(\\theta)$ 2$\\pi$ $cos(2\\theta)$ $\\pi$ $cos(3\\theta)$ 2$\\pi$/3 $cos(4\\theta)$ $\\pi$/4 Basis functions (\\(\\sin(\\theta), \\sin(\\theta/2) ...)\\) fs = 100 thetas = np . arange( 0 , 20 * np . pi, 1 / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 10 ): amp = 1 * float (i) '''Y= A sin(n.theta)''' ys = amp * np . sin(thetas / float (i)) lb = \"sin(\" + r'$\\theta$/' + str (i) + \")\" plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\sin(\\theta/n)$' ) plt . legend() plt . show() Assignment: What are the periodicites? Basis functions (\\(\\cos(\\theta), \\cos(\\theta/2) ...)\\) fs = 100 thetas = np . arange( 0 , 20 * np . pi, 2 * np . pi / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 9 ): amp = 1 * float (i) '''Y= A sin(n.theta)''' ys = amp * np . cos(thetas / float (i)) lb = \"cos(\" + r'$\\theta$/' + str (i) + \")\" plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\cos(\\theta/n)$' ) plt . legend() plt . show() Assignment: What are the periodicites? A : Fourier Series: Square Wave 1. Individual basis function terms: Here we want to plot the individual terms in the above series to compere the periodicity and amplitudes. L = 1 ns = 100 thetas = np . arange( 0 , 2 , 1 / ns) plt . figure(figsize = [ 15 , 4 ]) for n in [i for i in range ( 10 ) if i % 2 != 0 ]: '''creating individual terms in the series''' fs = ( 4 / np . pi) * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)) plt . plot(thetas,fs) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( \"Basis terms\" ) plt . show() 2.Subsequent development by adding terms Here we want to observe the plote of terms (e.g., first term, sum of first two terms, sum of first three terms) to realize how final series is developed by contibution of individual terms. L = 1 ns = 100 thetas = np . arange( 0 , 2 , 1 / ns) plt . figure(figsize = [ 15 , 4 ]) k = 1 for n in [i for i in range ( 10 ) if i % 2 != 0 ]: if n == 1 : '''taking care of first term''' fi = ( 4 / np . pi) * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)) plt . plot(thetas,fi, label = \"first term\" ) ff = fi else : '''terms following first term''' fi = ff + ( 4 / np . pi) * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)) plt . plot(thetas,fi, label = \"sum of \" + str (k) + \" terms\" ) ff = fi k = k + 1 plt . legend() plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( 'sum of n - terms' ) plt . show() 3. Sum of all n-terms L = 1 ns = 100 thetas = np . arange( 0 , 10 , 1 / ns) plt . figure(figsize = [ 15 , 4 ]) '''including n = 1,3,5,...19th terms in basis expansison''' fs = sum ([( 4 / np . pi * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)))\\ for n in [i for i in range ( 20 ) if i % 2 != 0 ]]) plt . plot(thetas,fs,label = \"sawtooth wave\" ) plt . legend() plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( 'Square wave' ) plt . show() Assignment Develope the Fourier series of Triangular wave following aboves method: B: Fourier Transform Experiment 1: Signal without noise fs = 100.0 ts = np . arange( 0 , 10 , 1 / fs) freq_cos = [ 2 , 4 , 6 , 8 , 10 , 12 ] freq_sin = [ 1 , 3 , 5 , 7 , 9 ] '''prepare candidate signals''' sigs = [np . cos( 2 * np . pi * f1 * ts) + np . sin( 2 * np . pi * f2 * ts)\\ for f1,f2 in zip (freq_cos,freq_sin)] '''resultant signal''' rsig = sum (sigs) fft_rsig = np . fft . fft(rsig) fft_freq = np . fft . fftfreq( len (fft_rsig), 1 / fs) plt . figure(figsize = [ 18 , 10 ]) plt . subplot( 311 ) plt . plot(ts,rsig) plt . title( \"Original signal in Time Domain\" ) plt . subplot( 312 ) plt . stem(fft_freq, abs (fft_rsig . real),\\ label = \"Real part\" ) plt . title( \"FFT in Frequency Domain\" ) plt . ylim( 0 , 1000 ) plt . xlim( 0 , 15 ) plt . legend(loc = 1 ) plt . subplot( 313 ) plt . stem(fft_freq, abs (fft_rsig . imag),\\ label = \"Imaginary part\" ) plt . xlabel( \"frequency (Hz)\" ) plt . legend(loc = 1 ) plt . xlim( 0 , 15 ) plt . ylim( 0 , 1000 ) plt . show() Experiment 2: Signal with noise fs = 100.0 ts = np . arange( 0 , 10 , 1 / fs) freq_cos = [ 2 , 4 , 6 , 8 , 10 , 12 ] freq_sin = [ 1 , 3 , 5 , 7 , 9 ] '''prepare candidate signals''' sigs = [np . cos( 2 * np . pi * f1 * ts) + np . sin( 2 * np . pi * f2 * ts) \\ for f1,f2 in zip (freq_cos,freq_sin)] '''resultant signal + noise''' noise = 10 * np . random . rand( len (rsig)) rsig_noise = sum (sigs) + noise fft_rsig_noise = np . fft . fft(rsig_noise) fft_freq_noise = np . fft . fftfreq( len (fft_rsig_noise), 1 / fs) plt . figure(figsize = [ 18 , 10 ]) plt . subplot( 311 ) plt . plot(ts,rsig_noise) plt . title( \"Original signal with noise in Time Domain\" ) plt . subplot( 312 ) plt . stem(fft_freq_noise, abs (fft_rsig_noise . real),\\ label = \"Real part\" ) plt . title( \"FFT in Frequency Domain\" ) plt . ylim( 0 , 1000 ) plt . xlim( 0 , 15 ) plt . legend(loc = 1 ) plt . subplot( 313 ) plt . stem(fft_freq_noise, abs (fft_rsig_noise . imag),\\ label = \"Imaginary part\" ) plt . xlabel( \"frequency (Hz)\" ) plt . legend(loc = 1 ) plt . xlim( 0 , 15 ) plt . ylim( 0 , 1000 ) plt . show()","title":"Arange demo with Fourier Series"},{"location":"Arange-FourierSeries/code/#fourier-series","text":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set()","title":"Fourier Series"},{"location":"Arange-FourierSeries/code/#basis-functions-sintheta-sin2theta","text":"fs = 100 thetas = np . arange( 0 , 2 * np . pi, 1 / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 6 ): amp = 1 * 1 / float (i) '''Y= A sin(n.theta)''' ys = amp * np . sin(i * thetas) lb = \"sin(\" + str (i) + r'$\\theta$)' plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\sin(n\\theta)$' ) plt . legend() plt . show() Basis function Periodicity //(sin(\\theta)//) 2$\\pi$ $sin(2\\theta)$ $\\pi$ $sin(3\\theta)$ 2$\\pi$/3 $sin(4\\theta)$ $\\pi$/4","title":"Basis functions (\\(\\sin(\\theta), \\sin(2\\theta) ... )\\)"},{"location":"Arange-FourierSeries/code/#basis-functions-costheta-cos2theta","text":"fs = 100 thetas = np . arange( 0 , 2 * np . pi, 1 / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 6 ): amp = 1 * 1 / float (i) '''Y= A sin(n.theta)''' ys = amp * np . cos(i * thetas) lb = \"cos(\" + str (i) + r'$\\theta$)' plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\cos(n\\theta)$' ) plt . legend() plt . show() Basis function Periodicity $cos(\\theta)$ 2$\\pi$ $cos(2\\theta)$ $\\pi$ $cos(3\\theta)$ 2$\\pi$/3 $cos(4\\theta)$ $\\pi$/4","title":"Basis functions (\\(\\cos(\\theta), \\cos(2\\theta) ...)\\)"},{"location":"Arange-FourierSeries/code/#basis-functions-sintheta-sintheta2","text":"fs = 100 thetas = np . arange( 0 , 20 * np . pi, 1 / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 10 ): amp = 1 * float (i) '''Y= A sin(n.theta)''' ys = amp * np . sin(thetas / float (i)) lb = \"sin(\" + r'$\\theta$/' + str (i) + \")\" plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\sin(\\theta/n)$' ) plt . legend() plt . show()","title":"Basis functions (\\(\\sin(\\theta), \\sin(\\theta/2) ...)\\)"},{"location":"Arange-FourierSeries/code/#assignment-what-are-the-periodicites","text":"","title":"Assignment: What are the periodicites?"},{"location":"Arange-FourierSeries/code/#basis-functions-costheta-costheta2","text":"fs = 100 thetas = np . arange( 0 , 20 * np . pi, 2 * np . pi / fs) plt . figure(figsize = [ 20 , 6 ]) for i in range ( 1 , 9 ): amp = 1 * float (i) '''Y= A sin(n.theta)''' ys = amp * np . cos(thetas / float (i)) lb = \"cos(\" + r'$\\theta$/' + str (i) + \")\" plt . plot(thetas,ys, label = lb) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( r'$\\cos(\\theta/n)$' ) plt . legend() plt . show()","title":"Basis functions (\\(\\cos(\\theta), \\cos(\\theta/2) ...)\\)"},{"location":"Arange-FourierSeries/code/#assignment-what-are-the-periodicites_1","text":"","title":"Assignment: What are the periodicites?"},{"location":"Arange-FourierSeries/code/#a-fourier-series-square-wave","text":"","title":"A : Fourier Series: Square Wave"},{"location":"Arange-FourierSeries/code/#1-individual-basis-function-terms","text":"Here we want to plot the individual terms in the above series to compere the periodicity and amplitudes. L = 1 ns = 100 thetas = np . arange( 0 , 2 , 1 / ns) plt . figure(figsize = [ 15 , 4 ]) for n in [i for i in range ( 10 ) if i % 2 != 0 ]: '''creating individual terms in the series''' fs = ( 4 / np . pi) * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)) plt . plot(thetas,fs) plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( \"Basis terms\" ) plt . show()","title":"1. Individual basis function terms:"},{"location":"Arange-FourierSeries/code/#2subsequent-development-by-adding-terms","text":"Here we want to observe the plote of terms (e.g., first term, sum of first two terms, sum of first three terms) to realize how final series is developed by contibution of individual terms. L = 1 ns = 100 thetas = np . arange( 0 , 2 , 1 / ns) plt . figure(figsize = [ 15 , 4 ]) k = 1 for n in [i for i in range ( 10 ) if i % 2 != 0 ]: if n == 1 : '''taking care of first term''' fi = ( 4 / np . pi) * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)) plt . plot(thetas,fi, label = \"first term\" ) ff = fi else : '''terms following first term''' fi = ff + ( 4 / np . pi) * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)) plt . plot(thetas,fi, label = \"sum of \" + str (k) + \" terms\" ) ff = fi k = k + 1 plt . legend() plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( 'sum of n - terms' ) plt . show()","title":"2.Subsequent development by adding terms"},{"location":"Arange-FourierSeries/code/#3-sum-of-all-n-terms","text":"L = 1 ns = 100 thetas = np . arange( 0 , 10 , 1 / ns) plt . figure(figsize = [ 15 , 4 ]) '''including n = 1,3,5,...19th terms in basis expansison''' fs = sum ([( 4 / np . pi * ( 1 / n) * np . sin(n * np . pi * thetas / float (L)))\\ for n in [i for i in range ( 20 ) if i % 2 != 0 ]]) plt . plot(thetas,fs,label = \"sawtooth wave\" ) plt . legend() plt . xlabel( \"Angle (\" + r'$\\theta$)' ) plt . ylabel( 'Square wave' ) plt . show()","title":"3. Sum of all n-terms"},{"location":"Arange-FourierSeries/code/#assignment","text":"Develope the Fourier series of Triangular wave following aboves method:","title":"Assignment"},{"location":"Arange-FourierSeries/code/#b-fourier-transform","text":"","title":"B: Fourier Transform"},{"location":"Arange-FourierSeries/code/#experiment-1-signal-without-noise","text":"fs = 100.0 ts = np . arange( 0 , 10 , 1 / fs) freq_cos = [ 2 , 4 , 6 , 8 , 10 , 12 ] freq_sin = [ 1 , 3 , 5 , 7 , 9 ] '''prepare candidate signals''' sigs = [np . cos( 2 * np . pi * f1 * ts) + np . sin( 2 * np . pi * f2 * ts)\\ for f1,f2 in zip (freq_cos,freq_sin)] '''resultant signal''' rsig = sum (sigs) fft_rsig = np . fft . fft(rsig) fft_freq = np . fft . fftfreq( len (fft_rsig), 1 / fs) plt . figure(figsize = [ 18 , 10 ]) plt . subplot( 311 ) plt . plot(ts,rsig) plt . title( \"Original signal in Time Domain\" ) plt . subplot( 312 ) plt . stem(fft_freq, abs (fft_rsig . real),\\ label = \"Real part\" ) plt . title( \"FFT in Frequency Domain\" ) plt . ylim( 0 , 1000 ) plt . xlim( 0 , 15 ) plt . legend(loc = 1 ) plt . subplot( 313 ) plt . stem(fft_freq, abs (fft_rsig . imag),\\ label = \"Imaginary part\" ) plt . xlabel( \"frequency (Hz)\" ) plt . legend(loc = 1 ) plt . xlim( 0 , 15 ) plt . ylim( 0 , 1000 ) plt . show()","title":"Experiment 1: Signal without noise"},{"location":"Arange-FourierSeries/code/#experiment-2-signal-with-noise","text":"fs = 100.0 ts = np . arange( 0 , 10 , 1 / fs) freq_cos = [ 2 , 4 , 6 , 8 , 10 , 12 ] freq_sin = [ 1 , 3 , 5 , 7 , 9 ] '''prepare candidate signals''' sigs = [np . cos( 2 * np . pi * f1 * ts) + np . sin( 2 * np . pi * f2 * ts) \\ for f1,f2 in zip (freq_cos,freq_sin)] '''resultant signal + noise''' noise = 10 * np . random . rand( len (rsig)) rsig_noise = sum (sigs) + noise fft_rsig_noise = np . fft . fft(rsig_noise) fft_freq_noise = np . fft . fftfreq( len (fft_rsig_noise), 1 / fs) plt . figure(figsize = [ 18 , 10 ]) plt . subplot( 311 ) plt . plot(ts,rsig_noise) plt . title( \"Original signal with noise in Time Domain\" ) plt . subplot( 312 ) plt . stem(fft_freq_noise, abs (fft_rsig_noise . real),\\ label = \"Real part\" ) plt . title( \"FFT in Frequency Domain\" ) plt . ylim( 0 , 1000 ) plt . xlim( 0 , 15 ) plt . legend(loc = 1 ) plt . subplot( 313 ) plt . stem(fft_freq_noise, abs (fft_rsig_noise . imag),\\ label = \"Imaginary part\" ) plt . xlabel( \"frequency (Hz)\" ) plt . legend(loc = 1 ) plt . xlim( 0 , 15 ) plt . ylim( 0 , 1000 ) plt . show()","title":"Experiment 2: Signal with noise"},{"location":"Arange-Oscillation/code/","text":"Oscialltion import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() $$ Y = Asin(\\omega t + \\phi )$$ fs = 100 ts = np . arange( 0 , 60 , 1 / fs) ts array([0.000e+00, 1.000e-02, 2.000e-02, ..., 5.997e+01, 5.998e+01, 5.999e+01]) np . sin(ts) array([ 0. , 0.00999983, 0.01999867, ..., -0.27610536, -0.28570267, -0.29527141]) freq = 0.5 ts * ( 2 * np . pi * freq) array([0.00000000e+00, 3.14159265e-02, 6.28318531e-02, ..., 1.88401311e+02, 1.88432727e+02, 1.88464143e+02]) Single oscillation fs = 100 ts = np . arange( 0 , 60 , 1 / fs) amp = 1.0 freq = 0.025 '''initialize figure''' plt . figure(figsize = [ 20 , 4 ]) '''create oscillation: Y = Asin(wt)| w = 2.pi.f''' ys = amp * np . sin(ts * ( 2 * np . pi * freq)) lb = \"amp: \" + str (amp) + \",freq:\" + str (freq) plt . plot(ts,ys, label = lb) plt . xlabel( \"Time in seconds\" ) plt . ylabel( \"Displacement\" ) plt . legend() plt . show() Variable Amplitude and Frequency in Oscillation fs = 100 ts = np . arange( 0 , 60 , 1 / fs) '''initialize figure''' plt . figure(figsize = [ 20 , 6 ]) '''iterate over each amplitude and frequency and create a oscillation plot''' for i in range ( 1 , 10 ): amp = 100 * 1 / float (i) freq = 0.025 * i '''Y= A sin(wt) | w = 2*pi*f''' ys = amp * np . sin(ts * 2 * np . pi * freq) lb = \"amp: \" + str (amp)[ 0 : 4 ] + \",freq:\" + str (freq)[ 0 : 4 ] plt . plot(ts,ys,label = lb ) plt . xlabel( \"Time in seconds\" ) plt . ylabel( \"Displacement\" ) plt . legend() plt . show() Superposition of oscillations fs = 100 ts = np . arange( 0 , 60 , 1 / fs) '''initialize figure''' plt . figure(figsize = [ 20 , 6 ]) '''define 10 different amplitudes and frequencies''' amps = [ 1 * 1 / float (i) for i in range ( 1 , 10 )] freqs = [ 0.025 * i for i in range ( 1 , 10 )] '''List comprehension: ''' ys = sum ([amp * np . sin(ts * 2 * np . pi * freq) for amp,freq in zip (amps,freqs)]) '''create a plot''' plt . plot(ts,ys, label = \"Superposed oscillation\" ) plt . legend() plt . show() Is this a sawtooth wave? Assignment: Write python function for creating a random oscialltion with amplitude in range (1,5) and frequency in range (0.25,1) Create 20 different oscillations by implementing python function defined above and implement superposition principle to obtain single resultant oscillation. def oscillator (): '''implement y = Asin(wt)+Bcos(wt) with random amplitude in (1,5) and freq in (0.25,1)'''","title":"Arange demo with oscillation"},{"location":"Arange-Oscillation/code/#oscialltion","text":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() $$ Y = Asin(\\omega t + \\phi )$$ fs = 100 ts = np . arange( 0 , 60 , 1 / fs) ts array([0.000e+00, 1.000e-02, 2.000e-02, ..., 5.997e+01, 5.998e+01, 5.999e+01]) np . sin(ts) array([ 0. , 0.00999983, 0.01999867, ..., -0.27610536, -0.28570267, -0.29527141]) freq = 0.5 ts * ( 2 * np . pi * freq) array([0.00000000e+00, 3.14159265e-02, 6.28318531e-02, ..., 1.88401311e+02, 1.88432727e+02, 1.88464143e+02])","title":"Oscialltion"},{"location":"Arange-Oscillation/code/#single-oscillation","text":"fs = 100 ts = np . arange( 0 , 60 , 1 / fs) amp = 1.0 freq = 0.025 '''initialize figure''' plt . figure(figsize = [ 20 , 4 ]) '''create oscillation: Y = Asin(wt)| w = 2.pi.f''' ys = amp * np . sin(ts * ( 2 * np . pi * freq)) lb = \"amp: \" + str (amp) + \",freq:\" + str (freq) plt . plot(ts,ys, label = lb) plt . xlabel( \"Time in seconds\" ) plt . ylabel( \"Displacement\" ) plt . legend() plt . show()","title":"Single oscillation"},{"location":"Arange-Oscillation/code/#variable-amplitude-and-frequency-in-oscillation","text":"fs = 100 ts = np . arange( 0 , 60 , 1 / fs) '''initialize figure''' plt . figure(figsize = [ 20 , 6 ]) '''iterate over each amplitude and frequency and create a oscillation plot''' for i in range ( 1 , 10 ): amp = 100 * 1 / float (i) freq = 0.025 * i '''Y= A sin(wt) | w = 2*pi*f''' ys = amp * np . sin(ts * 2 * np . pi * freq) lb = \"amp: \" + str (amp)[ 0 : 4 ] + \",freq:\" + str (freq)[ 0 : 4 ] plt . plot(ts,ys,label = lb ) plt . xlabel( \"Time in seconds\" ) plt . ylabel( \"Displacement\" ) plt . legend() plt . show()","title":"Variable Amplitude and Frequency in Oscillation"},{"location":"Arange-Oscillation/code/#superposition-of-oscillations","text":"fs = 100 ts = np . arange( 0 , 60 , 1 / fs) '''initialize figure''' plt . figure(figsize = [ 20 , 6 ]) '''define 10 different amplitudes and frequencies''' amps = [ 1 * 1 / float (i) for i in range ( 1 , 10 )] freqs = [ 0.025 * i for i in range ( 1 , 10 )] '''List comprehension: ''' ys = sum ([amp * np . sin(ts * 2 * np . pi * freq) for amp,freq in zip (amps,freqs)]) '''create a plot''' plt . plot(ts,ys, label = \"Superposed oscillation\" ) plt . legend() plt . show() Is this a sawtooth wave?","title":"Superposition of oscillations"},{"location":"Arange-Oscillation/code/#assignment","text":"Write python function for creating a random oscialltion with amplitude in range (1,5) and frequency in range (0.25,1) Create 20 different oscillations by implementing python function defined above and implement superposition principle to obtain single resultant oscillation. def oscillator (): '''implement y = Asin(wt)+Bcos(wt) with random amplitude in (1,5) and freq in (0.25,1)'''","title":"Assignment:"},{"location":"Arange-Wave/code/","text":"Wave Propagation A wave function is defined as $$Y = A \\sin(\\omega t + kx)$$ Differential equation $$\\large{\\frac{\\partial^{2}U}{dt^{2}} = c^{2}\\frac{\\partial^{2}U}{dx^{2}}}$$ import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() Define Amplitude, angular frequency, wave vector amp = 1.0 freq = 1 w = 2 * np . pi * freq #angular frequency k = 2 * np . pi #wave vector Calcualte frequency, weblength and velocity wave_length = 2 * np . pi / k velocity = freq * wave_length (freq,wave_length,velocity) (1, 1.0, 1.0) Plot the different configuration of wave motion xs = np . arange( 0 , 5 , 1 / 20 ) ts = np . arange( 0 , 5 , 1 / 20 ) len (xs), len (ts) (100, 100) 1. Experiment: Plot the particle displacement keeping time fixed When we fix the time, it is a photograph of the wave motion at specific time t0. It can be represent as $$Y = A \\sin (kx + \\phi_{0})$$ A special case is closed vibrating string plt . figure(figsize = [ 18 , 2 ]) t0 = 0.0 y = amp * np . sin(w * t0 - k * xs) plt . stem(y) plt . xlabel( \"Distance\" ) plt . ylabel( \"Amplitude\" ) plt . title( 'Particle displacements at t=' + str (t0)) Text(0.5, 1.0, 'Particle displacements at t=0.0') plt . figure(figsize = [ 18 , 18 ]) t0s = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] for i,t0 in enumerate (t0s): plt . subplot( len (t0s), 1 ,i + 1 ) y = amp * np . sin(w * t0 - k * xs) plt . stem(y) plt . title( 'Particle displacements at t=' + str (t0)) plt . show() 2. Experiment: Plot the particle displacement keeping position fixed When we fix the position and look at a single particle displacement, it behaves like a harmonic oscillator. It can be represent as $$Y = A \\sin (\\omega t + \\phi_{0})$$ A special case is harmonic oscillator plt . figure(figsize = [ 18 , 2 ]) x0 = 0.0 y = amp * np . sin(w * ts - k * x0) plt . stem(y) plt . xlabel( \"Time\" ) plt . ylabel( \"Amplitude\" ) plt . title( \"Particle motion at x=\" + str (x0)) Text(0.5, 1.0, 'Particle motion at x=0.0') plt . figure(figsize = [ 18 , 18 ]) x0s = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] for i,x0 in enumerate (t0s): plt . subplot( len (x0s), 1 ,i + 1 ) y = amp * np . sin(w * ts - k * x0) plt . stem(y) plt . title( 'Particle displacements at x=' + str (x0)) plt . show() Animation 1D Wave propagation in 1D Next Step: Wave Propagation in 2D ( numpy meshgrid )","title":"Arange demo with wave"},{"location":"Arange-Wave/code/#wave-propagation","text":"A wave function is defined as $$Y = A \\sin(\\omega t + kx)$$ Differential equation $$\\large{\\frac{\\partial^{2}U}{dt^{2}} = c^{2}\\frac{\\partial^{2}U}{dx^{2}}}$$ import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() Define Amplitude, angular frequency, wave vector amp = 1.0 freq = 1 w = 2 * np . pi * freq #angular frequency k = 2 * np . pi #wave vector Calcualte frequency, weblength and velocity wave_length = 2 * np . pi / k velocity = freq * wave_length (freq,wave_length,velocity) (1, 1.0, 1.0)","title":"Wave Propagation"},{"location":"Arange-Wave/code/#plot-the-different-configuration-of-wave-motion","text":"xs = np . arange( 0 , 5 , 1 / 20 ) ts = np . arange( 0 , 5 , 1 / 20 ) len (xs), len (ts) (100, 100)","title":"Plot the different configuration of wave motion"},{"location":"Arange-Wave/code/#1-experiment-plot-the-particle-displacement-keeping-time-fixed","text":"When we fix the time, it is a photograph of the wave motion at specific time t0. It can be represent as $$Y = A \\sin (kx + \\phi_{0})$$ A special case is closed vibrating string plt . figure(figsize = [ 18 , 2 ]) t0 = 0.0 y = amp * np . sin(w * t0 - k * xs) plt . stem(y) plt . xlabel( \"Distance\" ) plt . ylabel( \"Amplitude\" ) plt . title( 'Particle displacements at t=' + str (t0)) Text(0.5, 1.0, 'Particle displacements at t=0.0') plt . figure(figsize = [ 18 , 18 ]) t0s = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] for i,t0 in enumerate (t0s): plt . subplot( len (t0s), 1 ,i + 1 ) y = amp * np . sin(w * t0 - k * xs) plt . stem(y) plt . title( 'Particle displacements at t=' + str (t0)) plt . show()","title":"1. Experiment: Plot the particle displacement keeping time fixed"},{"location":"Arange-Wave/code/#2-experiment-plot-the-particle-displacement-keeping-position-fixed","text":"When we fix the position and look at a single particle displacement, it behaves like a harmonic oscillator. It can be represent as $$Y = A \\sin (\\omega t + \\phi_{0})$$ A special case is harmonic oscillator plt . figure(figsize = [ 18 , 2 ]) x0 = 0.0 y = amp * np . sin(w * ts - k * x0) plt . stem(y) plt . xlabel( \"Time\" ) plt . ylabel( \"Amplitude\" ) plt . title( \"Particle motion at x=\" + str (x0)) Text(0.5, 1.0, 'Particle motion at x=0.0') plt . figure(figsize = [ 18 , 18 ]) x0s = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] for i,x0 in enumerate (t0s): plt . subplot( len (x0s), 1 ,i + 1 ) y = amp * np . sin(w * ts - k * x0) plt . stem(y) plt . title( 'Particle displacements at x=' + str (x0)) plt . show()","title":"2. Experiment: Plot the particle displacement keeping position fixed"},{"location":"Arange-Wave/code/#animation-1d","text":"Wave propagation in 1D","title":"Animation 1D"},{"location":"Arange-Wave/code/#next-step","text":"Wave Propagation in 2D ( numpy meshgrid )","title":"Next Step:"},{"location":"Array-Intro/code/","text":"Array : numpy.array , numpy.array.shape , numpy.reshape , numpy.concatenate import numpy as np L = [ 1 , 3 , 5 , 7 , \"apple\" , 2.4 ] L [1, 3, 5, 7, 'apple', 2.4] L[ 4 ] 'apple' L + L [1, 3, 5, 7, 1, 3, 5, 7] A = np . array([ 1 , 2 , 3 , 4 ]) array([1, 2, 3, 4]) A = np . array([[ 2 , 3 , 5 ],[ 3 , 5 , 7 ],[ 6 , 8 , 9 ]]) np . zeros([ 3 , 4 ]) array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . ones([ 3 , 3 ]) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) X = np . random . rand( 5 , 5 ) X array([[0.24328245, 0.97738463, 0.18213799, 0.83052358, 0.3653109 ], [0.63924704, 0.32634262, 0.93738712, 0.04190707, 0.16856443], [0.02439421, 0.64339841, 0.25104598, 0.07552312, 0.33859142], [0.24346396, 0.09796353, 0.3732192 , 0.2614549 , 0.13293965], [0.13669362, 0.3105436 , 0.51570164, 0.04806873, 0.88910613]]) X = np . array([[ 1 , 3 , 4 ],[ 2 , 4 , 7 ],[ 3 , 7 , 9 ]]) X array([[1, 3, 4], [2, 4, 7], [3, 7, 9]]) LL = [[ 1 , 3 , 4 ],[ 2 , 4 , 7 ],[ 3 , 7 , 9 ]] LL[ 1 ][ 2 ] 7 X[ 1 , 2 ] 7 Array Indexing X = np . random . rand( 5 , 5 ) X array([[0.53015554, 0.34082372, 0.17875984, 0.64703264, 0.2235947 ], [0.34290729, 0.66984187, 0.40666649, 0.82657358, 0.66325498], [0.22065645, 0.93000416, 0.57442424, 0.70437704, 0.55194144], [0.06551909, 0.43306581, 0.40549847, 0.31129663, 0.99645275], [0.26043963, 0.73481174, 0.05637365, 0.2926374 , 0.82444519]]) X[ 1 ] array([0.34290729, 0.66984187, 0.40666649, 0.82657358, 0.66325498]) X[ 2 , 3 ] 0.7043770359239762 X[ 2 :] array([[0.22065645, 0.93000416, 0.57442424, 0.70437704, 0.55194144], [0.06551909, 0.43306581, 0.40549847, 0.31129663, 0.99645275], [0.26043963, 0.73481174, 0.05637365, 0.2926374 , 0.82444519]]) X[:, 1 ] array([0.34082372, 0.66984187, 0.93000416, 0.43306581, 0.73481174]) X[:, 3 ] array([0.64703264, 0.82657358, 0.70437704, 0.31129663, 0.2926374 ]) X[ 1 : 3 , 2 : 4 ] array([[0.55330373, 0.06741463], [0.97854604, 0.18927008]]) for i in range ( 5 ): for j in range ( 5 ): if i == j: print (X[i,j]) 0.530155537186793 0.6698418707020536 0.5744242448498665 0.31129663385625317 0.8244451854101277 np . diag(X) array([0.53015554, 0.66984187, 0.57442424, 0.31129663, 0.82444519]) Visualization of an array import matplotlib.pyplot as plt import seaborn as sns sns . set() X = np . random . rand( 10 , 10 ) plt . figure(figsize = [ 15 , 10 ]) sns . heatmap(X, annot = True ,cmap = \"YlGnBu\" ) <matplotlib.axes._subplots.AxesSubplot at 0x1a174a4240> Subarray : Splitting to 4 subarrays plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 2 , 2 , 1 ) sns . heatmap(X[ 0 : 5 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 2 ) sns . heatmap(X[ 5 : 10 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 3 ) sns . heatmap(X[ 0 : 5 , 5 : 10 ], annot = True ) plt . subplot( 2 , 2 , 4 ) sns . heatmap(X[ 5 : 10 , 5 : 10 ], annot = True ) plt . show() Shape: Change shape of array: from [10 by 10] to [20 by 5] Find Array shape X . shape (10, 10) Modify shape X . shape = ( 20 , 5 ) X array([[0.93852928, 0.5629153 , 0.40031156, 0.05806378, 0.97983286], [0.94297538, 0.10800161, 0.11069516, 0.90720548, 0.19857628], [0.58431666, 0.82412861, 0.92679995, 0.77921264, 0.5673404 ], [0.03117379, 0.36952646, 0.44022574, 0.33607312, 0.4144617 ], [0.43455739, 0.74942589, 0.51639448, 0.61336095, 0.48473164], [0.40440422, 0.60614083, 0.57705755, 0.79288636, 0.21180724], [0.72241987, 0.19907833, 0.80578831, 0.36106624, 0.0778791 ], [0.11341762, 0.05541729, 0.67124287, 0.59567082, 0.27976907], [0.24750836, 0.52605463, 0.97923593, 0.1903222 , 0.26829744], [0.46525502, 0.34985357, 0.44309043, 0.69688965, 0.33400436], [0.23468938, 0.94940977, 0.89626526, 0.86187936, 0.99052044], [0.96050669, 0.0065963 , 0.49770304, 0.85285478, 0.65581363], [0.09482151, 0.57702584, 0.86883705, 0.14260322, 0.10167723], [0.2856549 , 0.11293518, 0.75423858, 0.12482127, 0.68321163], [0.34892935, 0.23105718, 0.39435598, 0.2528592 , 0.74207706], [0.8628569 , 0.08761234, 0.70950303, 0.60906093, 0.15525657], [0.09659796, 0.40521082, 0.3779406 , 0.03447772, 0.06621547], [0.76406415, 0.18869095, 0.73287798, 0.98307838, 0.41521145], [0.30999207, 0.0850545 , 0.79054693, 0.72950549, 0.94548169], [0.88379333, 0.51067006, 0.08175909, 0.63589428, 0.01402678]]) Reshaping the Array X = np . arange( 35 ) X array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]) XX = np . reshape(X, ( 7 , 5 )) XX array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [30, 31, 32, 33, 34]]) X = np . arange( 35 ) . reshape( 7 , 5 ) X array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [30, 31, 32, 33, 34]]) Flatten the Array X = np . random . rand( 4 , 5 ) X . shape (4, 5) X array([[0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554], [0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586], [0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464], [0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322]]) type (X) numpy.ndarray Y = X . flatten() Y . shape (20,) Y array([0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554, 0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586, 0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464, 0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322]) Concatenate the Array A = np . random . rand( 3 , 4 ) B = np . random . rand( 3 , 4 ) Verticle addition C = np . concatenate((A,B),axis = 0 ) C array([[0.72279121, 0.69991245, 0.02998703, 0.12220733], [0.0016784 , 0.57873713, 0.43938855, 0.20021548], [0.18972308, 0.44890851, 0.76709578, 0.73554246], [0.89593371, 0.48033999, 0.45776717, 0.65204457], [0.53473347, 0.76373758, 0.67908483, 0.74560973], [0.32444887, 0.838872 , 0.72398484, 0.81445926]]) Horizontal Addition D = np . concatenate((A,B),axis = 1 ) D array([[0.72279121, 0.69991245, 0.02998703, 0.12220733, 0.89593371, 0.48033999, 0.45776717, 0.65204457], [0.0016784 , 0.57873713, 0.43938855, 0.20021548, 0.53473347, 0.76373758, 0.67908483, 0.74560973], [0.18972308, 0.44890851, 0.76709578, 0.73554246, 0.32444887, 0.838872 , 0.72398484, 0.81445926]]) Row sum and column sum X = np . random . rand( 8 , 6 ) sum of all elements X . sum() 24.566949569026413 Row sum np . sum(X,axis = 1 ) array([4.20288936, 2.3996539 , 2.46388761, 4.30291271, 2.60671914, 2.23461285, 2.68832592, 3.6679481 ]) Column sum np . sum(X,axis = 0 ) array([3.69453055, 3.84555591, 5.87500707, 2.90605569, 4.41441644, 3.83138392])","title":"Array Introduction"},{"location":"Array-Intro/code/#array","text":"numpy.array , numpy.array.shape , numpy.reshape , numpy.concatenate import numpy as np L = [ 1 , 3 , 5 , 7 , \"apple\" , 2.4 ] L [1, 3, 5, 7, 'apple', 2.4] L[ 4 ] 'apple' L + L [1, 3, 5, 7, 1, 3, 5, 7] A = np . array([ 1 , 2 , 3 , 4 ]) array([1, 2, 3, 4]) A = np . array([[ 2 , 3 , 5 ],[ 3 , 5 , 7 ],[ 6 , 8 , 9 ]]) np . zeros([ 3 , 4 ]) array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . ones([ 3 , 3 ]) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) X = np . random . rand( 5 , 5 ) X array([[0.24328245, 0.97738463, 0.18213799, 0.83052358, 0.3653109 ], [0.63924704, 0.32634262, 0.93738712, 0.04190707, 0.16856443], [0.02439421, 0.64339841, 0.25104598, 0.07552312, 0.33859142], [0.24346396, 0.09796353, 0.3732192 , 0.2614549 , 0.13293965], [0.13669362, 0.3105436 , 0.51570164, 0.04806873, 0.88910613]]) X = np . array([[ 1 , 3 , 4 ],[ 2 , 4 , 7 ],[ 3 , 7 , 9 ]]) X array([[1, 3, 4], [2, 4, 7], [3, 7, 9]]) LL = [[ 1 , 3 , 4 ],[ 2 , 4 , 7 ],[ 3 , 7 , 9 ]] LL[ 1 ][ 2 ] 7 X[ 1 , 2 ] 7","title":"Array :"},{"location":"Array-Intro/code/#array-indexing","text":"X = np . random . rand( 5 , 5 ) X array([[0.53015554, 0.34082372, 0.17875984, 0.64703264, 0.2235947 ], [0.34290729, 0.66984187, 0.40666649, 0.82657358, 0.66325498], [0.22065645, 0.93000416, 0.57442424, 0.70437704, 0.55194144], [0.06551909, 0.43306581, 0.40549847, 0.31129663, 0.99645275], [0.26043963, 0.73481174, 0.05637365, 0.2926374 , 0.82444519]]) X[ 1 ] array([0.34290729, 0.66984187, 0.40666649, 0.82657358, 0.66325498]) X[ 2 , 3 ] 0.7043770359239762 X[ 2 :] array([[0.22065645, 0.93000416, 0.57442424, 0.70437704, 0.55194144], [0.06551909, 0.43306581, 0.40549847, 0.31129663, 0.99645275], [0.26043963, 0.73481174, 0.05637365, 0.2926374 , 0.82444519]]) X[:, 1 ] array([0.34082372, 0.66984187, 0.93000416, 0.43306581, 0.73481174]) X[:, 3 ] array([0.64703264, 0.82657358, 0.70437704, 0.31129663, 0.2926374 ]) X[ 1 : 3 , 2 : 4 ] array([[0.55330373, 0.06741463], [0.97854604, 0.18927008]]) for i in range ( 5 ): for j in range ( 5 ): if i == j: print (X[i,j]) 0.530155537186793 0.6698418707020536 0.5744242448498665 0.31129663385625317 0.8244451854101277 np . diag(X) array([0.53015554, 0.66984187, 0.57442424, 0.31129663, 0.82444519])","title":"Array Indexing"},{"location":"Array-Intro/code/#visualization-of-an-array","text":"import matplotlib.pyplot as plt import seaborn as sns sns . set() X = np . random . rand( 10 , 10 ) plt . figure(figsize = [ 15 , 10 ]) sns . heatmap(X, annot = True ,cmap = \"YlGnBu\" ) <matplotlib.axes._subplots.AxesSubplot at 0x1a174a4240>","title":"Visualization of an array"},{"location":"Array-Intro/code/#subarray","text":"Splitting to 4 subarrays plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 2 , 2 , 1 ) sns . heatmap(X[ 0 : 5 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 2 ) sns . heatmap(X[ 5 : 10 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 3 ) sns . heatmap(X[ 0 : 5 , 5 : 10 ], annot = True ) plt . subplot( 2 , 2 , 4 ) sns . heatmap(X[ 5 : 10 , 5 : 10 ], annot = True ) plt . show()","title":"Subarray :"},{"location":"Array-Intro/code/#shape","text":"Change shape of array: from [10 by 10] to [20 by 5] Find Array shape X . shape (10, 10) Modify shape X . shape = ( 20 , 5 ) X array([[0.93852928, 0.5629153 , 0.40031156, 0.05806378, 0.97983286], [0.94297538, 0.10800161, 0.11069516, 0.90720548, 0.19857628], [0.58431666, 0.82412861, 0.92679995, 0.77921264, 0.5673404 ], [0.03117379, 0.36952646, 0.44022574, 0.33607312, 0.4144617 ], [0.43455739, 0.74942589, 0.51639448, 0.61336095, 0.48473164], [0.40440422, 0.60614083, 0.57705755, 0.79288636, 0.21180724], [0.72241987, 0.19907833, 0.80578831, 0.36106624, 0.0778791 ], [0.11341762, 0.05541729, 0.67124287, 0.59567082, 0.27976907], [0.24750836, 0.52605463, 0.97923593, 0.1903222 , 0.26829744], [0.46525502, 0.34985357, 0.44309043, 0.69688965, 0.33400436], [0.23468938, 0.94940977, 0.89626526, 0.86187936, 0.99052044], [0.96050669, 0.0065963 , 0.49770304, 0.85285478, 0.65581363], [0.09482151, 0.57702584, 0.86883705, 0.14260322, 0.10167723], [0.2856549 , 0.11293518, 0.75423858, 0.12482127, 0.68321163], [0.34892935, 0.23105718, 0.39435598, 0.2528592 , 0.74207706], [0.8628569 , 0.08761234, 0.70950303, 0.60906093, 0.15525657], [0.09659796, 0.40521082, 0.3779406 , 0.03447772, 0.06621547], [0.76406415, 0.18869095, 0.73287798, 0.98307838, 0.41521145], [0.30999207, 0.0850545 , 0.79054693, 0.72950549, 0.94548169], [0.88379333, 0.51067006, 0.08175909, 0.63589428, 0.01402678]])","title":"Shape:"},{"location":"Array-Intro/code/#reshaping-the-array","text":"X = np . arange( 35 ) X array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]) XX = np . reshape(X, ( 7 , 5 )) XX array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [30, 31, 32, 33, 34]]) X = np . arange( 35 ) . reshape( 7 , 5 ) X array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [30, 31, 32, 33, 34]])","title":"Reshaping the Array"},{"location":"Array-Intro/code/#flatten-the-array","text":"X = np . random . rand( 4 , 5 ) X . shape (4, 5) X array([[0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554], [0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586], [0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464], [0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322]]) type (X) numpy.ndarray Y = X . flatten() Y . shape (20,) Y array([0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554, 0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586, 0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464, 0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322])","title":"Flatten the Array"},{"location":"Array-Intro/code/#concatenate-the-array","text":"A = np . random . rand( 3 , 4 ) B = np . random . rand( 3 , 4 )","title":"Concatenate the Array"},{"location":"Array-Intro/code/#verticle-addition","text":"C = np . concatenate((A,B),axis = 0 ) C array([[0.72279121, 0.69991245, 0.02998703, 0.12220733], [0.0016784 , 0.57873713, 0.43938855, 0.20021548], [0.18972308, 0.44890851, 0.76709578, 0.73554246], [0.89593371, 0.48033999, 0.45776717, 0.65204457], [0.53473347, 0.76373758, 0.67908483, 0.74560973], [0.32444887, 0.838872 , 0.72398484, 0.81445926]])","title":"Verticle addition"},{"location":"Array-Intro/code/#horizontal-addition","text":"D = np . concatenate((A,B),axis = 1 ) D array([[0.72279121, 0.69991245, 0.02998703, 0.12220733, 0.89593371, 0.48033999, 0.45776717, 0.65204457], [0.0016784 , 0.57873713, 0.43938855, 0.20021548, 0.53473347, 0.76373758, 0.67908483, 0.74560973], [0.18972308, 0.44890851, 0.76709578, 0.73554246, 0.32444887, 0.838872 , 0.72398484, 0.81445926]])","title":"Horizontal Addition"},{"location":"Array-Intro/code/#row-sum-and-column-sum","text":"X = np . random . rand( 8 , 6 )","title":"Row sum and column sum"},{"location":"Array-Intro/code/#sum-of-all-elements","text":"X . sum() 24.566949569026413","title":"sum of all elements"},{"location":"Array-Intro/code/#row-sum","text":"np . sum(X,axis = 1 ) array([4.20288936, 2.3996539 , 2.46388761, 4.30291271, 2.60671914, 2.23461285, 2.68832592, 3.6679481 ])","title":"Row sum"},{"location":"Array-Intro/code/#column-sum","text":"np . sum(X,axis = 0 ) array([3.69453055, 3.84555591, 5.87500707, 2.90605569, 4.41441644, 3.83138392])","title":"Column sum"},{"location":"Meshgrid/code/","text":"Meshgrid : meshgrid import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() How to create a grid and it's application to ploting cost functions: xs = np . arange( 10 ) ys = np . arange( 10 ) XG,YG = np . meshgrid(xs,ys) xs array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) ys array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) XG array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) YG array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]]) plt . figure(figsize = [ 8 , 8 ]) plt . scatter(XG,YG) <matplotlib.collections.PathCollection at 0x1bd1762eb00> Example 1: Simple Cost Function: $$Z = x^2 + y^2$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG,YG = np . meshgrid(xs, ys, sparse = True ) ZG = XG ** 2 + YG ** 2 ZG array([[18. , 17.41, 16.84, ..., 16.29, 16.84, 17.41], [17.41, 16.82, 16.25, ..., 15.7 , 16.25, 16.82], [16.84, 16.25, 15.68, ..., 15.13, 15.68, 16.25], ..., [16.29, 15.7 , 15.13, ..., 14.58, 15.13, 15.7 ], [16.84, 16.25, 15.68, ..., 15.13, 15.68, 16.25], [17.41, 16.82, 16.25, ..., 15.7 , 16.25, 16.82]]) plt . figure(figsize = [ 8 , 6 ]) sns . heatmap(ZG, annot = False ,cmap = \"YlGnBu\" ,linewidths = .2 ) <matplotlib.axes._subplots.AxesSubplot at 0x1bd161a9b70> plt . figure(figsize = [ 8 , 6 ]) CS = plt . contour(xs,ys,ZG) plt . clabel(CS) <a list of 11 text.Text objects> from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D fig = plt . figure(figsize = [ 8 , 6 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . show() Example 2 : More Compex Cost Function $$\\large{Z = \\frac{sin(x^2+y^2).cos(x^2-y^2)}{x^2+y^2}}$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG, YG = np . meshgrid(xs, ys, sparse = True ) ZG = np . sin(XG ** 2 + YG ** 2 ) * np . cos(XG ** 2 - YG ** 2 ) / (XG ** 2 + YG ** 2 ) Heat Map plt . figure(figsize = [ 10 , 8 ]) sns . heatmap(ZG, annot = False ,cmap = \"YlGnBu\" ,linewidths = .2 ) <matplotlib.axes._subplots.AxesSubplot at 0x1bd167c8cf8> Contor Plot plt . figure(figsize = [ 10 , 8 ]) CS = plt . contour(xs,ys,ZG) plt . clabel(CS) <a list of 36 text.Text objects> 3D plot from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D fig = plt . figure(figsize = [ 18 , 10 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . show()","title":"Meshgrid Introduction"},{"location":"Meshgrid/code/#meshgrid-meshgrid","text":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() How to create a grid and it's application to ploting cost functions: xs = np . arange( 10 ) ys = np . arange( 10 ) XG,YG = np . meshgrid(xs,ys) xs array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) ys array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) XG array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) YG array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]]) plt . figure(figsize = [ 8 , 8 ]) plt . scatter(XG,YG) <matplotlib.collections.PathCollection at 0x1bd1762eb00>","title":"Meshgrid : meshgrid"},{"location":"Meshgrid/code/#example-1-simple-cost-function","text":"$$Z = x^2 + y^2$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG,YG = np . meshgrid(xs, ys, sparse = True ) ZG = XG ** 2 + YG ** 2 ZG array([[18. , 17.41, 16.84, ..., 16.29, 16.84, 17.41], [17.41, 16.82, 16.25, ..., 15.7 , 16.25, 16.82], [16.84, 16.25, 15.68, ..., 15.13, 15.68, 16.25], ..., [16.29, 15.7 , 15.13, ..., 14.58, 15.13, 15.7 ], [16.84, 16.25, 15.68, ..., 15.13, 15.68, 16.25], [17.41, 16.82, 16.25, ..., 15.7 , 16.25, 16.82]]) plt . figure(figsize = [ 8 , 6 ]) sns . heatmap(ZG, annot = False ,cmap = \"YlGnBu\" ,linewidths = .2 ) <matplotlib.axes._subplots.AxesSubplot at 0x1bd161a9b70> plt . figure(figsize = [ 8 , 6 ]) CS = plt . contour(xs,ys,ZG) plt . clabel(CS) <a list of 11 text.Text objects> from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D fig = plt . figure(figsize = [ 8 , 6 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . show()","title":"Example 1:  Simple Cost Function:"},{"location":"Meshgrid/code/#example-2-more-compex-cost-function","text":"$$\\large{Z = \\frac{sin(x^2+y^2).cos(x^2-y^2)}{x^2+y^2}}$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG, YG = np . meshgrid(xs, ys, sparse = True ) ZG = np . sin(XG ** 2 + YG ** 2 ) * np . cos(XG ** 2 - YG ** 2 ) / (XG ** 2 + YG ** 2 ) Heat Map plt . figure(figsize = [ 10 , 8 ]) sns . heatmap(ZG, annot = False ,cmap = \"YlGnBu\" ,linewidths = .2 ) <matplotlib.axes._subplots.AxesSubplot at 0x1bd167c8cf8> Contor Plot plt . figure(figsize = [ 10 , 8 ]) CS = plt . contour(xs,ys,ZG) plt . clabel(CS) <a list of 36 text.Text objects> 3D plot from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D fig = plt . figure(figsize = [ 18 , 10 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . show()","title":"Example 2 : More Compex  Cost Function"},{"location":"Meshgrid-OptimizationExample/code/","text":"Meshgrid : Optimization with Gradient Descent import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() Example 1: Simple Cost Function $$z = x^2 + y^2$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG, YG = np . meshgrid(xs, ys, sparse = True ) ZG = XG ** 2 + YG ** 2 Update position in Gradient Descent $$x \\leftarrow x - \\alpha \\frac{\\partial f(x,y)}{\\partial x}$$ $$y \\leftarrow y - \\alpha \\frac{\\partial f(x,y)}{\\partial y}$$ alpha = 0.01 x,y = 2 , 2 tol = 1e-5 X,Y = [],[] for i in range ( 1000 ): x = x - alpha * 2 * x y = y - alpha * 2 * y X . append(x) Y . append(y) if x * x + y * y < tol: print ( \"at step\" , i, \" minimum found!\" ,x,y,x * x + y * y) break at step 336 minimum found! 0.0022091108263220106 0.0022091108263220106 9.760341285946233e-06 from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm fig = plt . figure(figsize = [ 8 , 6 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0.5 ) Z = [x ** x + y * y for x,y in zip (X,Y)] ax . scatter3D(X,Y,Z) plt . show() plt . figure(figsize = [ 10 , 8 ]) CS = plt . contour(xs,ys,ZG) plt . scatter(X,Y,marker = \"*\" ) plt . clabel(CS) <a list of 12 text.Text objects> Example 2: More Complex Cost Function $$\\large{Z = -\\frac{sin(x^2+y^2).cos(x^2-y^2)}{x^2+y^2}}$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG, YG = np . meshgrid(xs, ys, sparse = True ) ZG = - np . sin(XG ** 2 + YG ** 2 ) * np . cos(XG ** 2 - YG ** 2 ) / (XG ** 2 + YG ** 2 ) Update position in Gradient Descent $$x \\leftarrow x - \\alpha \\frac{\\partial f(x,y)}{\\partial x}$$ $$y \\leftarrow y - \\alpha \\frac{\\partial f(x,y)}{\\partial y}$$ Optimization with Gradient Descend import sympy as sp from sympy import diff, sin, cos,exp from sympy.abc import x,y sp . diff( - sin(x * x + y * y) * cos(x * x - y * y) / (x * x + y * y),x) 2*x*sin(x**2 - y**2)*sin(x**2 + y**2)/(x**2 + y**2) - 2*x*cos(x**2 - y**2)*cos(x**2 + y**2)/(x**2 + y**2) + 2*x*sin(x**2 + y**2)*cos(x**2 - y**2)/(x**2 + y**2)**2 sp . diff( - sin(x * x + y * y) * sin(x * x - y * y) / (x * x + y * y),y) -2*y*sin(x**2 - y**2)*cos(x**2 + y**2)/(x**2 + y**2) + 2*y*sin(x**2 + y**2)*cos(x**2 - y**2)/(x**2 + y**2) + 2*y*sin(x**2 - y**2)*sin(x**2 + y**2)/(x**2 + y**2)**2 from numpy import sin,cos def fun (x,y): f = - sin(x * x + y * y) * cos(x * x - y * y) / (x * x + y * y) return f def find_diff (x,y): delx = 2 * x * sin(x ** 2 - y ** 2 ) * sin(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 ) \\ + 2 * x * cos(x ** 2 - y ** 2 ) * cos(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 ) \\ - 2 * x * sin(x ** 2 + y ** 2 ) * cos(x ** 2 - y ** 2 ) / (x ** 2 + y ** 2 ) ** 2 dely = - 2 * y * sin(x ** 2 - y ** 2 ) * sin(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 ) \\ + 2 * y * cos(x ** 2 - y ** 2 ) * cos(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 )\\ - 2 * y * sin(x ** 2 + y ** 2 ) * cos(x ** 2 - y ** 2 ) / (x ** 2 + y ** 2 ) ** 2 return delx,dely alpha = 0.01 x,y = 1 , 1 tol = 1e-5 '''There are many local minima, Gradient discend does not find global minimum''' X,Y = [],[] for i in range ( 10000 ): fdx,fdy = find_diff(x,y) x = x - alpha * fdx y = y - alpha * fdy X . append(x) Y . append(y) x,y,fun(x,y) (1.4989011738452018, 1.4989011738452018, 0.21723362821122166) from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm fig = plt . figure(figsize = [ 10 , 8 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0.0 ,\\ antialiased = False ) Z = [fun(x,y) for x,y in zip (X,Y)] ax . scatter3D(X,Y,Z) plt . show() plt . figure(figsize = [ 10 , 8 ]) CS = plt . contour(xs,ys,ZG) plt . scatter(X,Y) plt . clabel(CS) <a list of 36 text.Text objects> Other Optimization Methods: Method of Stepest Descend Newton-Raphson Method Newton-Raphson-Cartan Method Coordinate Descent Method Conjugate Gradient Method Stochastic gradient method","title":"Meshgrid demo with Optimization"},{"location":"Meshgrid-OptimizationExample/code/#meshgrid-optimization-with-gradient-descent","text":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set()","title":"Meshgrid : Optimization with Gradient Descent"},{"location":"Meshgrid-OptimizationExample/code/#example-1-simple-cost-function","text":"$$z = x^2 + y^2$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG, YG = np . meshgrid(xs, ys, sparse = True ) ZG = XG ** 2 + YG ** 2 Update position in Gradient Descent $$x \\leftarrow x - \\alpha \\frac{\\partial f(x,y)}{\\partial x}$$ $$y \\leftarrow y - \\alpha \\frac{\\partial f(x,y)}{\\partial y}$$ alpha = 0.01 x,y = 2 , 2 tol = 1e-5 X,Y = [],[] for i in range ( 1000 ): x = x - alpha * 2 * x y = y - alpha * 2 * y X . append(x) Y . append(y) if x * x + y * y < tol: print ( \"at step\" , i, \" minimum found!\" ,x,y,x * x + y * y) break at step 336 minimum found! 0.0022091108263220106 0.0022091108263220106 9.760341285946233e-06 from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm fig = plt . figure(figsize = [ 8 , 6 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0.5 ) Z = [x ** x + y * y for x,y in zip (X,Y)] ax . scatter3D(X,Y,Z) plt . show() plt . figure(figsize = [ 10 , 8 ]) CS = plt . contour(xs,ys,ZG) plt . scatter(X,Y,marker = \"*\" ) plt . clabel(CS) <a list of 12 text.Text objects>","title":"Example 1: Simple Cost Function"},{"location":"Meshgrid-OptimizationExample/code/#example-2-more-complex-cost-function","text":"$$\\large{Z = -\\frac{sin(x^2+y^2).cos(x^2-y^2)}{x^2+y^2}}$$ ns = 10 xs = np . arange( - 3 , 3 , 1 / ns) ys = np . arange( - 3 , 3 , 1 / ns) XG, YG = np . meshgrid(xs, ys, sparse = True ) ZG = - np . sin(XG ** 2 + YG ** 2 ) * np . cos(XG ** 2 - YG ** 2 ) / (XG ** 2 + YG ** 2 ) Update position in Gradient Descent $$x \\leftarrow x - \\alpha \\frac{\\partial f(x,y)}{\\partial x}$$ $$y \\leftarrow y - \\alpha \\frac{\\partial f(x,y)}{\\partial y}$$","title":"Example 2:  More Complex Cost Function"},{"location":"Meshgrid-OptimizationExample/code/#optimization-with-gradient-descend","text":"import sympy as sp from sympy import diff, sin, cos,exp from sympy.abc import x,y sp . diff( - sin(x * x + y * y) * cos(x * x - y * y) / (x * x + y * y),x) 2*x*sin(x**2 - y**2)*sin(x**2 + y**2)/(x**2 + y**2) - 2*x*cos(x**2 - y**2)*cos(x**2 + y**2)/(x**2 + y**2) + 2*x*sin(x**2 + y**2)*cos(x**2 - y**2)/(x**2 + y**2)**2 sp . diff( - sin(x * x + y * y) * sin(x * x - y * y) / (x * x + y * y),y) -2*y*sin(x**2 - y**2)*cos(x**2 + y**2)/(x**2 + y**2) + 2*y*sin(x**2 + y**2)*cos(x**2 - y**2)/(x**2 + y**2) + 2*y*sin(x**2 - y**2)*sin(x**2 + y**2)/(x**2 + y**2)**2 from numpy import sin,cos def fun (x,y): f = - sin(x * x + y * y) * cos(x * x - y * y) / (x * x + y * y) return f def find_diff (x,y): delx = 2 * x * sin(x ** 2 - y ** 2 ) * sin(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 ) \\ + 2 * x * cos(x ** 2 - y ** 2 ) * cos(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 ) \\ - 2 * x * sin(x ** 2 + y ** 2 ) * cos(x ** 2 - y ** 2 ) / (x ** 2 + y ** 2 ) ** 2 dely = - 2 * y * sin(x ** 2 - y ** 2 ) * sin(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 ) \\ + 2 * y * cos(x ** 2 - y ** 2 ) * cos(x ** 2 + y ** 2 ) / (x ** 2 + y ** 2 )\\ - 2 * y * sin(x ** 2 + y ** 2 ) * cos(x ** 2 - y ** 2 ) / (x ** 2 + y ** 2 ) ** 2 return delx,dely alpha = 0.01 x,y = 1 , 1 tol = 1e-5 '''There are many local minima, Gradient discend does not find global minimum''' X,Y = [],[] for i in range ( 10000 ): fdx,fdy = find_diff(x,y) x = x - alpha * fdx y = y - alpha * fdy X . append(x) Y . append(y) x,y,fun(x,y) (1.4989011738452018, 1.4989011738452018, 0.21723362821122166) from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm fig = plt . figure(figsize = [ 10 , 8 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0.0 ,\\ antialiased = False ) Z = [fun(x,y) for x,y in zip (X,Y)] ax . scatter3D(X,Y,Z) plt . show() plt . figure(figsize = [ 10 , 8 ]) CS = plt . contour(xs,ys,ZG) plt . scatter(X,Y) plt . clabel(CS) <a list of 36 text.Text objects>","title":"Optimization with Gradient Descend"},{"location":"Meshgrid-OptimizationExample/code/#other-optimization-methods","text":"Method of Stepest Descend Newton-Raphson Method Newton-Raphson-Cartan Method Coordinate Descent Method Conjugate Gradient Method Stochastic gradient method","title":"Other Optimization Methods:"},{"location":"Meshgrid-SimulationExample/code/","text":"Meshgrid - Simulated Annealing import numpy as np import matplotlib.pyplot as plt import random as random import seaborn as sns sns . set() Example : Simulated annealing $$V(x) = V_{o}\\left \\lbrace nA + \\sum_{i=1}^{n}[x_{i}^{2} - A \\cos(6\\pi x_{i})]\\right \\rbrace$$ $$n =2, A = 0.1, V_o = 1.0$$ $$Z = 0.2 + x^2 + y^2 -0.1 \\cos(6 \\pi x) - 0.1\\cos(6 \\pi y)$$ # Design variables at mesh points ns = 100 xs = np . arange( - 1.0 , 1.0 , 1 / ns) ys = np . arange( - 1.0 , 1.0 , 1 / ns) XG,YG = np . meshgrid(xs, ys) ZG = 0.2 + xs ** 2 + ys ** 2 \\ - 0.5 * np . cos( 6.0 * np . pi * XG)\\ - 0.5 * np . cos( 6.0 * np . pi * YG) 1. Heat Map plt . figure(figsize = [ 10 , 8 ]) sns . heatmap(ZG, annot = False ,cmap = \"YlGnBu\" ) <matplotlib.axes._subplots.AxesSubplot at 0x2489e758978> 2. Contour plot plt . figure(figsize = [ 10 , 8 ]) # Create a contour plot CS = plt . contour(XG, YG, ZG) plt . clabel(CS, inline = 1 , fontsize = 10 ) plt . title( 'Non-Convex Function' ) plt . xlabel( 'x1' ) plt . ylabel( 'x2' ) Text(0, 0.5, 'x2') 3. 3D plot from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm fig = plt . figure(figsize = [ 10 , 8 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ,\\ antialiased = False ) # Add a color bar which maps values to colors. fig . colorbar(surf, shrink = 0.5 , aspect = 5 ) plt . show() Simulate the Annealing Process def potential (x1,x2): obj = ( 2 * 10.0 ) + x1 ** 2 + x2 ** 2 - ( 10.0 ) * (np . cos( 2.0 * np . pi * x1) - np . cos( 2.0 * np . pi * x2)) return obj Setting up the parameters # Start location x_start = [ 0.5 , - 0.5 ] #Number of particles N = 2000 # Number of cycles n = 2000 # Number of trials per cycle m = 1000 # Number of accepted solutions na = 0.0 # Probability of accepting worse solution at the start p1 = 0.7 # Probability of accepting worse solution at the end p50 = 0.001 # Initial temperature t1 = - 1.0 / np . log(p1) # Final temperature t50 = - 1.0 / np . log(p50) # Fractional reduction every cycle frac = (t50 / t1) ** ( 1.0 / (n - 1.0 )) # Initialize x, particle coordinates x = np . zeros((n + 1 , 2 )) x[ 0 ] = x_start # dynamic xi xi = np . zeros( 2 ) xi = x_start na = na + 1.0 # Current best results so far xc = np . zeros( 2 ) xc = x[ 0 ] fc = potential(xi[ 0 ],xi[ 1 ]) fs = np . zeros(n + 1 ) fs[ 0 ] = fc # Current temperature t = t1 # DeltaE Average DeltaE_avg = 0.0 Performing Simulation X0 = [] Y0 = [] for i in range (n): # number of cycle #print('Cycle: ' + str(i) + ' with Temperature: ' + str(t)) for j in range (m): # number of trial # Generate new trial points xi[ 0 ] = xc[ 0 ] + random . random() - 0.5 xi[ 1 ] = xc[ 1 ] + random . random() - 0.5 # collect initial configuration if j == 0 : X0 . append(xi[ 0 ]) Y0 . append(xi[ 1 ]) # Clip to upper and lower bounds xi[ 0 ] = max ( min (xi[ 0 ], 1.0 ), - 1.0 ) xi[ 1 ] = max ( min (xi[ 1 ], 1.0 ), - 1.0 ) DeltaE = abs (potential(xi[ 0 ],xi[ 1 ]) - fc) #Metropolice step if (potential(xi[ 0 ],xi[ 1 ]) > fc): # Initialize DeltaE_avg if a worse solution was found # on the first iteration if (i == 0 and j == 0 ): DeltaE_avg = DeltaE # objective function is worse # generate probability of acceptance p = np . exp( - DeltaE / (DeltaE_avg * t)) # determine whether to accept worse point if (random . random() < p): # accept the worse solution accept = True else : # don't accept the worse solution accept = False else : # objective function is lower, automatically accept accept = True if (accept == True ): # update currently accepted solution xc[ 0 ] = xi[ 0 ] xc[ 1 ] = xi[ 1 ] fc = potential(xc[ 0 ],xc[ 1 ]) # increment number of accepted solutions na = na + 1.0 # update DeltaE_avg DeltaE_avg = (DeltaE_avg * (na - 1.0 ) + DeltaE) / na # Record the final best x values at the end of every cycle x[i + 1 ][ 0 ] = xc[ 0 ] x[i + 1 ][ 1 ] = xc[ 1 ] fs[i + 1 ] = fc # Lower the temperature for next cycle t = frac * t Result plt . figure(figsize = ( 10 , 8 )) plt . scatter(X0,Y0) #plt.savefig('inital-config.png') plt . show() <matplotlib.collections.PathCollection at 0x248a0938f98> plt . figure(figsize = ( 10 , 8 )) plt . scatter(x[:, 0 ],x[:, 1 ]) #plt.savefig('final-config.png') plt . show() Best solution: [0.99151835 0.55916777] Best objective: 1.9928084246547968 <matplotlib.collections.PathCollection at 0x248a1c58588> plt . figure(figsize = ( 18 , 4 )) plt . plot(fs, 'r.-' ) plt . legend([ 'Objective' ]) #plt.savefig('iterations.png') plt . show() plt . figure(figsize = ( 18 , 4 )) plt . plot(x[:, 0 ], 'b.-' ) plt . plot(x[:, 1 ], 'g--' ) plt . legend([ 'x1' , 'x2' ]) #plt.savefig('iterations.png') plt . show() <matplotlib.legend.Legend at 0x248a214fa90>","title":"Meshgrid demo with Simulation"},{"location":"Meshgrid-SimulationExample/code/#meshgrid-simulated-annealing","text":"import numpy as np import matplotlib.pyplot as plt import random as random import seaborn as sns sns . set()","title":"Meshgrid - Simulated Annealing"},{"location":"Meshgrid-SimulationExample/code/#example-simulated-annealing","text":"$$V(x) = V_{o}\\left \\lbrace nA + \\sum_{i=1}^{n}[x_{i}^{2} - A \\cos(6\\pi x_{i})]\\right \\rbrace$$ $$n =2, A = 0.1, V_o = 1.0$$ $$Z = 0.2 + x^2 + y^2 -0.1 \\cos(6 \\pi x) - 0.1\\cos(6 \\pi y)$$ # Design variables at mesh points ns = 100 xs = np . arange( - 1.0 , 1.0 , 1 / ns) ys = np . arange( - 1.0 , 1.0 , 1 / ns) XG,YG = np . meshgrid(xs, ys) ZG = 0.2 + xs ** 2 + ys ** 2 \\ - 0.5 * np . cos( 6.0 * np . pi * XG)\\ - 0.5 * np . cos( 6.0 * np . pi * YG)","title":"Example : Simulated annealing"},{"location":"Meshgrid-SimulationExample/code/#1-heat-map","text":"plt . figure(figsize = [ 10 , 8 ]) sns . heatmap(ZG, annot = False ,cmap = \"YlGnBu\" ) <matplotlib.axes._subplots.AxesSubplot at 0x2489e758978>","title":"1. Heat Map"},{"location":"Meshgrid-SimulationExample/code/#2-contour-plot","text":"plt . figure(figsize = [ 10 , 8 ]) # Create a contour plot CS = plt . contour(XG, YG, ZG) plt . clabel(CS, inline = 1 , fontsize = 10 ) plt . title( 'Non-Convex Function' ) plt . xlabel( 'x1' ) plt . ylabel( 'x2' ) Text(0, 0.5, 'x2')","title":"2. Contour plot"},{"location":"Meshgrid-SimulationExample/code/#3-3d-plot","text":"from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm fig = plt . figure(figsize = [ 10 , 8 ]) ax = fig . gca(projection = '3d' ) # Plot the surface. surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ,\\ antialiased = False ) # Add a color bar which maps values to colors. fig . colorbar(surf, shrink = 0.5 , aspect = 5 ) plt . show()","title":"3. 3D plot"},{"location":"Meshgrid-SimulationExample/code/#simulate-the-annealing-process","text":"def potential (x1,x2): obj = ( 2 * 10.0 ) + x1 ** 2 + x2 ** 2 - ( 10.0 ) * (np . cos( 2.0 * np . pi * x1) - np . cos( 2.0 * np . pi * x2)) return obj","title":"Simulate the Annealing Process"},{"location":"Meshgrid-SimulationExample/code/#setting-up-the-parameters","text":"# Start location x_start = [ 0.5 , - 0.5 ] #Number of particles N = 2000 # Number of cycles n = 2000 # Number of trials per cycle m = 1000 # Number of accepted solutions na = 0.0 # Probability of accepting worse solution at the start p1 = 0.7 # Probability of accepting worse solution at the end p50 = 0.001 # Initial temperature t1 = - 1.0 / np . log(p1) # Final temperature t50 = - 1.0 / np . log(p50) # Fractional reduction every cycle frac = (t50 / t1) ** ( 1.0 / (n - 1.0 )) # Initialize x, particle coordinates x = np . zeros((n + 1 , 2 )) x[ 0 ] = x_start # dynamic xi xi = np . zeros( 2 ) xi = x_start na = na + 1.0 # Current best results so far xc = np . zeros( 2 ) xc = x[ 0 ] fc = potential(xi[ 0 ],xi[ 1 ]) fs = np . zeros(n + 1 ) fs[ 0 ] = fc # Current temperature t = t1 # DeltaE Average DeltaE_avg = 0.0","title":"Setting up the parameters"},{"location":"Meshgrid-SimulationExample/code/#performing-simulation","text":"X0 = [] Y0 = [] for i in range (n): # number of cycle #print('Cycle: ' + str(i) + ' with Temperature: ' + str(t)) for j in range (m): # number of trial # Generate new trial points xi[ 0 ] = xc[ 0 ] + random . random() - 0.5 xi[ 1 ] = xc[ 1 ] + random . random() - 0.5 # collect initial configuration if j == 0 : X0 . append(xi[ 0 ]) Y0 . append(xi[ 1 ]) # Clip to upper and lower bounds xi[ 0 ] = max ( min (xi[ 0 ], 1.0 ), - 1.0 ) xi[ 1 ] = max ( min (xi[ 1 ], 1.0 ), - 1.0 ) DeltaE = abs (potential(xi[ 0 ],xi[ 1 ]) - fc) #Metropolice step if (potential(xi[ 0 ],xi[ 1 ]) > fc): # Initialize DeltaE_avg if a worse solution was found # on the first iteration if (i == 0 and j == 0 ): DeltaE_avg = DeltaE # objective function is worse # generate probability of acceptance p = np . exp( - DeltaE / (DeltaE_avg * t)) # determine whether to accept worse point if (random . random() < p): # accept the worse solution accept = True else : # don't accept the worse solution accept = False else : # objective function is lower, automatically accept accept = True if (accept == True ): # update currently accepted solution xc[ 0 ] = xi[ 0 ] xc[ 1 ] = xi[ 1 ] fc = potential(xc[ 0 ],xc[ 1 ]) # increment number of accepted solutions na = na + 1.0 # update DeltaE_avg DeltaE_avg = (DeltaE_avg * (na - 1.0 ) + DeltaE) / na # Record the final best x values at the end of every cycle x[i + 1 ][ 0 ] = xc[ 0 ] x[i + 1 ][ 1 ] = xc[ 1 ] fs[i + 1 ] = fc # Lower the temperature for next cycle t = frac * t","title":"Performing Simulation"},{"location":"Meshgrid-SimulationExample/code/#result","text":"plt . figure(figsize = ( 10 , 8 )) plt . scatter(X0,Y0) #plt.savefig('inital-config.png') plt . show() <matplotlib.collections.PathCollection at 0x248a0938f98> plt . figure(figsize = ( 10 , 8 )) plt . scatter(x[:, 0 ],x[:, 1 ]) #plt.savefig('final-config.png') plt . show() Best solution: [0.99151835 0.55916777] Best objective: 1.9928084246547968 <matplotlib.collections.PathCollection at 0x248a1c58588> plt . figure(figsize = ( 18 , 4 )) plt . plot(fs, 'r.-' ) plt . legend([ 'Objective' ]) #plt.savefig('iterations.png') plt . show() plt . figure(figsize = ( 18 , 4 )) plt . plot(x[:, 0 ], 'b.-' ) plt . plot(x[:, 1 ], 'g--' ) plt . legend([ 'x1' , 'x2' ]) #plt.savefig('iterations.png') plt . show() <matplotlib.legend.Legend at 0x248a214fa90>","title":"Result"},{"location":"Meshgrid-SphericalHrmonics/code/","text":"Meshgrid : Spherical Harmonics import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set() Example : Spherical Harmonics $$Y(\\theta,\\phi) : \\langle \\theta,\\phi|l,m \\rangle $$ value of l value of m Spherical Harmonics in position space l=0 m=0 $$ Y_0^0(\\theta,\\phi)=\\frac{1}{2}\\sqrt{\\frac{1}{\\pi}}$$ l=1 m=-1 $$ Y_1^{-1}(\\theta, \\phi) = \\frac{1}{2} \\sqrt{\\frac{3}{2\\pi}} e^{-i\\theta} \\sin(\\phi) $$ l=1 m=0 $$ Y_1^0(\\theta, \\phi) = \\frac{1}{2} \\sqrt{\\frac{3}{\\pi}} \\cos(\\phi) $$ l=1 m=1 $$ Y_1^1(\\theta, \\phi) = -\\frac{1}{2} \\sqrt{\\frac{3}{2\\pi}} e^{i\\theta} \\sin(\\phi) $$ import scipy.special as sp from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D phis,thetas = np . mgrid[ 0 : 2 * np . pi: 200 j, 0 :np . pi: 100 j] Example 1 : $$Y_0^0(\\theta, \\phi) = \\frac{1}{2} \\sqrt{\\frac{1}{\\pi}}$$ l = 0 m = 0 RG = np . abs(sp . sph_harm(m,l,phis,thetas)) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^1_ 0|$' , fontsize = 20 ) plt . show() Example 2: $$ Y_2^1(\\theta, \\phi)$$ l = 2 m = 1 RG = np . abs(sp . sph_harm(m,l,phis,thetas)) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^1_ 0|$' , fontsize = 20 ) plt . show() Example 3: $$Y_4^2(\\theta, \\phi)$$ Real part l = 4 m = 2 RG = np . abs(sp . sph_harm(m,l,phis,thetas) . real) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^4_ 2|$' , fontsize = 20 ) plt . show() Example 4 : $$Y_5^4(\\theta, \\phi)$$ Imaginary Part l = 5 m = 4 RG = np . abs(sp . sph_harm(m,l,phis,thetas) . imag) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^5_ 4|$' , fontsize = 20 ) plt . show()","title":"Meshgrid demo with Spherical Harmonics"},{"location":"Meshgrid-SphericalHrmonics/code/#meshgrid-spherical-harmonics","text":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns . set()","title":"Meshgrid : Spherical Harmonics"},{"location":"Meshgrid-SphericalHrmonics/code/#example-spherical-harmonics","text":"$$Y(\\theta,\\phi) : \\langle \\theta,\\phi|l,m \\rangle $$ value of l value of m Spherical Harmonics in position space l=0 m=0 $$ Y_0^0(\\theta,\\phi)=\\frac{1}{2}\\sqrt{\\frac{1}{\\pi}}$$ l=1 m=-1 $$ Y_1^{-1}(\\theta, \\phi) = \\frac{1}{2} \\sqrt{\\frac{3}{2\\pi}} e^{-i\\theta} \\sin(\\phi) $$ l=1 m=0 $$ Y_1^0(\\theta, \\phi) = \\frac{1}{2} \\sqrt{\\frac{3}{\\pi}} \\cos(\\phi) $$ l=1 m=1 $$ Y_1^1(\\theta, \\phi) = -\\frac{1}{2} \\sqrt{\\frac{3}{2\\pi}} e^{i\\theta} \\sin(\\phi) $$ import scipy.special as sp from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D phis,thetas = np . mgrid[ 0 : 2 * np . pi: 200 j, 0 :np . pi: 100 j]","title":"Example : Spherical Harmonics"},{"location":"Meshgrid-SphericalHrmonics/code/#example-1-y_00theta-phi-frac12-sqrtfrac1pi","text":"l = 0 m = 0 RG = np . abs(sp . sph_harm(m,l,phis,thetas)) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^1_ 0|$' , fontsize = 20 ) plt . show()","title":"Example 1 : $$Y_0^0(\\theta, \\phi) = \\frac{1}{2} \\sqrt{\\frac{1}{\\pi}}$$"},{"location":"Meshgrid-SphericalHrmonics/code/#example-2-y_21theta-phi","text":"l = 2 m = 1 RG = np . abs(sp . sph_harm(m,l,phis,thetas)) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^1_ 0|$' , fontsize = 20 ) plt . show()","title":"Example 2: $$ Y_2^1(\\theta, \\phi)$$"},{"location":"Meshgrid-SphericalHrmonics/code/#example-3-y_42theta-phi-real-part","text":"l = 4 m = 2 RG = np . abs(sp . sph_harm(m,l,phis,thetas) . real) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^4_ 2|$' , fontsize = 20 ) plt . show()","title":"Example 3: $$Y_4^2(\\theta, \\phi)$$ Real part"},{"location":"Meshgrid-SphericalHrmonics/code/#example-4-y_54theta-phi-imaginary-part","text":"l = 5 m = 4 RG = np . abs(sp . sph_harm(m,l,phis,thetas) . imag) '''spherical to cartesian''' XG = RG * np . sin(thetas) * np . cos(phis) YG = RG * np . sin(thetas) * np . sin(phis) ZG = RG * np . cos(thetas) '''normalization with maximum value''' NG = RG / RG . max() '''plot figure''' fig = plt . figure(figsize = [ 8 , 8 ]) ax = fig . gca(projection = '3d' ) surf = ax . plot_surface(XG, YG, ZG,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . title( r'$|Y^5_ 4|$' , fontsize = 20 ) plt . show()","title":"Example 4 : $$Y_5^4(\\theta, \\phi)$$ Imaginary Part"},{"location":"Statistics-Probability/code/","text":"Satistics : Probability Distribution https://numpy.org/doc/stable/reference/random/legacy.html 1. Normal Distribution import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns import numpy as np import pandas as pd import math as math sns . set() $$f(x,\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} e ^{\\frac{-(x-\\mu)^{2}}{2\\sigma^{2}}}$$ def normal (x,m,s): f = ( 1 / np . sqrt( 2 * np . pi * s)) * np . exp( - (x - m) ** 2 / ( 2 * s ** 2 )) return f x = np . arange( - 20 , 20 , 0.01 ) plt . figure(figsize = [ 8 , 6 ]) plt . plot(x,normal(x, 1.0 , 1.0 ),x,normal(x, 2.0 , 2.0 ),\\ x,normal(x, 3.0 , 3.0 ),x,normal(x, 4.0 , 4.0 ),\\ x,normal(x, 5.0 , 5.0 ),x,normal(x, 6.0 , 6.0 )) plt . show() Sampling Normal distribution mu, sigma = 0 , 0.1 # mean and standard deviation s = np . random . normal(mu, sigma, 1000 ) s[ 0 : 10 ] array([-0.00682791, -0.08991848, 0.24455438, 0.14724173, -0.01939023, 0.07666377, -0.00425391, -0.04374468, -0.03458053, -0.08640109]) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x18e5f580f98> Lets check this with uniform distribution s0 = np . random . rand( 10000 ) sns . distplot(s0) <matplotlib.axes._subplots.AxesSubplot at 0x18e5f7a5240> 2. Binomial Distribution The probability density for the binomial distribution is $$P(N) = \\binom{n}{N}p^N(1-p)^{n-N}$$, where n is the number of trials, p is the probability of success, and N is the number of successes. When estimating the standard error of a proportion in a population by using a random sample, the normal distribution works well unless the product p n <=5, where p = population proportion estimate, and n = number of samples, in which case the binomial distribution is used instead. For example, a sample of 15 people shows 4 who are left handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27 15 = 4, so the binomial distribution should be used in this case. Tossing n coins | throwing 10 balls in 2 compartments # number of coins in one experiment n = 10 # probability for head p = .5 #number of experiment N = 1000 s = np . random . binomial(n, p, 1000 ) s[ 0 : 10 ] array([5, 4, 5, 6, 3, 3, 5, 3, 2, 6]) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x18e60ba5828> Taking 100 coins n, p = 100 , .5 # number of trials, probability of each trial s = np . random . binomial(n, p, 100000 ) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x18e60bd0a90> 3. Multinomial Distribution The multinomial distribution is a multivariate generalization of the binomial distribution. Take an experiment with one of p possible outcomes. An example of such an experiment is throwing a dice, where the outcome can be 1 through 6. Each sample drawn from the distribution represents n such experiments. Its values, X_i = [X_0, X_1, ..., X_p] , represent the number of times the outcome was i. Tossing n-number of p-side dice # number of dice in one experiment n = 20 # number of face p = 6 #number of experiment N = 100 s = np . random . multinomial(n, [ 1 / float (p)] * 6 , 1000 ) s[ 0 : 5 ] array([[4, 4, 3, 3, 1, 5], [4, 4, 2, 4, 5, 1], [2, 5, 2, 3, 3, 5], [6, 5, 3, 1, 3, 2], [4, 4, 3, 2, 3, 4]]) data = [] for item in s: data . append({ \"1\" :item[ 0 ], \"2\" :item[ 1 ], \"3\" :item[ 2 ], \"4\" :item[ 3 ], \"5\" :item[ 4 ], \"6\" :item[ 5 ]}) DF = pd . DataFrame(data) DF[ 0 : 20 ] . plot(figsize = [ 18 , 4 ]) <matplotlib.axes._subplots.AxesSubplot at 0x18e60ccc2b0> DF . hist(figsize = [ 15 , 8 ], bins = 10 ) plt . show <function matplotlib.pyplot.show(*args, **kw)> 4. $\\chi^{2} $ Distribution When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing. s = np . random . chisquare( 2 , 10 ) s array([1.26510584, 0.84928556, 2.36818992, 0.54182822, 0.16216812, 0.45026504, 0.31461063, 0.14277381, 3.8927845 , 2.3985151 ]) plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 3 , 2 , 1 ) s = np . random . chisquare( 2 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 2 ) s = np . random . chisquare( 3 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 3 ) s = np . random . chisquare( 4 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 4 ) s = np . random . chisquare( 5 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 5 ) s = np . random . chisquare( 6 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 6 ) s = np . random . chisquare( 7 , 1000 ) sns . distplot(s, bins = 20 ) plt . show() 5. The Poisson distribution $$f(k; \\lambda)=\\frac{\\lambda^k e^{-\\lambda}}{k!}$$ For events with an expected separation $\\lambda$ the Poisson distribution $f(k; \\lambda)$ describes the probability of k events occurring within the observed interval $\\lambda$. s = np . random . poisson( 5 , 10000 ) s[ 0 : 10 ] array([4, 3, 7, 6, 2, 4, 7, 4, 8, 3]) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x15e25adc400> 6. Multivariate Gaussian Distribution (Gaussian Mixture) | from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D fig = plt . figure(figsize = [ 8 , 6 ]) ax = fig . gca(projection = '3d' ) size = 100 sigma_x = 4. sigma_y = 4. x = np . linspace( - 10 , 10 , size) y = np . linspace( - 10 , 10 , size) xg, yg = np . meshgrid(x, y) zg = ( 1 / ( 2 * np . pi * sigma_x * sigma_y) * np . exp( - (xg ** 2 / ( 2 * sigma_x ** 2 ) + yg ** 2 / ( 2 * sigma_y ** 2 )))) # Plot the surface. surf = ax . plot_surface(xg,yg,zg,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . show() 2D Gaussian Random numbers mean = [ 0 , 0 ] cov = [[ 1 , 0.5 ], [ 0.5 , 1 ]] x, y = np . random . multivariate_normal(mean, cov, 5000 ) . T plt . plot(x, y, 'x' ) plt . axis( 'equal' ) plt . show() 3D - Gaussian random numbers mean = [ 0 , 0 , 0 ] cov = [[ 1 , 0.5 , 0.5 ], [ 0.5 , 1 , 0.5 ], [ 0.5 , 0.5 , 1 ]] x, y, z = np . random . multivariate_normal(mean, cov, 5000 ) . T from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt fig = plt . figure(figsize = [ 10 , 8 ]) ax = fig . add_subplot( 111 , projection = '3d' ) ax . scatter(x, y, z, c = 'g' , marker = 'x' ) ax . set_xlabel( 'X Label' ) ax . set_ylabel( 'Y Label' ) ax . set_zlabel( 'Z Label' ) plt . show() Connection to Machine Learning (Optional) 1. Kernel Density Estimation In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. Let $(x1, x2, \u2026, xn)$ be a univariate independent and identically distributed sample drawn from some distribution with an unknown density \u0192. We are interested in estimating the shape of this function \u0192. Its kernel density estimator is $$ {f} {h}(x)={\\frac {1}{n}}\\sum {i=1}^{n}K_{h}(x-x_{i})={\\frac {1}{nh}}\\sum {i=1}^{n}K{\\Big (}{\\frac {x-x {i}}{h}}{\\Big )}$$ where K is the kernel \u2014 a non-negative function \u2014 and h > 0 is a smoothing parameter called the bandwidth. Data generater def generate_data (seed = 17 ): # Fix the seed to reproduce the results rand = np . random . RandomState(seed) x = [] dat = rand . lognormal( 0 , 0.3 , 1000 ) x = np . concatenate((x, dat)) dat = rand . normal( 3 , 1 , 1000 ) x = np . concatenate((x, dat)) return x Data exploration x_train = generate_data()[:, np . newaxis] fig, ax = plt . subplots(nrows = 1 , ncols = 3 , figsize = ( 15 , 5 )) plt . subplot( 131 ) plt . scatter(np . arange( len (x_train)), x_train, c = 'red' ) plt . xlabel( 'Sample no.' ) plt . ylabel( 'Value' ) plt . title( 'Scatter plot' ) plt . subplot( 132 ) plt . hist(x_train, bins = 50 ) plt . title( 'Histogram' ) fig . subplots_adjust(wspace = .3 ) plt . subplot( 133 ) sns . distplot(x_train, bins = 50 ) plt . title( 'Seaborn-kde' ) fig . subplots_adjust(wspace = .3 ) plt . show() Implement KDE with test data from sklearn.neighbors import KernelDensity x_test = np . linspace( - 1 , 7 , 2000 )[:, np . newaxis] model = KernelDensity() model . fit(x_train) log_dens = model . score_samples(x_test) plt . fill(x_test, np . exp(log_dens), c = 'b' ) pass Setting bandwidth in Kerneldensity bandwidths = [ 0.01 , 0.05 , 0.1 , 0.5 , 1 , 4 ] fig, ax = plt . subplots(nrows = 2 , ncols = 3 , figsize = ( 10 , 7 )) plt_ind = np . arange( 6 ) + 231 for b, ind in zip (bandwidths, plt_ind): kde_model = KernelDensity(kernel = 'gaussian' , bandwidth = b) kde_model . fit(x_train) score = kde_model . score_samples(x_test) plt . subplot(ind) plt . fill(x_test, np . exp(score), c = 'b' ) plt . title( \"h=\" + str (b)) fig . subplots_adjust(hspace = 0.5 , wspace = .3 ) plt . show() Mini Assignment: Generate two (1-d) random normal distribution samples with two means seperated by 1 unit. Find the representative distribution by implementation of kernelmdesity estimation. 2. Gaussian Mixture Model (Optional) from matplotlib.colors import LogNorm from sklearn import mixture n_samples = 1000 # generate random sample, two components np . random . seed( 0 ) # generate spherical data centered on (20, 20) shifted_gaussian = np . random . randn(n_samples, 2 ) + np . array([ 20 , 20 ]) # generate zero centered stretched Gaussian data C = np . array([[ 0.5 , - 0.7 ], [ 3.5 , 1.7 ]]) stretched_gaussian = np . dot(np . random . randn(n_samples, 2 ), C) # concatenate the two datasets into the final training set X_train = np . vstack([shifted_gaussian, stretched_gaussian]) # fit a Gaussian Mixture Model with two components clf = mixture . GaussianMixture(n_components = 2 , covariance_type = 'full' ) clf . fit(X_train) GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100, means_init=None, n_components=2, n_init=1, precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weights_init=None) x = np . linspace( - 20. , 30. ) y = np . linspace( - 20. , 40. ) X, Y = np . meshgrid(x, y) XX = np . array([X . ravel(), Y . ravel()]) . T Z = - clf . score_samples(XX) Z = Z . reshape(X . shape) plt . figure(figsize = [ 12 , 10 ]) # display predicted scores by the model as a contour plot CS = plt . contour(X, Y, Z, norm = LogNorm(vmin = 1.0 , vmax = 1000.0 ), levels = np . logspace( 0 , 3 , 10 )) CB = plt . colorbar(CS, shrink = 0.8 , extend = 'both' ) plt . scatter(X_train[:, 0 ], X_train[:, 1 ], .8 ) plt . title( 'Negative log-likelihood predicted by a GMM' ) plt . axis( 'tight' ) plt . show()","title":"Statistics with Probability"},{"location":"Statistics-Probability/code/#satistics-probability-distribution","text":"https://numpy.org/doc/stable/reference/random/legacy.html","title":"Satistics : Probability Distribution"},{"location":"Statistics-Probability/code/#1-normal-distribution","text":"import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns import numpy as np import pandas as pd import math as math sns . set() $$f(x,\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} e ^{\\frac{-(x-\\mu)^{2}}{2\\sigma^{2}}}$$ def normal (x,m,s): f = ( 1 / np . sqrt( 2 * np . pi * s)) * np . exp( - (x - m) ** 2 / ( 2 * s ** 2 )) return f x = np . arange( - 20 , 20 , 0.01 ) plt . figure(figsize = [ 8 , 6 ]) plt . plot(x,normal(x, 1.0 , 1.0 ),x,normal(x, 2.0 , 2.0 ),\\ x,normal(x, 3.0 , 3.0 ),x,normal(x, 4.0 , 4.0 ),\\ x,normal(x, 5.0 , 5.0 ),x,normal(x, 6.0 , 6.0 )) plt . show()","title":"1. Normal Distribution"},{"location":"Statistics-Probability/code/#sampling-normal-distribution","text":"mu, sigma = 0 , 0.1 # mean and standard deviation s = np . random . normal(mu, sigma, 1000 ) s[ 0 : 10 ] array([-0.00682791, -0.08991848, 0.24455438, 0.14724173, -0.01939023, 0.07666377, -0.00425391, -0.04374468, -0.03458053, -0.08640109]) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x18e5f580f98> Lets check this with uniform distribution s0 = np . random . rand( 10000 ) sns . distplot(s0) <matplotlib.axes._subplots.AxesSubplot at 0x18e5f7a5240>","title":"Sampling Normal distribution"},{"location":"Statistics-Probability/code/#2-binomial-distribution","text":"The probability density for the binomial distribution is $$P(N) = \\binom{n}{N}p^N(1-p)^{n-N}$$, where n is the number of trials, p is the probability of success, and N is the number of successes. When estimating the standard error of a proportion in a population by using a random sample, the normal distribution works well unless the product p n <=5, where p = population proportion estimate, and n = number of samples, in which case the binomial distribution is used instead. For example, a sample of 15 people shows 4 who are left handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27 15 = 4, so the binomial distribution should be used in this case. Tossing n coins | throwing 10 balls in 2 compartments # number of coins in one experiment n = 10 # probability for head p = .5 #number of experiment N = 1000 s = np . random . binomial(n, p, 1000 ) s[ 0 : 10 ] array([5, 4, 5, 6, 3, 3, 5, 3, 2, 6]) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x18e60ba5828> Taking 100 coins n, p = 100 , .5 # number of trials, probability of each trial s = np . random . binomial(n, p, 100000 ) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x18e60bd0a90>","title":"2. Binomial Distribution"},{"location":"Statistics-Probability/code/#3-multinomial-distribution","text":"The multinomial distribution is a multivariate generalization of the binomial distribution. Take an experiment with one of p possible outcomes. An example of such an experiment is throwing a dice, where the outcome can be 1 through 6. Each sample drawn from the distribution represents n such experiments. Its values, X_i = [X_0, X_1, ..., X_p] , represent the number of times the outcome was i. Tossing n-number of p-side dice # number of dice in one experiment n = 20 # number of face p = 6 #number of experiment N = 100 s = np . random . multinomial(n, [ 1 / float (p)] * 6 , 1000 ) s[ 0 : 5 ] array([[4, 4, 3, 3, 1, 5], [4, 4, 2, 4, 5, 1], [2, 5, 2, 3, 3, 5], [6, 5, 3, 1, 3, 2], [4, 4, 3, 2, 3, 4]]) data = [] for item in s: data . append({ \"1\" :item[ 0 ], \"2\" :item[ 1 ], \"3\" :item[ 2 ], \"4\" :item[ 3 ], \"5\" :item[ 4 ], \"6\" :item[ 5 ]}) DF = pd . DataFrame(data) DF[ 0 : 20 ] . plot(figsize = [ 18 , 4 ]) <matplotlib.axes._subplots.AxesSubplot at 0x18e60ccc2b0> DF . hist(figsize = [ 15 , 8 ], bins = 10 ) plt . show <function matplotlib.pyplot.show(*args, **kw)>","title":"3. Multinomial Distribution"},{"location":"Statistics-Probability/code/#4-chi2-distribution","text":"When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing. s = np . random . chisquare( 2 , 10 ) s array([1.26510584, 0.84928556, 2.36818992, 0.54182822, 0.16216812, 0.45026504, 0.31461063, 0.14277381, 3.8927845 , 2.3985151 ]) plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 3 , 2 , 1 ) s = np . random . chisquare( 2 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 2 ) s = np . random . chisquare( 3 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 3 ) s = np . random . chisquare( 4 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 4 ) s = np . random . chisquare( 5 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 5 ) s = np . random . chisquare( 6 , 1000 ) sns . distplot(s, bins = 20 ) plt . subplot( 3 , 2 , 6 ) s = np . random . chisquare( 7 , 1000 ) sns . distplot(s, bins = 20 ) plt . show()","title":"4.  $\\chi^{2} $ Distribution"},{"location":"Statistics-Probability/code/#5-the-poisson-distribution","text":"$$f(k; \\lambda)=\\frac{\\lambda^k e^{-\\lambda}}{k!}$$ For events with an expected separation $\\lambda$ the Poisson distribution $f(k; \\lambda)$ describes the probability of k events occurring within the observed interval $\\lambda$. s = np . random . poisson( 5 , 10000 ) s[ 0 : 10 ] array([4, 3, 7, 6, 2, 4, 7, 4, 8, 3]) sns . distplot(s) <matplotlib.axes._subplots.AxesSubplot at 0x15e25adc400>","title":"5. The Poisson distribution"},{"location":"Statistics-Probability/code/#6-multivariate-gaussian-distribution-gaussian-mixture","text":"| from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D fig = plt . figure(figsize = [ 8 , 6 ]) ax = fig . gca(projection = '3d' ) size = 100 sigma_x = 4. sigma_y = 4. x = np . linspace( - 10 , 10 , size) y = np . linspace( - 10 , 10 , size) xg, yg = np . meshgrid(x, y) zg = ( 1 / ( 2 * np . pi * sigma_x * sigma_y) * np . exp( - (xg ** 2 / ( 2 * sigma_x ** 2 ) + yg ** 2 / ( 2 * sigma_y ** 2 )))) # Plot the surface. surf = ax . plot_surface(xg,yg,zg,\\ cmap = cm . coolwarm,\\ linewidth = 0 ) plt . show() 2D Gaussian Random numbers mean = [ 0 , 0 ] cov = [[ 1 , 0.5 ], [ 0.5 , 1 ]] x, y = np . random . multivariate_normal(mean, cov, 5000 ) . T plt . plot(x, y, 'x' ) plt . axis( 'equal' ) plt . show() 3D - Gaussian random numbers mean = [ 0 , 0 , 0 ] cov = [[ 1 , 0.5 , 0.5 ], [ 0.5 , 1 , 0.5 ], [ 0.5 , 0.5 , 1 ]] x, y, z = np . random . multivariate_normal(mean, cov, 5000 ) . T from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt fig = plt . figure(figsize = [ 10 , 8 ]) ax = fig . add_subplot( 111 , projection = '3d' ) ax . scatter(x, y, z, c = 'g' , marker = 'x' ) ax . set_xlabel( 'X Label' ) ax . set_ylabel( 'Y Label' ) ax . set_zlabel( 'Z Label' ) plt . show()","title":"6. Multivariate Gaussian Distribution (Gaussian Mixture)"},{"location":"Statistics-Probability/code/#connection-to-machine-learning-optional","text":"","title":"Connection to Machine Learning (Optional)"},{"location":"Statistics-Probability/code/#1-kernel-density-estimation","text":"In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. Let $(x1, x2, \u2026, xn)$ be a univariate independent and identically distributed sample drawn from some distribution with an unknown density \u0192. We are interested in estimating the shape of this function \u0192. Its kernel density estimator is $$ {f} {h}(x)={\\frac {1}{n}}\\sum {i=1}^{n}K_{h}(x-x_{i})={\\frac {1}{nh}}\\sum {i=1}^{n}K{\\Big (}{\\frac {x-x {i}}{h}}{\\Big )}$$ where K is the kernel \u2014 a non-negative function \u2014 and h > 0 is a smoothing parameter called the bandwidth. Data generater def generate_data (seed = 17 ): # Fix the seed to reproduce the results rand = np . random . RandomState(seed) x = [] dat = rand . lognormal( 0 , 0.3 , 1000 ) x = np . concatenate((x, dat)) dat = rand . normal( 3 , 1 , 1000 ) x = np . concatenate((x, dat)) return x Data exploration x_train = generate_data()[:, np . newaxis] fig, ax = plt . subplots(nrows = 1 , ncols = 3 , figsize = ( 15 , 5 )) plt . subplot( 131 ) plt . scatter(np . arange( len (x_train)), x_train, c = 'red' ) plt . xlabel( 'Sample no.' ) plt . ylabel( 'Value' ) plt . title( 'Scatter plot' ) plt . subplot( 132 ) plt . hist(x_train, bins = 50 ) plt . title( 'Histogram' ) fig . subplots_adjust(wspace = .3 ) plt . subplot( 133 ) sns . distplot(x_train, bins = 50 ) plt . title( 'Seaborn-kde' ) fig . subplots_adjust(wspace = .3 ) plt . show() Implement KDE with test data from sklearn.neighbors import KernelDensity x_test = np . linspace( - 1 , 7 , 2000 )[:, np . newaxis] model = KernelDensity() model . fit(x_train) log_dens = model . score_samples(x_test) plt . fill(x_test, np . exp(log_dens), c = 'b' ) pass Setting bandwidth in Kerneldensity bandwidths = [ 0.01 , 0.05 , 0.1 , 0.5 , 1 , 4 ] fig, ax = plt . subplots(nrows = 2 , ncols = 3 , figsize = ( 10 , 7 )) plt_ind = np . arange( 6 ) + 231 for b, ind in zip (bandwidths, plt_ind): kde_model = KernelDensity(kernel = 'gaussian' , bandwidth = b) kde_model . fit(x_train) score = kde_model . score_samples(x_test) plt . subplot(ind) plt . fill(x_test, np . exp(score), c = 'b' ) plt . title( \"h=\" + str (b)) fig . subplots_adjust(hspace = 0.5 , wspace = .3 ) plt . show()","title":"1. Kernel Density Estimation"},{"location":"Statistics-Probability/code/#mini-assignment","text":"Generate two (1-d) random normal distribution samples with two means seperated by 1 unit. Find the representative distribution by implementation of kernelmdesity estimation.","title":"Mini Assignment:"},{"location":"Statistics-Probability/code/#2-gaussian-mixture-model-optional","text":"from matplotlib.colors import LogNorm from sklearn import mixture n_samples = 1000 # generate random sample, two components np . random . seed( 0 ) # generate spherical data centered on (20, 20) shifted_gaussian = np . random . randn(n_samples, 2 ) + np . array([ 20 , 20 ]) # generate zero centered stretched Gaussian data C = np . array([[ 0.5 , - 0.7 ], [ 3.5 , 1.7 ]]) stretched_gaussian = np . dot(np . random . randn(n_samples, 2 ), C) # concatenate the two datasets into the final training set X_train = np . vstack([shifted_gaussian, stretched_gaussian]) # fit a Gaussian Mixture Model with two components clf = mixture . GaussianMixture(n_components = 2 , covariance_type = 'full' ) clf . fit(X_train) GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100, means_init=None, n_components=2, n_init=1, precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weights_init=None) x = np . linspace( - 20. , 30. ) y = np . linspace( - 20. , 40. ) X, Y = np . meshgrid(x, y) XX = np . array([X . ravel(), Y . ravel()]) . T Z = - clf . score_samples(XX) Z = Z . reshape(X . shape) plt . figure(figsize = [ 12 , 10 ]) # display predicted scores by the model as a contour plot CS = plt . contour(X, Y, Z, norm = LogNorm(vmin = 1.0 , vmax = 1000.0 ), levels = np . logspace( 0 , 3 , 10 )) CB = plt . colorbar(CS, shrink = 0.8 , extend = 'both' ) plt . scatter(X_train[:, 0 ], X_train[:, 1 ], .8 ) plt . title( 'Negative log-likelihood predicted by a GMM' ) plt . axis( 'tight' ) plt . show()","title":"2.  Gaussian Mixture Model (Optional)"},{"location":"Statistics-Smpling/code/","text":"Satistics : Sampling and Inferences https://docs.scipy.org/doc/scipy/reference/stats.html import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns sns . set() Sampling with Montecarlo 1. Montecarlo Integration Example 1 np . arange( - 3 , 3 , 0.01 ) x = np . linspace( - 3 , 3 , 100 ) dist = stats . norm( 0 , 1 ) a = - 2 b = 0 plt . plot(x, dist . pdf(x)) plt . fill_between(np . linspace(a,b, 100 ), dist . pdf(np . linspace(a,b, 100 )), alpha = 0.5 ) plt . text(b + 0.1 , 0.1 , 'p= %.4f ' % (dist . cdf(b) - dist . cdf(a)), fontsize = 14 ) pass n = 10000 x = stats . norm . rvs(loc = 0 ,scale = 1 ,size = n) These points are generated randomly. No dependence to each other. plt . figure(figsize = [ 18 , 4 ]) plt . plot( range ( len (x[ 0 : 100 ])),x[ 0 : 100 ]) plt . scatter( range ( len (x[ 0 : 100 ])),x[ 0 : 100 ]) <matplotlib.collections.PathCollection at 0x1b34b5e8a58> np . sum((a < x) & (x < b)) / n 0.4733 from scipy.integrate import quad y, err = quad(dist . pdf, a, b) y 0.47724986805182085 2. Markov Chain Montecarlo In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolis\u2013Hastings algorithm. All MCMC sampling schemes have the following structure: Start with some state xk Repeat Draw a sample xk+1 from a proposal distribution p(xk) Calculate the acceptance probability A(x) - in general, this is a function of the proposal and target distributions at xk and xk+1 Choose a standard random uniform number r If r<A(x) set state to be xk+1, otherwise keep state as xk n = 10000 xs = np . zeros(n) x = 0 for i in range (n): p = np . random . uniform(x - 1 , x + 1 ) a = np . exp( - (p ** 2 - x ** 2 ) / 2 ) r = np . random . rand() if r < a: x = p xs[i] = x These data points create a chain call Markov Chain. The sample generation follows principle of detailed balance. plt . figure(figsize = [ 18 , 4 ]) plt . plot( range ( len (xs[ 0 : 100 ])),xs[ 0 : 100 ]) plt . scatter( range ( len (xs[ 0 : 100 ])),xs[ 0 : 100 ]) <matplotlib.collections.PathCollection at 0x1b34b63fd30> plt . hist(xs, 25 , histtype = 'step' , density = True ) xp = np . linspace(xs . min(), xs . max(), 100 ) plt . plot(xp, stats . norm() . pdf(xp)) pass 3. Gibb's Sampling In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. Gibbs sampling is a type of random walk through parameter space. Suppose the parameter vector $\\theta$ has two components $\\theta= [\\theta_1, \\theta_2]$, then each iteration of the Gibbs sampler cycles through each component and draws a new value conditional on all the others. There are thus 2 steps for each iteration. We consider the example of the bivariate normal distribution with unknown mean $\\theta$, but known covariance matrix $$\\left(\\begin{array}{cc}1 & \\rho \\\\ \\rho & 1 \\end{array}\\right).$$ If one observation $X=[x_1, x_2]$ is made and a uniform prior on $\\theta$ is used, the posterior is given by $$ \\left(\\begin{array}{c} \\theta_1 \\\\ \\theta_2 \\end{array}\\right) \\biggr\\rvert X \\sim N\\left(\\left(\\begin{array}{c}x_1 \\\\ x_2 \\end{array}\\right), \\left(\\begin{array}{cc}1 & \\rho \\\\ \\rho & 1 \\end{array}\\right)\\right) $$ In order to illustrate the use of the Gibbs sampler we need the conditional posterior distributions, which from the properties of multivariate normal distributions are given by $$\\theta_1|\\theta_2,\\: x_1 \\text{ ~ } N(\\rho\\theta_2,\\: 1-\\rho^2) \\text{ ~ } \\rho\\theta_2 + \\sqrt{1-\\rho^2}N(0,\\:1)$$ $$\\theta_2|\\theta_1,\\: x_2 \\text{ ~ } N(\\rho\\theta_1,\\: 1-\\rho^2) \\text{ ~ } \\rho\\theta_1 + \\sqrt{1-\\rho^2}N(0,\\:1)$$ The Gibbs sampler proceeds by alternately sampling from these two normal distributions. This is now coded in simple Python deliberately making the steps obvious. Experiemnt -1 (Flat distribution) '''sampling''' rho = - 0.45 niter = 1000 xs = np . zeros((niter, 2 )) x1,x2 = 0 , 0 xs[ 0 ,:] = x1,x2 for i in range ( 1 ,niter): x1 = np . random . normal(rho * x2, 1 - rho ** 2 ) x2 = np . random . normal(rho * x1, 1 - rho ** 2 ) xs[i,:] = x1,x2 '''contor''' dist = stats . multivariate_normal([ 0 , 0 ], np . array([[ 1 ,rho],[rho, 1 ]])) x = np . linspace( - 3.0 , 3.0 , 100 ) y = np . linspace( - 3.0 , 2.0 , 100 ) X, Y = np . meshgrid(x, y) Z = dist . pdf(np . c_[X . ravel(), Y . ravel()]) . reshape(X . shape) '''plot''' plt . figure(figsize = [ 18 , 10 ]) plt . contour(X, Y, Z) plt . scatter(xs[:, 0 ], xs[:, 1 ], s = 2 ) plt . plot(xs[:, 0 ], xs[:, 1 ],alpha = 0.1 ) plt . axis( 'square' ) plt . show() Experiment -2 (Narrow distribution) '''sampling''' rho = - 0.8 niter = 10 xs = np . zeros((niter, 2 )) x1,x2 = 0 , 0 xs[ 0 ,:] = x1,x2 for i in range ( 1 ,niter): x1 = np . random . normal(rho * x2, 1 - rho ** 2 ) x2 = np . random . normal(rho * x1, 1 - rho ** 2 ) xs[i,:] = x1,x2 '''contor''' dist = stats . multivariate_normal([ 0 , 0 ], np . array([[ 1 ,rho],[rho, 1 ]])) x = np . linspace( - 3.0 , 3.0 , 100 ) y = np . linspace( - 3.0 , 2.0 , 100 ) X, Y = np . meshgrid(x, y) Z = dist . pdf(np . c_[X . ravel(), Y . ravel()]) . reshape(X . shape) '''plot''' plt . figure(figsize = [ 18 , 10 ]) plt . contour(X, Y, Z) plt . scatter(xs[:, 0 ], xs[:, 1 ], s = 2 ) plt . plot(xs[:, 0 ], xs[:, 1 ],alpha = 0.5 ) plt . axis( 'square' ) plt . show() 4. Metropolice Hasting In statistics and statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. def circle (x, y): return (x - 1 ) ** 2 + (y - 2 ) ** 2 - 3 ** 2 def pgauss (x, y): return stats . multivariate_normal . pdf([x, y], mean = mus, cov = sigmas) def metropolis_hastings (p, iter = 1000 ): x, y = 0. , 0. samples = np . zeros(( iter , 2 )) for i in range ( iter ): x_star, y_star = np . array([x, y]) + np . random . normal(size = 2 ) frac = p(x_star, y_star) / p(x, y) if np . random . rand() < frac: x, y = x_star, y_star samples[i] = np . array([x, y]) return samples mus = np . array([ 5 , 5 ]) sigmas = np . array([[ 1 , .9 ], [ .9 , 1 ]]) samples = metropolis_hastings(circle, iter = 10000 ) sns . jointplot(samples[:, 0 ], samples[:, 1 ]) <seaborn.axisgrid.JointGrid at 0x1b34b7a46d8> samples = metropolis_hastings(pgauss, iter = 10000 ) sns . jointplot(samples[:, 0 ], samples[:, 1 ]) <seaborn.axisgrid.JointGrid at 0x1b34b838828>","title":"Statistics with sampling"},{"location":"Statistics-Smpling/code/#satistics-sampling-and-inferences","text":"https://docs.scipy.org/doc/scipy/reference/stats.html import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns sns . set()","title":"Satistics : Sampling and Inferences"},{"location":"Statistics-Smpling/code/#sampling-with-montecarlo","text":"","title":"Sampling with Montecarlo"},{"location":"Statistics-Smpling/code/#1-montecarlo-integration","text":"Example 1 np . arange( - 3 , 3 , 0.01 ) x = np . linspace( - 3 , 3 , 100 ) dist = stats . norm( 0 , 1 ) a = - 2 b = 0 plt . plot(x, dist . pdf(x)) plt . fill_between(np . linspace(a,b, 100 ), dist . pdf(np . linspace(a,b, 100 )), alpha = 0.5 ) plt . text(b + 0.1 , 0.1 , 'p= %.4f ' % (dist . cdf(b) - dist . cdf(a)), fontsize = 14 ) pass n = 10000 x = stats . norm . rvs(loc = 0 ,scale = 1 ,size = n) These points are generated randomly. No dependence to each other. plt . figure(figsize = [ 18 , 4 ]) plt . plot( range ( len (x[ 0 : 100 ])),x[ 0 : 100 ]) plt . scatter( range ( len (x[ 0 : 100 ])),x[ 0 : 100 ]) <matplotlib.collections.PathCollection at 0x1b34b5e8a58> np . sum((a < x) & (x < b)) / n 0.4733 from scipy.integrate import quad y, err = quad(dist . pdf, a, b) y 0.47724986805182085","title":"1.  Montecarlo Integration"},{"location":"Statistics-Smpling/code/#2-markov-chain-montecarlo","text":"In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolis\u2013Hastings algorithm. All MCMC sampling schemes have the following structure: Start with some state xk Repeat Draw a sample xk+1 from a proposal distribution p(xk) Calculate the acceptance probability A(x) - in general, this is a function of the proposal and target distributions at xk and xk+1 Choose a standard random uniform number r If r<A(x) set state to be xk+1, otherwise keep state as xk n = 10000 xs = np . zeros(n) x = 0 for i in range (n): p = np . random . uniform(x - 1 , x + 1 ) a = np . exp( - (p ** 2 - x ** 2 ) / 2 ) r = np . random . rand() if r < a: x = p xs[i] = x These data points create a chain call Markov Chain. The sample generation follows principle of detailed balance. plt . figure(figsize = [ 18 , 4 ]) plt . plot( range ( len (xs[ 0 : 100 ])),xs[ 0 : 100 ]) plt . scatter( range ( len (xs[ 0 : 100 ])),xs[ 0 : 100 ]) <matplotlib.collections.PathCollection at 0x1b34b63fd30> plt . hist(xs, 25 , histtype = 'step' , density = True ) xp = np . linspace(xs . min(), xs . max(), 100 ) plt . plot(xp, stats . norm() . pdf(xp)) pass","title":"2. Markov Chain Montecarlo"},{"location":"Statistics-Smpling/code/#3-gibbs-sampling","text":"In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. Gibbs sampling is a type of random walk through parameter space. Suppose the parameter vector $\\theta$ has two components $\\theta= [\\theta_1, \\theta_2]$, then each iteration of the Gibbs sampler cycles through each component and draws a new value conditional on all the others. There are thus 2 steps for each iteration. We consider the example of the bivariate normal distribution with unknown mean $\\theta$, but known covariance matrix $$\\left(\\begin{array}{cc}1 & \\rho \\\\ \\rho & 1 \\end{array}\\right).$$ If one observation $X=[x_1, x_2]$ is made and a uniform prior on $\\theta$ is used, the posterior is given by $$ \\left(\\begin{array}{c} \\theta_1 \\\\ \\theta_2 \\end{array}\\right) \\biggr\\rvert X \\sim N\\left(\\left(\\begin{array}{c}x_1 \\\\ x_2 \\end{array}\\right), \\left(\\begin{array}{cc}1 & \\rho \\\\ \\rho & 1 \\end{array}\\right)\\right) $$ In order to illustrate the use of the Gibbs sampler we need the conditional posterior distributions, which from the properties of multivariate normal distributions are given by $$\\theta_1|\\theta_2,\\: x_1 \\text{ ~ } N(\\rho\\theta_2,\\: 1-\\rho^2) \\text{ ~ } \\rho\\theta_2 + \\sqrt{1-\\rho^2}N(0,\\:1)$$ $$\\theta_2|\\theta_1,\\: x_2 \\text{ ~ } N(\\rho\\theta_1,\\: 1-\\rho^2) \\text{ ~ } \\rho\\theta_1 + \\sqrt{1-\\rho^2}N(0,\\:1)$$ The Gibbs sampler proceeds by alternately sampling from these two normal distributions. This is now coded in simple Python deliberately making the steps obvious. Experiemnt -1 (Flat distribution) '''sampling''' rho = - 0.45 niter = 1000 xs = np . zeros((niter, 2 )) x1,x2 = 0 , 0 xs[ 0 ,:] = x1,x2 for i in range ( 1 ,niter): x1 = np . random . normal(rho * x2, 1 - rho ** 2 ) x2 = np . random . normal(rho * x1, 1 - rho ** 2 ) xs[i,:] = x1,x2 '''contor''' dist = stats . multivariate_normal([ 0 , 0 ], np . array([[ 1 ,rho],[rho, 1 ]])) x = np . linspace( - 3.0 , 3.0 , 100 ) y = np . linspace( - 3.0 , 2.0 , 100 ) X, Y = np . meshgrid(x, y) Z = dist . pdf(np . c_[X . ravel(), Y . ravel()]) . reshape(X . shape) '''plot''' plt . figure(figsize = [ 18 , 10 ]) plt . contour(X, Y, Z) plt . scatter(xs[:, 0 ], xs[:, 1 ], s = 2 ) plt . plot(xs[:, 0 ], xs[:, 1 ],alpha = 0.1 ) plt . axis( 'square' ) plt . show() Experiment -2 (Narrow distribution) '''sampling''' rho = - 0.8 niter = 10 xs = np . zeros((niter, 2 )) x1,x2 = 0 , 0 xs[ 0 ,:] = x1,x2 for i in range ( 1 ,niter): x1 = np . random . normal(rho * x2, 1 - rho ** 2 ) x2 = np . random . normal(rho * x1, 1 - rho ** 2 ) xs[i,:] = x1,x2 '''contor''' dist = stats . multivariate_normal([ 0 , 0 ], np . array([[ 1 ,rho],[rho, 1 ]])) x = np . linspace( - 3.0 , 3.0 , 100 ) y = np . linspace( - 3.0 , 2.0 , 100 ) X, Y = np . meshgrid(x, y) Z = dist . pdf(np . c_[X . ravel(), Y . ravel()]) . reshape(X . shape) '''plot''' plt . figure(figsize = [ 18 , 10 ]) plt . contour(X, Y, Z) plt . scatter(xs[:, 0 ], xs[:, 1 ], s = 2 ) plt . plot(xs[:, 0 ], xs[:, 1 ],alpha = 0.5 ) plt . axis( 'square' ) plt . show()","title":"3. Gibb's Sampling"},{"location":"Statistics-Smpling/code/#4-metropolice-hasting","text":"In statistics and statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. def circle (x, y): return (x - 1 ) ** 2 + (y - 2 ) ** 2 - 3 ** 2 def pgauss (x, y): return stats . multivariate_normal . pdf([x, y], mean = mus, cov = sigmas) def metropolis_hastings (p, iter = 1000 ): x, y = 0. , 0. samples = np . zeros(( iter , 2 )) for i in range ( iter ): x_star, y_star = np . array([x, y]) + np . random . normal(size = 2 ) frac = p(x_star, y_star) / p(x, y) if np . random . rand() < frac: x, y = x_star, y_star samples[i] = np . array([x, y]) return samples mus = np . array([ 5 , 5 ]) sigmas = np . array([[ 1 , .9 ], [ .9 , 1 ]]) samples = metropolis_hastings(circle, iter = 10000 ) sns . jointplot(samples[:, 0 ], samples[:, 1 ]) <seaborn.axisgrid.JointGrid at 0x1b34b7a46d8> samples = metropolis_hastings(pgauss, iter = 10000 ) sns . jointplot(samples[:, 0 ], samples[:, 1 ]) <seaborn.axisgrid.JointGrid at 0x1b34b838828>","title":"4. Metropolice Hasting"}]}